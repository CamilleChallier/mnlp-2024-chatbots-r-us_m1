[
    {
        "course_id": 15002,
        "question_id": 1016,
        "question": "What is a good distance metric to be used when you want to compute the similarity between documents independent of their length?A penalty will be applied for any incorrect answers.",
        "question_options": [
            "Cosine similarity",
            "Euclidean distance",
            "Manhattan distance",
            "Chi-squared distance"
        ],
        "A_chat_id": 90741,
        "B_chat_id": 90742,
        "A": "Cosine similarity is the correct distance metric to use when you want to compute the similarity between documents independent of their length.",
        "B": "The correct answer is Cosine similarity."
    },
    {
        "course_id": 15118,
        "question_id": 6900,
        "question": "Consider the motion of a harmonic oscillator. Identify when the  acceleration is naught.\n\n",
        "question_options": null,
        "A_chat_id": 90743,
        "B_chat_id": 90744,
        "A": "The acceleration of a harmonic oscillator is zero at the extreme points of its motion, i.e. when the oscillator reaches its maximum displacement from the equilibrium position. This occurs when the velocity is zero and the direction of motion changes.",
        "B": "In the motion of a harmonic oscillator, such as a mass-spring system or a pendulum, the acceleration is zero at the extreme points of the oscillation. This means that the acceleration is zero when the displacement of the oscillator is at its maximum or minimum value. \n\nAt these points, the restoring force provided by the spring or the gravitational force acting on the mass balances the inertia of the oscillator, resulting in a momentary pause in the motion and zero acceleration. \n\nSo, in a harmonic oscillator, the acceleration is zero at the extreme points of the oscillation."
    },
    {
        "course_id": 0,
        "question_id": 6147,
        "question": "Which one of these Ciphers is perfectly secure?",
        "question_options": [
            "Hieroglyphs",
            "Caesar",
            "Vigen\\`{e}re",
            "Vernam"
        ],
        "A_chat_id": 90745,
        "B_chat_id": 90746,
        "A": "The Vernam cipher is perfectly secure, as long as the key is truly random, the key is at least as long as the message, and the key is never reused. It is also known as a one-time pad and provides perfect secrecy.",
        "B": "The Vernam cipher, also known as the one-time pad, is considered perfectly secure when implemented correctly. This is because it uses a random key that is as long as the message itself, making it practically impossible to break without the key. The other ciphers mentioned (Hieroglyphs, Caesar, and Vigen\u00e8re) all have vulnerabilities that can be exploited to decrypt the message without the key."
    },
    {
        "course_id": 9,
        "question_id": 6850,
        "question": "A vending machine dispensing books of stamps accepts only $1 bills, $2 bills and $5 bills.\n\nFind a recurrence relation for the number of ways to deposit $$n$$ dollars in the vending machine, where the order in which the coins and bills are deposited matters.",
        "question_options": [
            "$$a_{n} = a_{n-1} + a_{n-2} + a_{n-5}$$",
            "$$a_{n+1} = a_{n-1} + a_{n-2} + a_{n-5}$$",
            "$$a_{n+1} = a_{n-1} + 2a_{n-2} + 5a_{n-5}$$",
            "$$a_{n} = a_{n-1} + 2a_{n-2} + 5a_{n-5}$$"
        ],
        "A_chat_id": 90747,
        "B_chat_id": 90748,
        "A": "$$a_{n} = a_{n-1} + 2a_{n-2} + 5a_{n-5}$$",
        "B": "To find the recurrence relation for the number of ways to deposit \\( n \\) dollars in the vending machine, we can consider the possible last bill that was deposited to reach the total of \\( n \\) dollars.\n\nLet's analyze the options:\n- If the last bill deposited was a $1 bill, then there are \\( a_{n-1} \\) ways to reach \\( n \\) dollars.\n- If the last bill deposited was a $2 bill, then there are \\( a_{n-2} \\) ways to reach \\( n \\) dollars.\n- If the last bill deposited was a $5 bill, then there are \\( a_{n-5} \\) ways to reach \\( n \\) dollars.\n\nTherefore, the correct recurrence relation for the number of ways to deposit \\( n \\) dollars in the vending machine is:\n\n\\[ a_{n} = a_{n-1} + a_{n-2} + a_{n-5} \\]\n\nSo, the correct choice is:\n[ ] \\( a_{n} = a_{n-1} + a_{n-2} + a_{n-5} \\)"
    },
    {
        "course_id": 15125,
        "question_id": 3667,
        "question": "Which of the following graph analysis techniques do you believe would be most appropriate to identify communities on a social graph?",
        "question_options": [
            "Cliques",
            "Random Walks",
            "Shortest Paths",
            "Association rules"
        ],
        "A_chat_id": 90749,
        "B_chat_id": 90750,
        "A": "Random Walks would be the most appropriate graph analysis technique to identify communities on a social graph. Random Walks are a common technique used in community detection algorithms such as the Markov Clustering Algorithm (MCL) and the Infomap algorithm. By simulating random walks on the graph, the algorithm can detect densely connected regions or communities within the graph.",
        "B": "The most appropriate technique to identify communities on a social graph would be cliques. Cliques are subsets of nodes in a graph where every node is directly connected to every other node in the subset. Identifying cliques can help in recognizing tightly knit groups or communities within a social network."
    },
    {
        "course_id": 15125,
        "question_id": 1431,
        "question": "Vectorize the input with the Vector Space Model",
        "question_options": null,
        "A_chat_id": 90751,
        "B_chat_id": 90752,
        "A": "The Vector Space Model is a mathematical model used to represent text documents as vectors in a high-dimensional space. Each dimension in the vector represents a unique term in the corpus of documents. The value of each dimension is typically the frequency of the term in the document.\n\nTo vectorize the input using the Vector Space Model, we first need to preprocess the text by tokenizing it (splitting it into individual words), removing stopwords, and applying stemming or lemmatization. Then, we create a vocabulary of unique terms from the corpus.\n\nNext, we represent each document as a vector where each dimension corresponds to a term in the vocabulary. The value of each dimension is the term frequency-inverse document frequency (TF-IDF) weight of the term in the document. TF-IDF is a statistical measure that evaluates how important a term is to a document in a collection or corpus.\n\nFinally, we have a matrix where each row represents a document and each column represents a term in the vocabulary. This matrix is the vectorized representation of the input using the Vector Space Model.",
        "B": "To vectorize the input with the Vector Space Model, we need to follow these steps:\n\n1. Tokenization: Split the input text into individual words or tokens.\n2. Text pre-processing: Remove any stop words, punctuation, and apply stemming or lemmatization if necessary.\n3. Create a vocabulary: Build a vocabulary of unique words from the pre-processed text.\n4. Create a vector representation: For each document (input text), create a vector where each element represents the frequency or presence of a word from the vocabulary.\n5. Normalize the vectors: Normalize the vectors to ensure that each document's vector representation is on the same scale.\n\nHere is a simple example using Python:\n\n```python\nfrom sklearn.feature_extraction.text import CountVectorizer\n\n# Input text\ninput_text = \"Vectorize the input with the Vector Space Model\"\n\n# Step 1: Tokenization\ntokens = input_text.lower().split()\n\n# Step 2: Text pre-processing (not shown in this example)\n\n# Step 3: Create a vocabulary\nvocabulary = set(tokens)\n\n# Step 4: Create a vector representation\nvectorizer = CountVectorizer(vocabulary=vocabulary)\nvector = vectorizer.fit_transform([input_text]).toarray()\n\nprint(\"Vector representation:\")\nprint(vector)\n```\n\nThis code snippet demonstrates how to vectorize the input text using the CountVectorizer from scikit-learn. Remember to apply appropriate text pre-processing steps based on your specific requirements before vectorizing the input text."
    },
    {
        "course_id": 15126,
        "question_id": 2329,
        "question": "In which type of processors do you expect to find a reorder buffer?",
        "question_options": null,
        "A_chat_id": 90753,
        "B_chat_id": 90754,
        "A": "Reorder buffers are typically found in out-of-order processors. These processors allow instructions to be executed in a different order than they appear in the program, and the reorder buffer helps keep track of the original order of instructions for correct program execution.",
        "B": "Reorder buffers are typically found in out-of-order processors. These processors are designed to execute instructions out of order to improve performance by utilizing available execution units more efficiently. The reorder buffer helps in keeping track of the original program order of instructions and ensures that the results are committed in the correct order, even if they are executed out of order."
    },
    {
        "course_id": 15127,
        "question_id": 3332,
        "question": "Consider the following matrix-factorization problem. For the observed ratings $r_{u m}$ for a given pair $(u, m)$ of a user $u$ and a movie $m$, one typically tries to estimate the score by $$ f_{u m}=\\left\\langle\\mathbf{v}_{u}, \\mathbf{w}_{m}\\right\\rangle+b_{u}+b_{m} $$ Here $\\mathbf{v}_{u}$ and $\\mathbf{w}_{m}$ are vectors in $\\mathbb{R}^{D}$ and $b_{u}$ and $b_{m}$ are scalars, indicating the bias. How could you address the problem of potentially recommending a new movie without any ratings to users? [As in the previous point, this is also not a math question.]",
        "question_options": null,
        "A_chat_id": 90755,
        "B_chat_id": 90756,
        "A": "One way to address the problem of recommending a new movie without any ratings to users is to use collaborative filtering techniques. Collaborative filtering leverages the ratings and preferences of other users to make recommendations. \n\nOne common approach is to use matrix factorization to learn the latent factors representing users and items (movies in this case) based on existing ratings. Once these latent factors are learned, they can be used to predict the rating a user would give to a new movie.\n\nIn the context of the given formula, for a new movie, we would estimate the score $f_{um}$ for a user $u$ and the new movie $m$ by calculating the dot product of the learned user vector $\\mathbf{v}_{u}$ with the latent factors of the new movie $\\mathbf{w}_{m}$. The bias terms $b_{u}$ and $b_{m}$ can also be incorporated into the prediction.\n\nOverall, by utilizing collaborative filtering and matrix factorization techniques, we can make recommendations for new movies to users based on their preferences and behavior in the existing rating data.",
        "B": "To address the problem of recommending a new movie to users without any ratings, you can consider the following approaches:\n\n1. **Popularity-based Recommendations**: One common approach is to recommend new movies based on their popularity or general trends. This can involve recommending recently released movies, highly acclaimed films, or trending movies in a particular genre. While this approach may not be personalized, it can serve as a starting point for users to explore new content.\n\n2. **Content-based Filtering**: Another approach is to recommend new movies to users based on the characteristics of the movie itself. This can involve analyzing the content of the movie (e.g., genre, cast, plot keywords) and recommending movies that are similar in content to movies that the user has previously rated highly. Content-based filtering can help in providing recommendations for new movies that align with the user's preferences.\n\n3. **Collaborative Filtering**: Collaborative filtering techniques can also be used to recommend new movies to users. In this approach, recommendations are made based on the preferences and behaviors of similar users. This can involve identifying users with similar movie preferences and recommending new movies that have been highly rated by these users. Collaborative filtering can help in providing personalized recommendations for new movies to users.\n\n4. **Hybrid Approaches**: Hybrid recommendation systems combine multiple recommendation techniques, such as collaborative filtering and content-based filtering, to provide more accurate and diverse recommendations. By leveraging the strengths of different recommendation approaches, hybrid systems can offer more effective recommendations for new movies to users without ratings.\n\nBy utilizing these different approaches, you can address the challenge of recommending new movies to users without any ratings. Each approach has its strengths and limitations, and combining them can help in providing more diverse and personalized recommendations to users."
    },
    {
        "course_id": 15103,
        "question_id": 1716,
        "question": "Assume your colleague wants to wait until the next minor release to include a major bugfix instead of making a bugfix release. Explain why this is not a good idea.",
        "question_options": null,
        "A_chat_id": 90757,
        "B_chat_id": 90758,
        "A": "Waiting until the next minor release to include a major bugfix is not a good idea because a major bugfix should be addressed as soon as possible to prevent any negative impact on users or the system. Delays in fixing major bugs can lead to security vulnerabilities, system instability, and potential loss of user trust. It is important to prioritize bugfixes based on their severity and impact, and address them in a timely manner to ensure the overall health and reliability of the system.",
        "B": "Waiting until the next minor release to include a major bugfix instead of making a bugfix release is not a good idea for several reasons:\n\n1. Impact on users: If the bug is significant and affecting users, delaying the fix until the next minor release can negatively impact users' experience and productivity. Users may become frustrated with the product and may even switch to a competitor's product if the bug is not addressed in a timely manner.\n\n2. Reputation: Delaying a major bugfix can harm the reputation of the product and the company. Users may perceive the company as being unresponsive to their needs and not prioritizing the stability and quality of the product.\n\n3. Increased complexity: Including a major bugfix in a minor release may introduce additional complexity and risk. The bugfix may require changes to multiple parts of the codebase, which can increase the likelihood of introducing new bugs or regressions.\n\n4. Maintenance burden: Delaying the bugfix means that users will have to wait longer for a solution, potentially leading to an increase in support tickets and inquiries. This can put additional strain on the support and development teams, as they may have to spend more time addressing user complaints and issues.\n\nOverall, it is generally recommended to address major bugs as soon as possible through a bugfix release to minimize the impact on users, maintain the product's reputation, and reduce the overall complexity and maintenance burden."
    },
    {
        "course_id": 15127,
        "question_id": 3896,
        "question": "Which of the following is true for Generative Adversarial Networks (GANs) but not Diffusion Models:\n        ",
        "question_options": [
            "They can generate new data from input noise.",
            "They use two different models during training.",
            "They use a simple L2 loss function.",
            "They gradually remove noise from inputs using a Markov chain."
        ],
        "A_chat_id": 90759,
        "B_chat_id": 90760,
        "A": "The statement that is true for Generative Adversarial Networks (GANs) but not Diffusion Models is: \n- They can generate new data from input noise. \n\nBoth GANs and Diffusion Models use two different models during training and do not use a simple L2 loss function. However, GANs generate new data from input noise, while Diffusion Models gradually remove noise from inputs using a Markov chain.",
        "B": "The correct answer is:\n\n[ ] They can generate new data from input noise. \n\nGenerative Adversarial Networks (GANs) can generate new data from input noise, which is a key characteristic of GANs. Diffusion models do not generate data from input noise in the same way as GANs do."
    },
    {
        "course_id": 0,
        "question_id": 6154,
        "question": "Which of the following is well preserved by 2G?",
        "question_options": [
            "Confidentiality",
            "Message Integrity",
            "Challenge freshness",
            "Authentication of Mobile Station"
        ],
        "A_chat_id": 90761,
        "B_chat_id": 90762,
        "A": "Confidentiality is well preserved by 2G. 2G provides encryption to protect the confidentiality of the communication between the mobile station and the base station.",
        "B": "Confidentiality and Message Integrity are well preserved by 2G networks. Challenge freshness and Authentication of Mobile Station are not directly related to the security features provided by 2G networks."
    },
    {
        "course_id": 4,
        "question_id": 6593,
        "question": "Which of the following statements about mitigations are correct?",
        "question_options": [
            "Control-Flow Integrity can efficiently protect the forward edge\n            but, when using target sets, is limited on the backward edge",
            "Code-Pointer Integrity (specifically the implementation\n            described in the slides) uses a separate stack to protect code\n            pointers.",
            "Shadow stacks can be implemented in software with zero\n            overhead.",
            "Safe stacks protect against corruption of all data on the\n            stack."
        ],
        "A_chat_id": 90763,
        "B_chat_id": 90764,
        "A": "The correct statements about mitigations are:\n[ ] Control-Flow Integrity can efficiently protect the forward edge but, when using target sets, is limited on the backward edge\n[X] Code-Pointer Integrity (specifically the implementation described in the slides) uses a separate stack to protect code pointers.\n[ ] Shadow stacks can be implemented in software with zero overhead.\n[ ] Safe stacks protect against corruption of all data on the stack.",
        "B": "Let's analyze each statement:\n\n1. [ ] Control-Flow Integrity can efficiently protect the forward edge but, when using target sets, is limited on the backward edge:\n- This statement is partly correct. Control-Flow Integrity (CFI) can help prevent attacks that alter the control flow of a program, protecting the forward edge. However, when using target sets, there can be limitations on the backward edge protection. So, this statement is partially correct.\n\n2. [ ] Code-Pointer Integrity (specifically the implementation described in the slides) uses a separate stack to protect code pointers:\n- This statement is incorrect. Code-Pointer Integrity (CPI) does not necessarily use a separate stack to protect code pointers. CPI aims to protect function pointers and other code references from being tampered with, but the implementation may vary and not always involve a separate stack.\n\n3. [ ] Shadow stacks can be implemented in software with zero overhead:\n- This statement is incorrect. Shadow stacks, which store return addresses separately to protect against stack-based attacks, do incur some overhead in terms of memory and performance. While efforts can be made to minimize this overhead, implementing shadow stacks with zero overhead is not feasible.\n\n4. [ ] Safe stacks protect against corruption of all data on the stack:\n- This statement is incorrect. Safe stacks are designed to protect return addresses and function pointers specifically, rather than all data on the stack. They aim to prevent control-flow hijacking attacks by safeguarding critical control data.\n\nTherefore, the correct statements are:\n- [ ] Control-Flow Integrity can efficiently protect the forward edge but, when using target sets, is limited on the backward edge\n- [ ] Safe stacks protect against corruption of all data on the stack."
    },
    {
        "course_id": 15005,
        "question_id": 8,
        "question": "Suppose we run JOS and set a breakpoint at syscall (in lib/syscall.c). What are the Current Privilege Level (CPL) before invoking the syscall function and after executing the int 0x30 instruction?",
        "question_options": [
            "0 3",
            "0 0",
            "3 0",
            "3 3"
        ],
        "A_chat_id": 90765,
        "B_chat_id": 90766,
        "A": "3 0",
        "B": "Before invoking the `syscall` function, the Current Privilege Level (CPL) is 3 (Ring 3) because user applications run in Ring 3 in x86 systems. When the `int 0x30` instruction is executed, it triggers a system call, and the CPL changes to 0 (Ring 0) because system calls are executed in Kernel Mode (Ring 0) in x86 systems.\n\nSo, the correct answer is:\n[ ] 3 0"
    },
    {
        "course_id": 15011,
        "question_id": 4176,
        "question": "Last year Professor Ueli von Gruy\\`{e}res worked hard to to obtain an estimator $\\Alg$  to estimate the total cheese consumption of fondue lovers in Switzerland. For a small $\\epsilon >0$, his estimator \\Alg only  asks $3/\\epsilon^2$ random persons and have the following guarantee: if we  let $W$ denote the true answer and let $X$ be the random output of \\Alg then \\begin{align*} \\Pr[|X - W| \\geq \\epsilon W] \\leq 1/3\\,. %\\qquad \\mbox{ where $\\epsilon > 0$ is a small constant.} \\end{align*} However, Ueli is now stuck because the error probability of $1/3$ is too high.  We are  therefore going to help Ueli by designing a new estimator with a much higher success probability while still only asking relatively few persons. For a fixed small parameter $\\delta >0$, your task is to design and analyze an estimator that outputs a random value $Y$ with the following guarantee: \\begin{align} \\label{eq:guarantee2} \\Pr[|Y - W| \\geq \\epsilon W] \\leq \\delta\\,. %\\qquad \\mbox{ where $\\epsilon > 0$ is a small constant.} \\end{align} Your estimator should ask at most $3000\\log(1/\\delta)/\\epsilon^2$ persons about their preferences.  \\\\ While you should explain why your estimator works and what tools to use to analyze it, \\emph{you do not need to do any detailed calculations.} \\\\ {\\em (In this problem you are asked to (i) design an estimator that asks  at most $3000 \\log(1/\\delta)/\\epsilon^2$ persons  and (ii) explain why it satisfies the guarantee~\\eqref{eq:guarantee2}. Recall that you are allowed to refer to material covered in the lecture notes.)}",
        "question_options": null,
        "A_chat_id": 90767,
        "B_chat_id": 90768,
        "A": "To design an estimator with a higher success probability, we can use the Chernoff bound. By asking $m = \\frac{3000\\log(1/\\delta)}{\\epsilon^2}$ random persons about their preferences, we can estimate the total cheese consumption.\n\nUsing Chernoff bound, we can show that the probability of the estimator $Y$ being far from the true answer $W$ by more than $\\epsilon W$ is at most $\\delta$. This ensures that our estimator has a much higher success probability compared to the previous estimator.\n\nThe Chernoff bound is a powerful tool for bounding tail probabilities of sums of independent random variables, and it will enable us to analyze the performance of our estimator in satisfying the guarantee~\\eqref{eq:guarantee2}.\n\nBy carefully choosing the number of random persons to ask and using Chernoff bound, we can design an estimator that satisfies the desired guarantee with high probability.",
        "B": "To design an estimator with a higher success probability, we can use the Chernoff bound. \n\nLet's design the new estimator as follows:\n1. Randomly sample $m = 3000\\log(1/\\delta)/\\epsilon^2$ persons and aggregate their responses to calculate the average preference, denoted by $\\hat{W}$.\n2. Output $Y = \\hat{W}$.\n\nNow, let's analyze why this estimator satisfies the guarantee in Equation~\\eqref{eq:guarantee2}.\n\nFirst, note that $\\hat{W}$ is the average of $m$ independent random variables (preferences of individuals) and it is an unbiased estimator of the true total cheese consumption $W$. By the Chernoff bound, we have:\n\\begin{align*}\n    \\Pr\\left[|\\hat{W} - W| \\geq \\epsilon W\\right] \\leq 2\\exp\\left(-\\frac{m\\epsilon^2 W^2}{2}\\right).\n\\end{align*}\n\nSince $\\hat{W}$ is an estimator with high probability of being close to $W$, we can use this property to analyze the behavior of $Y = \\hat{W}$. By setting $\\delta = 2\\exp\\left(-\\frac{m\\epsilon^2 W^2}{2}\\right)$, we can ensure that:\n\\begin{align*}\n    \\Pr\\left[|Y - W| \\geq \\epsilon W\\right] = \\Pr\\left[|\\hat{W} - W| \\geq \\epsilon W\\right] \\leq \\delta.\n\\end{align*}\n\nTherefore, the new estimator $Y$ satisfies the guarantee in Equation~\\eqref{eq:guarantee2} with the desired success probability $\\delta$ by sampling at most $3000\\log(1/\\delta)/\\epsilon^2$ persons and taking their average preference. This approach leverages the concentration properties provided by the Chernoff bound to ensure a high success probability for the estimator."
    },
    {
        "course_id": 15002,
        "question_id": 982,
        "question": "Up to which linguistic processing level can each of the following sentences be considered as correct? The glass broke its leg, I no go rain, The cook put cherry stones in the cake, Cars flow beautifully; syntactic, pragmatic, syntactic, semantic, lexical",
        "question_options": null,
        "A_chat_id": 90769,
        "B_chat_id": 90770,
        "A": "The glass broke its leg - Syntactic\nI no go rain - Syntactic\nThe cook put cherry stones in the cake - Semantic\nCars flow beautifully - Pragmatic",
        "B": "1. \"The glass broke its leg\" - This sentence can be considered correct up to the semantic level. At the syntactic level, the sentence is grammatically correct, but at the semantic level, it is not accurate as glasses do not have legs to break.\n\n2. \"I no go rain\" - This sentence can be considered correct up to the syntactic level. It follows a grammatical structure commonly used in certain dialects of English (e.g., African American Vernacular English or Caribbean English). However, at the semantic level, it may not be accurate in standard English as it is missing the verb \"will\" to form a complete sentence in standard English.\n\n3. \"The cook put cherry stones in the cake\" - This sentence can be considered correct up to the pragmatic level. At the syntactic level, the sentence is grammatically correct. At the semantic level, it is also accurate. However, at the pragmatic level, it raises questions about the cook's intentions or actions (why would someone put cherry stones in a cake?).\n\n4. \"Cars flow beautifully\" - This sentence can be considered correct up to the lexical level. At the syntactic level, the sentence is grammatically correct. At the semantic level, it is also accurate. However, at the lexical level, the verb \"flow\" may not be the most appropriate choice for describing the movement of cars, as it conveys a sense of fluidity rather than the movement of vehicles on roads."
    },
    {
        "course_id": 0,
        "question_id": 6096,
        "question": "Which of the following provides forward secrecy.",
        "question_options": [
            "Transferring keys in plain.",
            "Ephemeral Diffie-Hellman.",
            "Semi-static Diffie-Hellman.",
            "Static Diffie-Hellman."
        ],
        "A_chat_id": 90771,
        "B_chat_id": 90772,
        "A": "Ephemeral Diffie-Hellman provides forward secrecy. Ephemeral Diffie-Hellman involves generating new key pairs for each session, ensuring that even if a long-term private key is compromised, past communications remain secure. Static Diffie-Hellman and Semi-static Diffie-Hellman do not provide forward secrecy as they use the same key for multiple sessions. Transferring keys in plain also does not provide forward secrecy as the keys can be intercepted and used to decrypt past communications.",
        "B": "Ephemeral Diffie-Hellman provides forward secrecy. It generates a new set of keys for each session, ensuring that if one session key is compromised, it does not affect the security of past or future sessions."
    },
    {
        "course_id": 0,
        "question_id": 5879,
        "question": "Let $n$ be an integer. Which of the following is \\emph{not} a group in the general case?",
        "question_options": [
            "$(\\mathbf{R},+)$",
            "$(\\mathbf{Q}\\setminus \\{0\\},\\times)$",
            "$(\\mathbf{Z}_n,+ \\pmod{n})$",
            "$(\\mathbf{Z}_n,\\times \\pmod{n})$"
        ],
        "A_chat_id": 90773,
        "B_chat_id": 90774,
        "A": "$(\\mathbf{Q}\\setminus \\{0\\},\\times)$ is not a group in the general case. While it is closed under multiplication and has an identity element (1), it does not satisfy the property of having inverses for all elements.",
        "B": "To determine which of the given sets are not groups, we need to check the group properties for each set:\n\n1. For a set to form a group under addition, it needs to satisfy the following properties:\n   a. Closure (the sum of two elements in the set is also in the set),\n   b. Associativity,\n   c. Identity element (0 in this case),\n   d. Inverse element (the additive inverse of each element exists in the set).\n\n2. For a set to form a group under multiplication, it needs to satisfy the following properties:\n   a. Closure (the product of two elements in the set is also in the set),\n   b. Associativity,\n   c. Identity element (1 in this case),\n   d. Inverse element (the multiplicative inverse of each element exists in the set).\n\nLet's check each set in turn:\n\n1. $(\\mathbf{R},+)$:\n   - This set forms a group under addition since it satisfies all the group properties.\n\n2. $(\\mathbf{Q}\\setminus \\{0\\},\\times)$:\n   - This set forms a group under multiplication since it satisfies all the group properties.\n\n3. $(\\mathbf{Z}_n,+ \\pmod{n})$:\n   - This set forms a group under addition modulo $n$ if and only if $n$ is a prime number. For composite $n$, closure under addition modulo $n$ may not hold, so this may not always be a group.\n\n4. $(\\mathbf{Z}_n,\\times \\pmod{n})$:\n   - This set forms a group under multiplication modulo $n$ if and only if $n$ is a prime number. For composite $n$, closure under multiplication modulo $n$ may not hold, so this may not always be a group.\n\nTherefore, the sets $(\\mathbf{Z}_n,+ \\pmod{n})$ and $(\\mathbf{Z}_n,\\times \\pmod{n})$ are not groups in the general case unless $n$ is a prime number."
    },
    {
        "course_id": 15008,
        "question_id": 2746,
        "question": "how can the results from a classifier impact the metric (precision) used? What could be a better suited metric to use with imbalanced data?",
        "question_options": null,
        "A_chat_id": 90776,
        "B_chat_id": 90777,
        "A": "The results from a classifier can impact the precision metric when dealing with imbalanced data because precision is sensitive to the number of false positives. In the case of imbalanced data, where one class is much more prevalent than the other, precision can be skewed by the high number of true negatives. This can lead to a high precision but low recall, which may not accurately reflect the classifier's performance.\n\nA better suited metric to use with imbalanced data is the F1 score, which takes into account both precision and recall. The F1 score is the harmonic mean of precision and recall, providing a balanced evaluation of the classifier's performance on imbalanced data. Other metrics such as the area under the precision-recall curve (AUC-PR) or the Matthews correlation coefficient (MCC) are also commonly used with imbalanced data to provide a more comprehensive evaluation of the classifier's performance.",
        "B": "When the results from a classifier are imbalanced, meaning one class is significantly more prevalent than the other, it can impact the precision metric. Precision is calculated as the number of true positive predictions divided by the total number of positive predictions. In imbalanced data, even if the classifier correctly identifies a few positive instances (true positives), the large number of negative instances can overwhelm the true positives, leading to a low precision value.\n\nIn such cases, where imbalanced data is present, using precision alone may not provide a clear picture of the classifier's performance. It is important to consider other metrics that can provide a more comprehensive evaluation. One such metric is the F1 score, which is the harmonic mean of precision and recall. The F1 score provides a balance between precision and recall, making it a better suited metric for imbalanced data.\n\nAdditionally, metrics such as ROC-AUC (Receiver Operating Characteristic - Area Under the Curve) and PR-AUC (Precision-Recall Area Under the Curve) can also be more suitable for evaluating classifiers on imbalanced data. These metrics take into account the trade-off between true positive rate and false positive rate, providing a more holistic assessment of the classifier's performance.\n\nIn summary, when dealing with imbalanced data, it is important to consider metrics beyond just precision, such as F1 score, ROC-AUC, and PR-AUC, to get a more accurate evaluation of the classifier's performance."
    },
    {
        "course_id": 15003,
        "question_id": 7345,
        "question": "Design an algorithm that implements consensus using multiple TRB instances.",
        "question_options": null,
        "A_chat_id": 90778,
        "B_chat_id": 90779,
        "A": "To implement consensus using multiple TRB instances, we can use the following algorithm:\n\n1. Initialize a set of TRB instances, each with its own unique identifier.\n\n2. Each TRB instance will have a list of proposals from different nodes in the system.\n\n3. Nodes in the system can submit their proposals to any TRB instance.\n\n4. Each TRB instance will run a consensus algorithm (such as Paxos or Raft) to reach an agreement on a single proposal.\n\n5. Once a TRB instance has reached a decision on a proposal, it will broadcast the decision to all nodes in the system.\n\n6. If multiple TRB instances reach different decisions, a higher-level consensus algorithm can be used to reach an agreement between the TRB instances.\n\n7. Once a global decision is reached, it can be considered as the final consensus for the system.\n\nBy using multiple TRB instances and a higher-level consensus algorithm, we can achieve distributed consensus in a robust and fault-tolerant manner.",
        "B": "Here is a high-level algorithm for achieving consensus using multiple TRB (Total Order Broadcast) instances:\n\n1. Initialize the system:\n   - Create multiple instances of TRB, each responsible for ordering messages in a specific domain or topic.\n   - Assign a unique identifier to each TRB instance.\n\n2. Send a message for ordering:\n   - When a process needs to send a message for ordering, it first selects the appropriate TRB instance based on the domain or topic of the message.\n   - The process then sends the message to the selected TRB instance for ordering.\n\n3. Order messages within each TRB instance:\n   - Each TRB instance receives messages from processes and orders them according to the total order broadcast protocol it implements.\n   - The TRB instance delivers the messages to all processes in the same order.\n\n4. Achieve consensus:\n   - To achieve consensus on a specific decision or value, processes can use the ordered messages from the corresponding TRB instances.\n   - By ensuring that all processes receive the messages in the same order, they can reach a consensus on the final decision or value.\n\n5. Handle failures:\n   - Implement fault tolerance mechanisms to handle failures in TRB instances or processes.\n   - Use techniques like leader election, replica synchronization, or Byzantine fault tolerance to ensure the system can recover from failures and maintain consensus.\n\n6. Scale the system:\n   - To scale the system, add more TRB instances to handle different domains or topics and distribute the workload evenly.\n   - Ensure that the communication between processes and TRB instances is efficient to handle a large number of messages.\n\nThis algorithm provides a framework for implementing consensus using multiple TRB instances. It leverages the properties of total order broadcast to ensure that messages are ordered consistently across processes, enabling them to reach consensus on decisions or values. Additional details and optimizations can be added based on the specific requirements of the system and the total order broadcast protocol being used."
    },
    {
        "course_id": 15011,
        "question_id": 4202,
        "question": "Write the dual of the following linear program: \\begin{align*} \\text{Maximize}  \\quad &6x_1 + 14 x_2 + 13 x_3\\\\ \\text{Subject to} \\quad & x_1 + 3x_2 + x_3 \\leq 24 \\\\ & x_1 + 2x_2 + 4 x_3 \\leq 60 \\\\ & x_1, x_2, x_3 \\geq 0 \\end{align*} Hint: How can you convince your friend that the above linear program has optimum value at most $z$?",
        "question_options": null,
        "A_chat_id": 90780,
        "B_chat_id": 90781,
        "A": "To write the dual of the given linear program, we first need to define the variables for the dual program. Let $y_1$ and $y_2$ be the dual variables corresponding to the first and second constraints respectively. \n\nThe dual of the given linear program is as follows:\n\\begin{align*}\n\\text{Minimize} \\quad &24y_1 + 60y_2 \\\\\n\\text{Subject to} \\quad & y_1 + y_2 \\geq 6 \\\\\n& 3y_1 + 2y_2 \\geq 14 \\\\\n& y_1 + 4y_2 \\geq 13 \\\\\n& y_1, y_2 \\geq 0\n\\end{align*}\n\nTo convince a friend that the above linear program has an optimum value at most $z$, we can use the Weak Duality Theorem. According to the Weak Duality Theorem, the optimal value of the primal linear program is always less than or equal to the optimal value of the dual linear program. Therefore, if we solve the dual linear program and obtain an optimal value of $z$, we can guarantee that the primal linear program has an optimum value at most $z$.",
        "B": "To write the dual of the given linear program, we need to follow these steps:\n\nStep 1: Write the primal linear program in standard form. The given primal problem is already in standard form.\n\nStep 2: Introduce dual variables for each constraint in the primal problem. Let $y_1$ and $y_2$ be the dual variables for the first and second constraints, respectively.\n\nStep 3: Write the dual problem by:\n- Minimizing the sum of the products of the dual variables and the right-hand sides of the primal constraints.\n- Subject to the coefficients of the dual variables in this sum being greater than or equal to the coefficients of the primal objective function.\n\nThe dual problem for the given primal problem is:\n\n\\begin{align*}\n\\text{Minimize} \\quad &24y_1 + 60y_2 \\\\\n\\text{Subject to} \\quad &y_1 + y_2 \\geq 6 \\\\\n&3y_1 + 2y_2 \\geq 14 \\\\\n&y_1 + 4y_2 \\geq 13 \\\\\n&y_1, y_2 \\geq 0\n\\end{align*}\n\nTo convince your friend that the primal linear program has an optimum value at most $z$, where $z$ is the optimum value of the dual linear program, you can use the Weak Duality Theorem. This theorem states that for any feasible solutions $(x_1, x_2, x_3)$ of the primal problem and $(y_1, y_2)$ of the dual problem, the following inequality holds:\n\n\\[ 6x_1 + 14 x_2 + 13 x_3 \\leq 24y_1 + 60y_2 \\]\n\nThis implies that the objective function value of the primal problem is at most the objective function value of the dual problem for any feasible solutions. Thus, the optimum value of the primal problem is at most the optimum value of the dual problem."
    },
    {
        "course_id": 15113,
        "question_id": 7052,
        "question": "A vehicle is in linear motion. The scalar velocity as a function of time is given by\n\n\\begin{equation*}\nv(t) = b_0t^2 -\\,a_0t\\ ,\n\\end{equation*}\n\nwhere $b_0$ and $a_0$ are positive constants. Determine the average velocity $\\bar{v}(t)$ at time $t$.\n",
        "question_options": null,
        "A_chat_id": 90782,
        "B_chat_id": 90783,
        "A": "The average velocity at time t is given by the formula:\n\n\\begin{equation*}\n\\bar{v}(t) = \\frac{1}{t} \\int_{0}^{t} v(\\tau) d\\tau\n\\end{equation*}\n\nSubstitute the given velocity function into the formula:\n\n\\begin{equation*}\n\\bar{v}(t) = \\frac{1}{t} \\int_{0}^{t} (b_0\\tau^2 - a_0\\tau) d\\tau\n\\end{equation*}\n\n\\begin{equation*}\n\\bar{v}(t) = \\frac{1}{t} \\left[\\frac{b_0}{3}\\tau^3 - \\frac{a_0}{2}\\tau^2\\right]_{0}^{t}\n\\end{equation*}\n\n\\begin{equation*}\n\\bar{v}(t) = \\frac{1}{t} \\left(\\frac{b_0}{3}t^3 - \\frac{a_0}{2}t^2\\right - \\left(\\frac{b_0}{3} \\cdot 0^3 - \\frac{a_0}{2} \\cdot 0^2\\right)\n\\end{equation*}\n\n\\begin{equation*}\n\\bar{v}(t) = \\frac{1}{t} \\left(\\frac{b_0}{3}t^3 - \\frac{a_0}{2}t^2\\right)\n\\end{equation*}\n\n\\begin{equation*}\n\\bar{v}(t) = \\frac{b_0}{3}t^2 - \\frac{a_0}{2}t\n\\end{equation*} \n\nTherefore, the average velocity at time t is $\\bar{v}(t) = \\frac{b_0}{3}t^2 - \\frac{a_0}{2}t$.",
        "B": "The average velocity $\\bar{v}(t)$ of an object over a given time interval is given by the formula:\n\n\\[\\bar{v}(t) = \\frac{\\Delta x}{\\Delta t}\\]\n\nwhere $\\Delta x$ is the change in position of the object during the time interval $\\Delta t$. \n\nGiven that the object is in linear motion and the scalar velocity function is given by $v(t) = b_0t^2 - a_0t$, we can find the change in position $\\Delta x$ by integrating the velocity function over the time interval from 0 to $t$:\n\n\\[\\Delta x = \\int_0^t v(t) dt = \\int_0^t (b_0t^2 - a_0t) dt\\]\n\n\\[= \\left[\\frac{b_0}{3}t^3 - \\frac{a_0}{2}t^2\\right]_0^t\\]\n\n\\[= \\frac{b_0}{3}t^3 - \\frac{a_0}{2}t^2\\]\n\nTherefore, the average velocity $\\bar{v}(t)$ at time $t$ is:\n\n\\[\\bar{v}(t) = \\frac{\\Delta x}{\\Delta t} = \\frac{\\frac{b_0}{3}t^3 - \\frac{a_0}{2}t^2}{t} = \\frac{b_0}{3}t^2 - \\frac{a_0}{2}t\\]\n\nSo, the average velocity at time $t$ is $\\bar{v}(t) = \\frac{b_0}{3}t^2 - \\frac{a_0}{2}t$."
    },
    {
        "course_id": 15002,
        "question_id": 1029,
        "question": "You have been publishing a daily column for the Gazette over the last few years and have recently reached a milestone --- your 1000th column! Realizing you'd like to go skiing more often, you decide it might be easier to automate your job by training a story generation system on the columns you've already written. Then, whenever your editor pitches you a title for a column topic, you'll just be able to give the title to your story generation system, produce the text body of the column, and publish it to the website!\nYour column generation system has become quite successful and you've managed to automate most of your job simply by typing your editor's title pitches into your model to produce your column every day. Two years later, during the COVID--25 pandemic, your editor proposes to use your system to generate an information sheet about the pandemic for anyone looking for information about symptoms, treatments, testing sites, medical professionals, etc. Given the similarity to a previous pandemic many years before, COVID--19, you train your model on all news articles published about COVID--19 between the years of 2019--2022. Then, you generate the information page from your trained model.\nGive an example of a potential harm that your model could produce from the perspective of human interaction harms.",
        "question_options": null,
        "A_chat_id": 90784,
        "B_chat_id": 90785,
        "A": "One potential harm that could arise from using a story generation system trained on past columns to generate information about the COVID-19 pandemic is the dissemination of inaccurate or outdated information. While the model may be able to generate text based on the patterns it has learned from past articles, it may not be able to distinguish between reliable and unreliable sources of information. This could lead to the spread of misinformation or outdated facts, potentially causing confusion or harm to individuals seeking accurate information about the pandemic. It is important to ensure that any information generated by the model is carefully reviewed and verified by human experts before being shared with the public to mitigate these risks.",
        "B": "One potential harm that could arise from relying solely on an automated story generation system for generating content is the lack of human empathy and understanding in the generated text. While the system may be able to produce informative and well-structured content based on the input data, it may not fully grasp the nuances of human emotions, experiences, and sensitivities.\n\nIn the context of a pandemic information sheet, this could manifest as a lack of empathy towards the struggles and fears that individuals may be facing during such a crisis. The generated text may come across as cold, clinical, or detached, failing to provide the comfort, reassurance, and support that a human writer could offer in times of distress.\n\nAdditionally, without human oversight and intervention, the automated system may inadvertently generate inaccurate or misleading information, leading to confusion, mistrust, or even harm to individuals seeking reliable guidance during a public health emergency.\n\nTherefore, it is crucial to strike a balance between automation and human involvement in content generation, especially when dealing with sensitive topics that require a nuanced understanding of human emotions and interactions."
    },
    {
        "course_id": 15011,
        "question_id": 4211,
        "question": "Suppose that Alice and Bob have two documents $d_A$ and $d_B$ respectively, and Charlie wants to learn about the difference between them. We represent each document by its word frequency vector as follows. We assume that words in $d_A$ and $d_B$ come from some dictionary of size $n$, and let $x\\in \\mathbb{R}^n$ be a vector such that for every word $i\\in [n]$\\footnote{We let $[n]:=\\{1,2,\\ldots, n\\}$.} the entry $x_i$ equals the number of times the $i$-th word in the dictionary occurs in $d_A$. Similarly, let $y\\in \\mathbb{R}^n$ be a vector such that for every word $i\\in [n]$ the entry $y_i$ denotes the number of times the $i$-th word in the dictionary occurs in $d_B$. We assume that the number of words in each document is bounded by a polynomial in $n$. Suppose that there exists $i^*\\in [n]$ such that for all $i\\in [n]\\setminus \\{i^*\\}$ one has $|x_i-y_i|\\leq 2$, and for $i^*$ one has $|x_{i^*}-y_{i^*}|\\geq n^{1/2}$.  Show that Alice and Bob can each send a $O(\\log^2 n)$-bit message to Charlie, from which Charlie can recover the identity of the special word $i^*$. Your solution must succeed with probability at least $9/10$. You may assume that Alice, Bob and Charlie have a source of shared random bits.",
        "question_options": null,
        "A_chat_id": 90786,
        "B_chat_id": 90787,
        "A": "To solve this problem, Alice and Bob can use the following protocol:\n\n1. Alice and Bob randomly select a subset of $O(\\log^2 n)$ words from their respective documents $d_A$ and $d_B$ and compute the sum of the frequencies of these selected words. Let $s_A$ be the sum of frequencies of the selected words in $d_A$ and $s_B$ be the sum of frequencies of the selected words in $d_B$.\n\n2. Alice and Bob share the values of $s_A$ and $s_B$ with Charlie.\n\n3. Charlie computes the difference $s_A - s_B$ and checks if the absolute value of this difference is at least $n^{1/2}$. If it is, then Charlie concludes that the special word $i^*$ is in the subset of selected words. If not, Charlie concludes that the special word $i^*$ is not in the subset.\n\n4. To ensure that the protocol succeeds with probability at least $9/10$, Alice and Bob can repeat the protocol multiple times with different randomly selected subsets of words. By repeating the protocol $O(\\log n)$ times, the probability of failure decreases exponentially, ensuring the desired success probability.\n\n5. By using $O(\\log^2 n)$-bit messages for each repetition of the protocol, Alice and Bob can send a total of $O(\\log^2 n)$ bits to Charlie to enable him to recover the identity of the special word $i^*$.",
        "B": "To solve this problem, we can use the following protocol:\n\n1. Alice and Bob each compute the dot product of their word frequency vectors with a randomly chosen vector $r\\in\\{-1,1\\}^n$. Let $a = \\langle x, r \\rangle$ and $b = \\langle y, r \\rangle$. They send the values $a$ and $b$ to Charlie.\n\n2. Charlie computes the dot product of $x$ and $y$ to obtain $z = \\langle x, y \\rangle$.\n\n3. Charlie then computes $c = \\text{sign}(a - b)z$. If $c > 0$, he outputs \"Alice's word is special\"; if $c < 0$, he outputs \"Bob's word is special\".\n\nLet's analyze this protocol:\n\n- If $i^* = i$, then $|x_i - y_i| \\geq n^{1/2}$, and hence, with high probability, $|a - b| = |\\langle x - y, r \\rangle| \\geq n^{1/2}$. Therefore, $c = \\text{sign}(a - b)z$ will be positive, and Charlie will correctly identify the special word.\n\n- If $i \\neq i^*$, then $|x_i - y_i| \\leq 2$ implies $|a - b| = |\\langle x - y, r \\rangle| \\leq 2\\|r\\|_1 \\leq 2n$. In this case, $c = 0$, and Charlie will not make any incorrect determinations.\n\nNow, we need to analyze the error probability of this protocol. Let $p$ be the probability that Charlie outputs an incorrect result. This happens when $c = 0$ but the special word is selected. Thus, we have:\n\n\\[\np = \\mathbb{P}(c = 0 \\mid i^* = i) = \\mathbb{P}(|a - b| \\leq 2)\n\\]\n\nBy Hoeffding's inequality, we have:\n\n\\[\n\\mathbb{P}(|a - b| \\geq t) \\leq 2\\exp\\left(-\\frac{2t^2}{n}\\right)\n\\]\n\nSetting $t = 2$, we find:\n\n\\[\np \\leq 2\\exp\\left(-\\frac{8}{n}\\right)\n\\]\n\nIn order to ensure that the error probability is at most $1/10$, we need:\n\n\\[\n2\\exp\\left(-\\frac{8}{n}\\right) \\leq \\frac{1}{10} \\implies \\exp\\left(-\\frac{8}{n}\\right) \\leq \\frac{1}{20}\n\\]\n\nSolving this inequality gives $n \\geq 40\\log(20) \\approx 180$. Therefore, we can take $n = 256$ for simplicity.\n\nThe communication complexity of this protocol is $O(\\log n)$ bits, as both $a$ and $b$ can be represented with $O(\\log n)$ bits. Since $n = 256$, the communication complexity is $O(\\log 256) = O(8) = O(\\log^2 n)$ bits.\n\nThus, Alice and Bob can each send $O(\\log^2 n)$ bits to Charlie, who can then recover the identity of the special word $i^*$ with probability at least $9/10$."
    },
    {
        "course_id": 0,
        "question_id": 5654,
        "question": "Tick the \\textbf{false} statement.",
        "question_options": [
            "Cryptographic primitives used in Bluetooth are provably secure.",
            "In WEP, authentication is done with the pre-shared keys.",
            "The security of Bluetooth 2.0 pairing is based on PIN.",
            "Due to memory limitations, dummy devices can share the same key with everyone."
        ],
        "A_chat_id": 90788,
        "B_chat_id": 90789,
        "A": "The false statement is: Cryptographic primitives used in Bluetooth are provably secure. While Bluetooth does use cryptographic primitives for security, it is not guaranteed that these primitives are provably secure. The other statements are true: authentication in WEP is done with pre-shared keys, Bluetooth 2.0 pairing security is based on PIN, and due to memory limitations, dummy devices can share the same key with everyone.",
        "B": "[ ] Cryptographic primitives used in Bluetooth are provably secure."
    },
    {
        "course_id": 15115,
        "question_id": 7304,
        "question": "You heard about electrons and point charges in the lecture. Explain in your own words what an electron and what a point charge is and what their respective properties are. What is the difference and what are the similarities of electrons and point charges?\n",
        "question_options": null,
        "A_chat_id": 90790,
        "B_chat_id": 90791,
        "A": "An electron is a subatomic particle that carries a negative charge. It is a fundamental component of atoms and plays a crucial role in various physical and chemical processes. Electrons have a mass of approximately 9.11 x 10^-31 kg and are located in the outer shells of atoms.\n\nA point charge, on the other hand, is a theoretical concept used in physics to represent a particle or object with a specific amount of charge at a single point in space. Point charges are used to simplify calculations in electromagnetism and other areas of physics.\n\nThe main similarity between electrons and point charges is that they both carry an electric charge. However, electrons have a specific mass and are physical particles with a defined size, while point charges are considered to be infinitely small and do not have a mass.\n\nIn summary, electrons are physical particles with mass and charge, while point charges are theoretical concepts used to simplify calculations in physics.",
        "B": "An electron is a fundamental subatomic particle that carries a negative electric charge. It is a component of an atom and orbits the nucleus. Electrons are incredibly small and have a mass of about 1/1836th of a proton. They play a crucial role in determining the chemical and physical properties of matter.\n\nA point charge, on the other hand, is a theoretical concept used in physics to represent a charged object that has no size or volume, essentially concentrated at a single point. It helps simplify calculations in electrostatics and allows us to understand the behavior of charged particles in an electric field.\n\nThe main similarity between electrons and point charges is that they both carry electric charge. Electrons are actual particles with mass and volume, while point charges are mathematical constructs with no physical size. Both electrons and point charges interact with each other through the electromagnetic force, leading to the creation of electric fields and the phenomenon of attraction or repulsion between charges.\n\nThe main difference between the two is that electrons are real particles with mass and other intrinsic properties, while point charges are idealized mathematical constructs used for simplification in physics calculations. Electrons are also subject to the laws of quantum mechanics, while point charges are part of classical physics models.\n\nIn summary, electrons are actual subatomic particles with mass and volume, while point charges are theoretical constructs with no physical size, used to simplify calculations in physics. Both carry electric charge and interact through electromagnetic forces, but their nature and properties are fundamentally different."
    },
    {
        "course_id": 15002,
        "question_id": 961,
        "question": "Consider the following context-free grammar \\(G\\) (where \\(\\text{S}\\) is the top-level symbol):\n\n\\(R_{01}: \\text{S} \\rightarrow \\text{NP VP}\\)\n\\(R_{02}: \\text{NP} \\rightarrow \\text{NP0}\\)\n\\(R_{03}: \\text{NP} \\rightarrow \\text{Det NP0}\\)\n\\(R_{04}: \\text{NP0} \\rightarrow \\text{N}\\)\n\\(R_{05}: \\text{NP0} \\rightarrow \\text{Adj N}\\)\n\\(R_{06}: \\text{NP0} \\rightarrow \\text{NP0 PNP}\\)\n\\(R_{07}: \\text{VP} \\rightarrow \\text{V}\\)\n\\(R_{08}: \\text{VP} \\rightarrow \\text{V NP}\\)\n\\(R_{09}: \\text{VP} \\rightarrow \\text{V NP PNP}\\)\n\\(R_{10}: \\text{PNP} \\rightarrow \\text{Prep NP}\\)\n\ncomplemented by the lexicon \\(L\\):\na        : Det\nblue     : Adj, N\ndrink    : N, V\ndrinks   : N, V\nfriends  : N\nfrom     : Prep\ngave     : V\nletter   : N\nmy       : Det\nneighbor : N\nnice     : Adj, N\nof       : Prep\npostman  : N\nran      : V\nthe      : Det\nto       : PrepHow many (syntactic and lexical) rules does the extended Chomsky Normal Form grammar equivalent to \\(G\\) contain, if produced as described in the parsing lecture?",
        "question_options": [
            "the grammar \\(G\\) cannot be converted to extended Chomsky Normal Form",
            "the grammar \\(G\\) already is in extended Chomsky Normal Form",
            "11 rules",
            "31 rules",
            "48 rules"
        ],
        "A_chat_id": 90792,
        "B_chat_id": 90793,
        "A": "To convert the given context-free grammar \\(G\\) to extended Chomsky Normal Form, we need to follow these steps:\n\n1. Eliminate \u03b5-rules: There are no \u03b5-rules in this grammar.\n2. Eliminate unit rules: There are no unit rules in this grammar.\n3. Convert to Chomsky Normal Form:\n- Replace terminals in rules with new non-terminal symbols.\n- If a rule has more than 2 non-terminals, introduce new non-terminal symbols.\n\nAfter converting the grammar to Chomsky Normal Form, we need to count the number of rules in the resulting grammar. \n\nThe given grammar \\(G\\) consists of 10 syntactic rules and 18 lexical rules, totaling to 28 rules. Additionally, after converting to extended Chomsky Normal Form, we introduce new rules for each terminal in the lexicon, resulting in a total of 31 rules. \n\nTherefore, the extended Chomsky Normal Form equivalent to grammar \\(G\\) contains 31 rules.",
        "B": "To convert the given context-free grammar \\(G\\) into the extended Chomsky Normal Form (CNF), we need to eliminate any rules that are not in the CNF and introduce new rules as needed. The extended CNF allows rules of the form \\(A \\rightarrow BC\\) where \\(A, B, C\\) are non-terminal symbols, as well as rules of the form \\(A \\rightarrow a\\) where \\(A\\) is a non-terminal symbol and \\(a\\) is a terminal symbol.\n\nFirst, let's identify the rules that are not in CNF in the given grammar \\(G\\):\n\n1. Rule \\(R_{03}: \\text{NP} \\rightarrow \\text{Det NP0}\\) is not in CNF because it has a production involving multiple non-terminals.\n\nTo convert this rule into CNF, we need to introduce a new non-terminal symbol for the combination \\(\\text{Det NP0}\\). Let's call this new non-terminal symbol \\(X_1\\). We then introduce a new rule:\n\n\\(R_{11}: \\text{NP} \\rightarrow X_1\\)\n\\(R_{12}: X_1 \\rightarrow \\text{Det NP0}\\)\n\nNext, let's check the lexicon \\(L\\) to see if any modifications are needed:\n\n- The lexicon \\(L\\) seems to be already in CNF as each terminal symbol is associated with a single non-terminal symbol.\n\nNow, let's count the total number of syntactic and lexical rules in the extended CNF grammar:\n\n- The original grammar \\(G\\) has 10 syntactic rules.\n- After converting \\(R_{03}\\) into CNF, we have introduced 2 new syntactic rules.\n- The lexicon \\(L\\) contains 16 terminal symbols associated with non-terminal symbols.\n\nTherefore, the total number of rules in the extended CNF grammar equivalent to \\(G\\) is \\(10 + 2 + 16 = 28\\).\n\nSo, the correct answer is:\n[ ] 31 rules"
    },
    {
        "course_id": 15116,
        "question_id": 7224,
        "question": "Determine the lines of current $y = y(x)$ and the acceleration $\\vec{a}(x,y)$ for a stationary bidimensional flow described by the velocity field :\n\\begin{equation*} \n{\\bf v} = (\\frac{{v_0 }}{l})(x\\vec e_x  - y\\vec e_y)~.\n\\end{equation*}\n",
        "question_options": null,
        "A_chat_id": 90794,
        "B_chat_id": 90795,
        "A": "To determine the lines of current, we can use the fact that the velocity field is tangent to the lines of current. This means that the velocity vector is parallel to the tangent vector of the lines of current. \n\nGiven the velocity field ${\\bf v} = (\\frac{{v_0 }}{l})(x\\vec e_x  - y\\vec e_y)$, we can write the tangent vector to the lines of current as $\\vec{T} = (dx, dy)$. \n\nSince the velocity field is parallel to the tangent vector, we have:\n\n$$\\frac{{v_0 }}{l}(x, -y) = k(dx, dy)$$\n\nwhere k is a constant of proportionality. This gives us the following system of equations:\n\n$$\\frac{{v_0 }}{l}x = kdx$$\n$$\\frac{{v_0 }}{l}(-y) = kdy$$\n\nFrom the first equation, we get:\n\n$$\\frac{dx}{x} = \\frac{v_0}{lk}dx$$\n\nIntegrating both sides gives:\n\n$$\\ln|x| = \\frac{v_0}{lk}x + C_1$$\n\nSolving for y in the second equation, we get:\n\n$$\\frac{dy}{y} = -\\frac{v_0}{lk}dx$$\n\nIntegrating both sides gives:\n\n$$\\ln|y| = -\\frac{v_0}{lk}x + C_2$$\n\nExponentiating both equations gives the equations of the lines of current:\n\n$$y = Ce^{-\\frac{v_0}{lk}x}$$\n\nwhere C is a constant. \n\nTo determine the acceleration vector $\\vec{a}(x,y)$, we can differentiate the velocity field with respect to time to get the acceleration field. \n\nThe acceleration field is given by:\n\n$$\\vec{a}(x,y) = \\frac{d{\\bf v}}{dt} = \\left(\\frac{v_0}{l}\\frac{d}{dt}(x\\vec{e_x}) - \\frac{v_0}{l}\\frac{d}{dt}(y\\vec{e_y})\\right)$$\n\nSince the flow is stationary, the acceleration is zero and we have:\n\n$$\\vec{a}(x,y) = 0$$\n\nTherefore, the acceleration vector for this stationary bidimensional flow is zero.",
        "B": "To determine the lines of current, we need to find the streamlines of the flow. The streamlines are given by the equation:\n\\begin{equation*}\n\\frac{{dx}}{v_x} = \\frac{{dy}}{v_y} = \\frac{{dt}}{v}~,\n\\end{equation*}\nwhere $v_x$ and $v_y$ are the components of velocity vector $\\textbf{v}$ and $v$ is the magnitude of the velocity vector.\n\nFrom the given velocity field:\n\\begin{align*}\nv_x &= \\frac{{v_0}}{l}x \\\\\nv_y &= -\\frac{{v_0}}{l}y \\\\\nv &= \\sqrt{v_x^2 + v_y^2} = \\frac{{v_0}}{l}\\sqrt{x^2 + y^2}~.\n\\end{align*}\n\nNow, we can write the streamline equation as:\n\\begin{equation*}\n\\frac{{dx}}{{\\frac{{v_0}}{l}x}} = \\frac{{dy}}{{-\\frac{{v_0}}{l}y}} = \\frac{{dt}}{{\\frac{{v_0}}{l}\\sqrt{x^2 + y^2}}}~.\n\\end{equation*}\n\nSolving the first part of the equation gives:\n\\begin{equation*}\n\\frac{{dx}}{{x}} = -\\frac{{dy}}{{y}} \\implies \\frac{{dy}}{{y}} = -\\frac{{dx}}{{x} } \\implies \\ln{|y|} = -\\ln{|x|} + C_1~,\n\\end{equation*}\nwhere $C_1$ is a constant of integration.\n\nThis simplifies to:\n\\begin{equation*}\n\\ln{\\left|\\frac{{y}}{{x}}\\right|} = C_1~.\n\\end{equation*}\n\nTherefore, the lines of current are given by:\n\\begin{equation*}\ny = cx~,\n\\end{equation*}\nwhere $c$ is an arbitrary constant.\n\nNext, we need to calculate the acceleration $\\vec{a}(x,y)$ for the given flow field. The acceleration is given by the rate of change of velocity with respect to time, i.e.,\n\\begin{equation*}\n\\vec{a}(x,y) = \\frac{{d\\vec{v}}}{{dt}}~.\n\\end{equation*}\n\nWe already have the expression for velocity $\\textbf{v}$, so we can differentiate with respect to time to find the acceleration. Let's find the acceleration components $a_x$ and $a_y$:\n\\begin{align*}\n\\frac{{d\\textbf{v}}}{{dt}} &= \\left(\\frac{{dv_x}}{{dt}}\\right)\\vec{e}_x + \\left(\\frac{{dv_y}}{{dt}}\\right)\\vec{e}_y \\\\\n&= \\left(\\frac{{d}}{{dt}}\\left(\\frac{{v_0}}{l}x\\right)\\right)\\vec{e}_x + \\left(\\frac{{d}}{{dt}}\\left(-\\frac{{v_0}}{l}y\\right)\\right)\\vec{e}_y \\\\\n&= \\left(\\frac{{v_0}}{l}\\frac{{dx}}{{dt}}\\right)\\vec{e}_x + \\left(-\\frac{{v_0}}{l}\\frac{{dy}}{{dt}}\\right)\\vec{e}_y \\\\\n&= \\left(\\frac{{v_0}}{l}v_x\\right)\\vec{e}_x - \\left(\\frac{{v_0}}{l}v_y\\right)\\vec{e}_y \\\\\n&= \\left(v_x\\right)\\vec{e}_x - \\left(v_y\\right)\\vec{e}_y \\\\\n&= \\left(\\frac{{v_0}}{l}x\\right)\\vec{e}_x - \\left(-\\frac{{v_0}}{l}y\\right)\\vec{e}_y \\\\\n&= \\frac{{v_0}}{l}(x\\vec{e}_x + y\\vec{e}_y)~.\n\\end{align*}\n\nTherefore, the acceleration vector $\\vec{a}(x,y)$ for the given flow field is $\\vec{a}(x,y) = \\frac{{v_0}}{l}(x\\vec{e}_x + y\\vec{e}_y)$."
    },
    {
        "course_id": 15003,
        "question_id": 7348,
        "question": "Implement a uniform reliable broadcast algorithm without using any failure detector, i.e., using only BestEffort-Broadcast(BEB).",
        "question_options": null,
        "A_chat_id": 90796,
        "B_chat_id": 90797,
        "A": "One way to implement a uniform reliable broadcast algorithm without using any failure detector is as follows:\n\n1. Each process, upon receiving a message m, broadcasts m using BEB.\n2. Each process, upon delivering a message m from BEB, delivers m only once to its application.\n3. Upon delivering a message m from BEB, a process p delivers m to its application if and only if it has not already delivered m.\n\nThis algorithm ensures that all correct processes deliver the same set of messages in the same order, without using any failure detector.",
        "B": "To implement a uniform reliable broadcast algorithm using only BestEffort-Broadcast (BEB), we can use the following steps:\n\n1. **Initialization**: Each process will maintain three sets of variables:\n   - `Delivered`: Set of messages that have been delivered.\n   - `Pending`: Set of messages that have been broadcast but not yet delivered.\n   - `Acknowledged`: Set of acknowledgments received for messages.\n\n2. **Broadcast**: When a process wants to broadcast a message `m`, it follows these steps:\n   - Send the message `m` using BestEffort-Broadcast (BEB).\n   - Add `m` to the `Pending` set.\n\n3. **Deliver**: When a process receives a message `m`:\n   - Check if `m` has not been delivered before (not in `Delivered` set).\n   - Check if all processes have acknowledged `m` (by checking if `Acknowledged` contains acknowledgments from all processes).\n   - If both conditions are met, mark `m` as delivered by adding it to the `Delivered` set and deliver it to the upper layer.\n\n4. **Acknowledge**: When a process receives a message `m`:\n   - Send an acknowledgment for `m` to all other processes using BestEffort-Broadcast (BEB).\n   - Add the acknowledgment to the `Acknowledged` set for `m`.\n\nBy following these steps, we ensure that a message is delivered by a process only if it has been acknowledged by all other processes. This way, we achieve uniform reliability in the broadcast.\n\nIt's important to note that this algorithm assumes no process failures and relies on the properties of BestEffort-Broadcast. In a real distributed system with potential failures, additional mechanisms like failure detectors would be needed to handle failures and ensure reliability."
    },
    {
        "course_id": 0,
        "question_id": 6030,
        "question": "For two $\\ell$-bit numbers, the \\textbf{tightest} complexity to compute their gcd is",
        "question_options": [
            "$\\mathcal{O}(2^{\\ell})$.",
            "$\\mathcal{O}(\\ell^3)$.",
            "$\\mathcal{O}(\\ell^2)$.",
            "$\\mathcal{O}(\\log{\\ell}^2)$."
        ],
        "A_chat_id": 90798,
        "B_chat_id": 90799,
        "A": "The tightest complexity to compute the gcd of two $\\ell$-bit numbers is $\\mathcal{O}(\\ell^3)$. This is because the Euclidean algorithm, which is commonly used to compute the gcd, has a complexity of $\\mathcal{O}(\\log{AB})$, where A and B are the input numbers. Since the input numbers are $\\ell$-bits long, the complexity becomes $\\mathcal{O}(\\ell^3)$ in this case.",
        "B": "To compute the gcd of two $\\ell$-bit numbers, we typically use the Euclidean algorithm. The Euclidean algorithm has a time complexity that is logarithmic in the input size. \n\nThe input size for two $\\ell$-bit numbers is $2\\ell$. Therefore, the tightest complexity to compute their gcd is $\\mathcal{O}(\\log{2\\ell}) = \\mathcal{O}(\\log{\\ell})$. \n\nTherefore, the correct choice is:\n[ ] $\\mathcal{O}(\\log{\\ell}^2)$."
    },
    {
        "course_id": 15002,
        "question_id": 4948,
        "question": "In an automated email router of a company, we want to make the distinction between three kind of\nemails: technical (about computers), financial, and the rest ('irrelevant'). For this we plan to use a\nNaive Bayes approach.\nWhat is the main assumption made by Naive Bayes classifiers? Why is it 'Naive'?\n\nWe will consider the following three messages:\n\nThe Dow industrials tumbled 120.54 to 10924.74, hurt by GM's sales forecast\nand two economic reports. Oil rose to $71.92.\n\nBitTorrent Inc. is boosting its network capacity as it prepares to become a centralized hub for legal video content. In May, BitTorrent announced a deal with\nWarner Brothers to distribute its TV and movie content via the BT platform. It\nhas now lined up IP transit for streaming videos at a few gigabits per second\n\nIntel will sell its XScale PXAxxx applications processor and 3G baseband processor businesses to Marvell for $600 million, plus existing liabilities. The deal\ncould make Marvell the top supplier of 3G and later smartphone processors, and\nenable Intel to focus on its core x86 and wireless LAN chipset businesses, the\ncompanies say.\n\nSuppose we have collected the following statistics $3^{3}$ about the word frequencies within the corresponding classes, where '0.00...' stands for some very small value:\n\\begin{center}\n\\begin{tabular}{|c|c|c|c|c|c|c|c|}\n\\hline\n & technical & financial & irrelevant &  & technical & financial & irrelevan \\\\\n\\hline\n$\\$<$ number $>$ & 0.01 & 0.07 & 0.05 & deal & 0.01 & 0.02 & $0.00 \\ldots$ \\\\\n\\hline\nDow & $0.00 \\ldots$ & 0.08 & $0.00 \\ldots$ & forecast & $0.00 \\ldots$ & 0.03 & 0.01 \\\\\n\\hline\nGM & $0.00 \\ldots$ & 0.03 & $0.00 \\ldots$ & gigabit & 0.03 & $0.00 \\ldots$ & $0.00 \\ldots$ \\\\\n\\hline\nIP & 0.03 & $0.00 \\ldots$ & $0.00 \\ldots$ & hub & 0.06 & $0.00 \\ldots$ & 0.01 \\\\\n\\hline\nIntel & 0.02 & 0.02 & $0.00 \\ldots$ & network & 0.04 & 0.01 & $0.00 \\ldots$ \\\\\n\\hline\nbusiness & 0.01 & 0.07 & 0.04 & processor & 0.07 & 0.01 & $0.00 \\ldots$ \\\\\n\\hline\ncapacity & 0.01 & $0.00 \\ldots$ & $0.00 \\ldots$ & smartphone & 0.04 & 0.04 & 0.01 \\\\\n\\hline\nchipset & 0.04 & 0.01 & $0.00 \\ldots$ & wireless & 0.02 & 0.01 & $0.00 \\ldots$ \\\\\n\\hline\ncompany & 0.01 & 0.04 & 0.05 & sen & re & . & . \\\\\n\\hline\n\\end{tabular}\n\\end{center}\n\nWe now want to specifically focus on the processing of compounds such as 'network capacity' in\nthe second text.\n\nOutline how you would build a pre-processor for compound words",
        "question_options": null,
        "A_chat_id": 90800,
        "B_chat_id": 90801,
        "A": "To build a pre-processor for compound words like 'network capacity', we can consider the following steps:\n\n1. Tokenization: Break down the text into individual words by tokenizing it. This will help us identify compound words as a sequence of tokens.\n\n2. Compound word identification: Look for sequences of tokens that frequently occur together and are likely to form compound words. In our example, 'network capacity' could be identified as a compound word based on the frequency of occurrence in the text.\n\n3. Compound word extraction: Once compound words are identified, extract them as single entities to preserve their meaning and context. In our example, 'network capacity' could be extracted as a single token representing the compound concept.\n\n4. Frequency analysis: Analyze the frequency of occurrence of compound words in different classes to understand their significance and relevance in classification.\n\n5. Feature engineering: Incorporate the extracted compound words as features in the Naive Bayes classifier to improve its performance in distinguishing between different classes. This will help capture the specific characteristics of compound words that contribute to the classification task.\n\nBy implementing these steps in the pre-processing stage, we can enhance the effectiveness of the Naive Bayes classifier in handling compound words and improving the accuracy of classification for text data.",
        "B": "The main assumption made by Naive Bayes classifiers is that features are independent of each other given the class label. This is why it is called \"naive\" because it assumes a strong independence between the features, which may not hold true in reality. Despite this simplifying assumption, Naive Bayes classifiers are known to perform well in practice, especially in text classification tasks.\n\nTo build a pre-processor for compound words like 'network capacity' in the second text, we can consider the following steps:\n\n1. Tokenization: Break down the text into individual words or tokens. This can be done using whitespace or punctuation as delimiters.\n\n2. Identification of Compound Words: Identify compound words in the text by looking for pairs or groups of words that commonly occur together. In the given example, 'network capacity' is a compound word.\n\n3. Handling Compound Words: Once compound words are identified, you can choose to treat them as separate entities or combine them into a single token. In this case, 'network capacity' can be considered as a single token.\n\n4. Frequency Calculation: Calculate the frequency of compound words within each class. You can use techniques like term frequency-inverse document frequency (TF-IDF) to weigh the importance of compound words in distinguishing between classes.\n\n5. Feature Extraction: Transform the text data into a format that can be used by the Naive Bayes classifier. This typically involves converting the text into a numerical representation, such as a bag-of-words model or TF-IDF matrix.\n\n6. Training the Naive Bayes Classifier: Train the classifier using the extracted features and the corresponding class labels. The classifier will learn the probability distribution of features given each class.\n\n7. Classification: Given a new text input, preprocess it using the same steps (tokenization, compound word identification, etc.) and then use the trained Naive Bayes classifier to predict the most likely class for the input text.\n\nBy following these steps and incorporating the handling of compound words like 'network capacity', you can improve the accuracy of the Naive Bayes classifier in distinguishing between different types of emails based on their content."
    },
    {
        "course_id": 0,
        "question_id": 5904,
        "question": "Tick the \\textit{wrong} assertion.",
        "question_options": [
            "If $\\Big(\\frac{b}{n}\\Big)=+$1 for $b \\in \\mathbb{Z}_n^* $, then $b$ is a quadratic residue.",
            "If $\\Big(\\frac{b}{p}\\Big)=+$1 for $b \\in \\mathbb{Z}_p^* $, then $b$ is a quadratic residue, where $p$ is prime.",
            "If $b$ is a quadratic residue for $b \\in \\mathbb{Z}_n^* $, then $\\Big(\\frac{b}{n}\\Big)=+1$.",
            "If $b$ is a quadratic residue for $b \\in \\mathbb{Z}_p^* $, then $\\Big(\\frac{b}{p}\\Big)=+1$, where $p$ is prime."
        ],
        "A_chat_id": 90802,
        "B_chat_id": 90803,
        "A": "The wrong assertion is: If $b$ is a quadratic residue for $b \\in \\mathbb{Z}_n^* $, then $\\Big(\\frac{b}{n}\\Big)=+1$. This statement is incorrect because the Legendre symbol $\\Big(\\frac{b}{n}\\Big)$ can be equal to -1 even if $b$ is a quadratic residue modulo $n$.",
        "B": "The wrong assertion is:\n[ ] If $b$ is a quadratic residue for $b \\in \\mathbb{Z}_n^* $, then $\\Big(\\frac{b}{n}\\Big)=+1$."
    },
    {
        "course_id": 0,
        "question_id": 6105,
        "question": "Let $E$ be an elliptic curve. Solving which of the following problems would help you to break Elliptic Curve Diffie-Hellman (ECDH) over $E$?",
        "question_options": [
            "Let $P,R \\in E$. Given $P$ and $P+R$, compute $R$.",
            "Let $P,Q \\in E$. Given $P$ and $Q$, compute the product between $P$ and $Q$, i.e., $P \\times Q$.",
            "Let $P \\in E$ and $\\ell \\in \\mathbb{N}$. Given $P$ and $\\ell P$, compute $\\ell$.",
            "Find a point which is not on the curve $E$."
        ],
        "A_chat_id": 90804,
        "B_chat_id": 90805,
        "A": "Finding a point which is not on the curve $E$ would help break Elliptic Curve Diffie-Hellman (ECDH) over $E$. If a point not on the curve is used in the key exchange process, it can lead to vulnerabilities and compromise the security of the communication.",
        "B": "The problem that would help break Elliptic Curve Diffie-Hellman (ECDH) over $E$ is:\n\n[ ] Let $P \\in E$ and $\\ell \\in \\mathbb{N}$. Given $P$ and $\\ell P$, compute $\\ell$.\n\nThis problem corresponds to the Discrete Logarithm Problem (DLP) on elliptic curves, which is the foundation of the security of ECDH. If an efficient algorithm could solve this problem, then the security of ECDH would be compromised."
    },
    {
        "course_id": 15116,
        "question_id": 7222,
        "question": "Determine the relation between pressure and height for a gas for : $T=T(z)$ but no exchange of heat in the gas (adiabatic atmosphere).\nEstimate the pressure at an altitude of 10 km. Consider the atmosphere as an ideal, biatomic gas of molecular mass 28.\n\nN.A.: R = 8.2 J/mol K ; T$_0$=293 K.\\\\\n",
        "question_options": null,
        "A_chat_id": 90806,
        "B_chat_id": 90807,
        "A": "In an adiabatic atmosphere with no exchange of heat, the relationship between pressure and height for a gas can be described by the barometric formula:\n\n$P(z) = P_0 * e^{-\\frac{Mgz}{RT}}$\n\nwhere:\n- P(z) is the pressure at height z\n- P$_0$ is the pressure at sea level\n- M is the molar mass of the gas\n- g is the acceleration due to gravity\n- R is the ideal gas constant\n- T is the temperature at height z\n- z is the height\n\nGiven that the atmosphere is considered as an ideal, biatomic gas with a molecular mass of 28, we can substitute the values into the formula.\n\nM = 28 g/mol\ng = 9.81 m/s$^2$\nR = 8.2 J/mol K\nT$_0$ = 293 K (temperature at sea level)\n\nAt an altitude of 10 km, the temperature T can be estimated using the standard lapse rate of 6.5 K/km. Therefore, T = T$_0$ - 6.5 * 10 = 293 - 65 = 228 K.\n\nSubstitute the values into the formula:\n\n$P(10 \\, km) = P_0 * e^{-\\frac{28 * 9.81 * 10000}{8.2 * 228}}$\n\n$P(10 \\, km) = P_0 * e^{-\\frac{274680}{1869.6}}$\n\n$P(10 \\, km) = P_0 * e^{-146.9}$\n\nSince we do not have a specific value for P$_0$, the pressure at 10 km altitude can be expressed in terms of P$_0$ as above.",
        "B": "To determine the relation between pressure and height for a gas in an adiabatic atmosphere, we can use the ideal gas law and the equation of state for an adiabatic process.\n\nThe ideal gas law is given by:\n\\[ PV = nRT \\]\nwhere:\n- P is the pressure,\n- V is the volume,\n- n is the number of moles,\n- R is the ideal gas constant,\n- T is the temperature.\n\nFor an adiabatic process, we have the relation:\n\\[ PV^{\\gamma} = \\text{constant} \\]\nwhere $\\gamma$ is the adiabatic index, which for a diatomic gas is approximately 7/5.\n\nWe also know that for an adiabatic process, the temperature changes with height according to $T=T(z)$.\n\nFrom the ideal gas law, we have:\n\\[ P = \\frac{nRT}{V} \\]\n\nSince $V$ is not constant in an adiabatic process, we can express the volume in terms of the height $z$ using the ideal gas law:\n\\[ V = \\frac{nRT}{P} \\]\n\nSubstitute this expression for $V$ into the adiabatic equation:\n\\[ P\\left(\\frac{nRT}{P}\\right)^{\\gamma} = \\text{constant} \\]\n\nSimplifying, we get:\n\\[ P^{1-\\gamma}T^{\\gamma} = \\text{constant} \\]\n\nSince $T=T(z)$, we can express $T$ in terms of $z$ and find the relation between pressure and height.\n\nNow, to estimate the pressure at an altitude of 10 km, we can use the relation we derived above. We need to know the temperature profile $T(z)$ in order to calculate the pressure at that altitude. Given the atmospheric conditions and the molecular mass, we can calculate the temperature profile using the ideal gas law and atmospheric conditions.\n\nLet's calculate the pressure at 10 km altitude by determining the temperature at that altitude first."
    },
    {
        "course_id": 4,
        "question_id": 6580,
        "question": "Which of the following apply to recent Android-based mobile systems but not\n    to Linux-based desktop systems?",
        "question_options": [
            "Apps should use the binder interface to communicate with other\n            apps.",
            "By default, each app runs as its own user.",
            "All apps run in a strict container with only limited system\n            calls available.",
            "Arbitrary apps can exchange files through shared\n            directories."
        ],
        "A_chat_id": 90809,
        "B_chat_id": 90810,
        "A": "Arbitrary apps can exchange files through shared directories. Android-based mobile systems have a more restricted security model compared to Linux-based desktop systems. In Android, apps are isolated from each other and cannot freely exchange files through shared directories, unlike in Linux-based desktop systems where this is possible.",
        "B": "Let's break down each statement and see whether it applies to recent Android-based mobile systems, Linux-based desktop systems, or both:\n\n1. Apps should use the binder interface to communicate with other apps.\n- This statement applies to recent Android-based mobile systems as the binder interface is used for inter-process communication in Android. It does not apply to Linux-based desktop systems.\n\n2. By default, each app runs as its own user.\n- This statement applies to both recent Android-based mobile systems and Linux-based desktop systems. In both cases, apps typically run with their own user privileges for security purposes.\n\n3. All apps run in a strict container with only limited system calls available.\n- This statement applies more to recent Android-based mobile systems than Linux-based desktop systems. While both systems have security measures in place, the concept of running apps in a strict container with limited system calls is more commonly associated with mobile systems like Android.\n\n4. Arbitrary apps can exchange files through shared directories.\n- This statement applies to Linux-based desktop systems where arbitrary apps can exchange files through shared directories. However, in recent Android-based mobile systems, there are restrictions and limitations on direct file access between apps for security reasons.\n\nBased on the analysis above, the statement \"Apps should use the binder interface to communicate with other apps.\" applies to recent Android-based mobile systems but not to Linux-based desktop systems."
    },
    {
        "course_id": 0,
        "question_id": 5748,
        "question": "Which of the following operation is not part of AES?",
        "question_options": [
            "AddRoundKey",
            "SubBytes",
            "MixColumns",
            "MulRoundKey"
        ],
        "A_chat_id": 90812,
        "B_chat_id": 90813,
        "A": "MulRoundKey is not part of the AES encryption/decryption process. The correct operation is AddRoundKey, SubBytes, and MixColumns.",
        "B": "The operation \"MulRoundKey\" is not part of the AES encryption algorithm. The correct operations in AES are:\n- AddRoundKey\n- SubBytes\n- MixColumns"
    },
    {
        "course_id": 0,
        "question_id": 6134,
        "question": "Which of the following integers has the square roots $\\{2,3\\}$ when taken modulo $5$ \\textbf{and} the square roots $\\{3,10\\}$ when taken modulo $13$.",
        "question_options": [
            "$4$.",
            "$9$.",
            "$6$.",
            "$5$."
        ],
        "A_chat_id": 90814,
        "B_chat_id": 90815,
        "A": "The correct answer is $9$. \nFor modulo $5$, the square roots of $9$ are $2$ and $3$.\nFor modulo $13$, the square roots of $9$ are $3$ and $10$.",
        "B": "Let's start by finding the square roots of each integer when taken modulo $5$.\n\nFor an integer $a$, the square roots of $a$ modulo $5$ are $b$ such that $b^2 \\equiv a \\pmod{5}$. We are given that the square roots of $a$ modulo $5$ are $\\{2, 3\\}$. This means that when we square $2$ and $3$ and take the result modulo $5$, we should get $a$. \n\nLet's test each option:\n\n1. For $a = 4$: $2^2 \\equiv 4 \\pmod{5}$ and $3^2 \\equiv 4 \\pmod{5}$.\n2. For $a = 9$: $2^2 \\equiv 4 \\pmod{5}$ and $3^2 \\equiv 4 \\pmod{5}$.\n3. For $a = 6$: $2^2 \\equiv 4 \\pmod{5}$ and $3^2 \\equiv 4 \\pmod{5}$.\n4. For $a = 5$: $2^2 \\equiv 4 \\pmod{5}$ and $3^2 \\equiv 4 \\pmod{5}$.\n\nNone of the options have the square roots $\\{2, 3\\}$ when taken modulo $5$. So, none of the options satisfy the first condition.\n\nNow, let's move on to finding the square roots of each integer when taken modulo $13$.\n\nFor an integer $a$, the square roots of $a$ modulo $13$ are $b$ such that $b^2 \\equiv a \\pmod{13}$. We are given that the square roots of $a$ modulo $13$ are $\\{3, 10\\}$. This means that when we square $3$ and $10$ and take the result modulo $13$, we should get $a$. \n\nLet's test each option:\n\n1. For $a = 4$: $3^2 \\equiv 9 \\pmod{13}$ and $10^2 \\equiv 9 \\pmod{13}$.\n2. For $a = 9$: $3^2 \\equiv 9 \\pmod{13}$ and $10^2 \\equiv 9 \\pmod{13}$.\n3. For $a = 6$: $3^2 \\equiv 9 \\pmod{13}$ and $10^2 \\equiv 9 \\pmod{13}$.\n4. For $a = 5$: $3^2 \\equiv 9 \\pmod{13}$ and $10^2 \\equiv 9 \\pmod{13}$.\n\nNone of the options have the square roots $\\{3, 10\\}$ when taken modulo $13$. So, none of the options satisfy the second condition.\n\nThus, the correct answer is: None of the given integers satisfy both conditions simultaneously."
    },
    {
        "course_id": 15002,
        "question_id": 1036,
        "question": "You have been publishing a daily column for the Gazette over the last few years and have recently reached a milestone --- your 1000th column! Realizing you'd like to go skiing more often, you decide it might be easier to automate your job by training a story generation system on the columns you've already written. Then, whenever your editor pitches you a title for a column topic, you'll just be able to give the title to your story generation system, produce the text body of the column, and publish it to the website!\nYour column generation system has become quite successful and you've managed to automate most of your job simply by typing your editor's title pitches into your model to produce your column every day. Two years later, during the COVID--25 pandemic, your editor proposes to use your system to generate an information sheet about the pandemic for anyone looking for information about symptoms, treatments, testing sites, medical professionals, etc. Given the similarity to a previous pandemic many years before, COVID--19, you train your model on all news articles published about COVID--19 between the years of 2019--2022. Then, you generate the information page from your trained model.\nGive an example of a potential harm that your model could produce from the perspective of leaking private information.",
        "question_options": null,
        "A_chat_id": 90816,
        "B_chat_id": 90817,
        "A": "One potential harm that the model could produce is leaking private medical information. If the generated information sheet includes details about specific individuals who were affected by COVID-19 during the previous pandemic, this could violate their privacy rights. Additionally, if the model inadvertently includes personal information such as addresses, phone numbers, or medical records of individuals in the generated content, it could lead to serious consequences for those individuals. It is important to carefully review and filter the output generated by the model to ensure that no private information is leaked.",
        "B": "One potential harm that could arise from using a story generation system trained on personal columns and COVID-19 news articles is the unintentional leakage of private information. For example, if your personal columns contained details about your family, friends, or personal experiences, and your model was not properly trained to filter out or anonymize such information, there could be a risk of private details being included in the generated content.\n\nIf the model inadvertently generates a column or information sheet that includes personal details about individuals mentioned in your previous columns or news articles, it could lead to a breach of privacy and confidentiality. This could potentially harm relationships, violate trust, and even have legal implications if sensitive information is exposed to the public without consent.\n\nTherefore, it is crucial to ensure that any AI system trained on personal or sensitive data is designed with robust privacy protection measures in place, such as data anonymization, data minimization, and regular audits to detect and mitigate any potential risks of leaking private information."
    },
    {
        "course_id": 0,
        "question_id": 5795,
        "question": "Tick the \\emph{false} assertion.",
        "question_options": [
            "RSA-PSS is a signature scheme.",
            "RSA-OAEP is an encryption scheme.",
            "The RSA based encryption scheme of the standard PKCS \\#1 v1.5 is vulnerable to a side channel attack.",
            "The RSA based scheme ISO/IEC 9796 is an encryption scheme."
        ],
        "A_chat_id": 90819,
        "B_chat_id": 90820,
        "A": "The false assertion is: The RSA based encryption scheme of the standard PKCS \\#1 v1.5 is vulnerable to a side channel attack. The RSA based encryption scheme of the standard PKCS \\#1 v1.5 is indeed vulnerable to a padding oracle attack, not a side channel attack. Side channel attacks typically exploit information leaked during the computation of cryptographic operations, such as timing or power consumption. RSA-PSS is a signature scheme, RSA-OAEP is an encryption scheme, and the RSA based scheme ISO/IEC 9796 is also an encryption scheme.",
        "B": "Here is the false assertion:\n\n[ ] The RSA based encryption scheme of the standard PKCS \\#1 v1.5 is vulnerable to a side channel attack."
    },
    {
        "course_id": 15127,
        "question_id": 3333,
        "question": "Consider a classification problem on linearly separable data. We train an SVM model and a logistic regression model. For logistic regression (LR) we add a small regularization term (penalty on weights) in order to make the optimum well-defined. Each model gives us a margin. Consider a datapoint $\\mathbf{x}_{0}$ that is correctly classified and strictly outside both margins Which one of the following statements is incorrect ?",
        "question_options": [
            "There exists a direction in which we can slightly move $\\mathbf{x}_{0}$ without changing the LR decision boundary after retraining.",
            "$\\mathbf{x}_{0}$ isn't a support vector",
            "There exists a direction in which we can arbitrarily move $\\mathbf{x}_{0}$ without changing the SVM decision boundary after retraining.",
            "If we remove $\\mathbf{x}_{0}$ from the dataset and retrain, this will change the LR decision boundary.",
            "If we remove $\\mathbf{x}_{0}$ from the dataset and retrain, this will not change the SVM decision boundary."
        ],
        "A_chat_id": 90821,
        "B_chat_id": 90822,
        "A": "The statement that is incorrect is: \n\n[ ] If we remove $\\mathbf{x}_{0}$ from the dataset and retrain, this will change the LR decision boundary.\n\nThe decision boundary of the logistic regression model may not necessarily change if we remove a point that is outside the margin. Removing a point may affect the margin or the weights of the logistic regression model, but it does not guarantee a change in the decision boundary.",
        "B": "Let's analyze each statement:\n\n[ ] There exists a direction in which we can slightly move $\\mathbf{x}_{0}$ without changing the LR decision boundary after retraining.\n- This statement is correct. By adding a small regularization term in logistic regression, the model is less sensitive to individual data points. Therefore, moving $\\mathbf{x}_{0}$ slightly should not change the decision boundary significantly.\n\n[ ] $\\mathbf{x}_{0}$ isn't a support vector\n- This statement is incorrect. $\\mathbf{x}_{0}$ is correctly classified and strictly outside both margins, so it is a support vector for both the SVM and logistic regression models.\n\n[ ] There exists a direction in which we can arbitrarily move $\\mathbf{x}_{0}$ without changing the SVM decision boundary after retraining.\n- This statement is correct. SVM is only influenced by support vectors, so moving $\\mathbf{x}_{0}$ in any direction that keeps it outside the margin will not change the decision boundary.\n\n[ ] If we remove $\\mathbf{x}_{0}$ from the dataset and retrain, this will change the LR decision boundary.\n- This statement is correct. Since logistic regression considers all data points in the training process, removing $\\mathbf{x}_{0}$ will likely change the decision boundary.\n\n[ ] If we remove $\\mathbf{x}_{0}$ from the dataset and retrain, this will not change the SVM decision boundary.\n- This statement is incorrect. Since $\\mathbf{x}_{0}$ is a support vector for the SVM model, removing it from the dataset will likely change the decision boundary of the SVM model.\n\nTherefore, the incorrect statement is:\n[ ] $\\mathbf{x}_{0}$ isn't a support vector"
    },
    {
        "course_id": 15127,
        "question_id": 1205,
        "question": "Consider the following matrix-factorization problem. For the observed ratings $r_{u m}$ for a given pair $(u, m)$ of a user $u$ and a movie $m$, one typically tries to estimate the score by $$ f_{u m}=\\left\\langle\\mathbf{v}_{u}, \\mathbf{w}_{m}\\right\\rangle+b_{u}+b_{m} $$ Here $\\mathbf{v}_{u}$ and $\\mathbf{w}_{m}$ are vectors in $\\mathbb{R}^{D}$ and $b_{u}$ and $b_{m}$ are scalars, indicating the bias. Assume that our objective is given by $$ \\frac{1}{2} \\sum_{u \\sim m}\\left(f_{u m}-r_{u m}\\right)^{2}+\\frac{\\lambda}{2}\\left[\\sum_{u \\in \\mathbf{U}}\\left(b_{u}^{2}+\\left\\|\\mathbf{v}_{u}\\right\\|^{2}\\right)+\\sum_{m \\in \\mathbf{M}}\\left(b_{m}^{2}+\\left\\|\\mathbf{w}_{m}\\right\\|^{2}\\right)\\right] $$ where $\\lambda>0$. Here $\\mathbf{U}$ denotes the set of all users, $M$ the set of all movies, and $u \\sim m$ represents the sum over all $(u, m)$ pairs for which a rating exists. Write the optimal values of $b_{u}$, provided that all other values are fixed.",
        "question_options": null,
        "A_chat_id": 90824,
        "B_chat_id": 90825,
        "A": "To find the optimal value of $b_u$, we differentiate the objective function with respect to $b_u$ and set it to zero. \n\nTaking the derivative of the objective function with respect to $b_u$, we get:\n\n$$\\frac{\\partial}{\\partial b_u} \\left[\\frac{1}{2} \\sum_{u \\sim m}\\left(f_{u m}-r_{u m}\\right)^{2}+\\frac{\\lambda}{2}\\left[\\sum_{u \\in \\mathbf{U}}\\left(b_{u}^{2}+\\left\\|\\mathbf{v}_{u}\\right\\|^{2}\\right)+\\sum_{m \\in \\mathbf{M}}\\left(b_{m}^{2}+\\left\\|\\mathbf{w}_{m}\\right\\|^{2}\\right)\\right] \\right]=0$$\n\nExpanding the terms and simplifying, we get:\n\n$$\\sum_{u \\sim m}\\left(f_{u m}-r_{u m}\\right) + \\lambda b_u = 0$$\n\nRearranging the terms, we get the optimal value for $b_u$ as:\n\n$$b_u = -\\frac{1}{\\lambda} \\sum_{u \\sim m}\\left(f_{u m}-r_{u m}\\right)$$\n\nTherefore, the optimal value of $b_u$ is given by the negative sum of the residuals divided by $\\lambda$.",
        "B": "To find the optimal value of $b_{u}$, we can minimize the objective function with respect to $b_{u}$ while keeping all other values fixed. Let's write out the objective function in terms of $b_{u}$ and the other terms:\n\n$$\n\\text{Objective} = \\frac{1}{2} \\sum_{u \\sim m} \\left( \\left\\langle \\mathbf{v}_{u}, \\mathbf{w}_{m} \\right\\rangle + b_{u} + b_{m} - r_{u m} \\right)^2 + \\frac{\\lambda}{2} \\left[ \\sum_{u \\in \\mathbf{U}} \\left( b_{u}^{2} + \\left\\| \\mathbf{v}_{u} \\right\\|^2 \\right) + \\sum_{m \\in \\mathbf{M}} \\left( b_{m}^{2} + \\left\\| \\mathbf{w}_{m} \\right\\|^2 \\right) \\right]\n$$\n\nNow, let's focus on the terms involving $b_{u}$:\n\n$$\n\\frac{1}{2} \\sum_{u \\sim m} \\left( \\left\\langle \\mathbf{v}_{u}, \\mathbf{w}_{m} \\right\\rangle + b_{u} + b_{m} - r_{u m} \\right)^2 + \\frac{\\lambda}{2} \\left[ b_{u}^{2} + \\left\\| \\mathbf{v}_{u} \\right\\|^2 \\right]\n$$\n\nTo find the optimal value of $b_{u}$, we take the derivative of the objective function with respect to $b_{u}$ and set it to zero:\n\n$$\n\\frac{\\partial \\text{Objective}}{\\partial b_{u}} = \\sum_{u \\sim m} \\left( \\left\\langle \\mathbf{v}_{u}, \\mathbf{w}_{m} \\right\\rangle + b_{u} + b_{m} - r_{u m} \\right) + \\lambda b_{u} = 0\n$$\n\nSolving for $b_{u}$, we get:\n\n$$\nb_{u} = -\\frac{1}{\\lambda} \\left( \\sum_{u \\sim m} \\left( \\left\\langle \\mathbf{v}_{u}, \\mathbf{w}_{m} \\right\\rangle - r_{u m} - b_{m} \\right) \\right)\n$$\n\nTherefore, the optimal value of $b_{u}$ is given by this equation."
    },
    {
        "course_id": 15002,
        "question_id": 4931,
        "question": "You are responsible for a project aiming at providing on-line recommendations to the customers of\na on-line book selling company.\nThe general idea behind this recommendation system is to cluster books according to both customers\nand content similarities, so as to propose books similar to the books already bought by a given\ncustomer. The core of the recommendation system is a clustering algorithm aiming at regrouping\nbooks likely to be appreciate by the same person. This clustering should not only be achieved\nbased on the purchase history of customers, but should also be refined by the content of the books\nthemselves. It's that latter aspect we want to address in this exam question.\n\nConsider the following six 'documents' (toy example):\nd1: 'Because cows are not sorted as they return from the fields to their home pen, cow flows\nare improved.'\nd2: 'He was convinced that if he owned the fountain pen that he'd seen in the shop window for years, he could write fantastic stories with it. That was the kind of pen you cannot forget.'\nd3: 'With this book you will learn how to draw humans, animals (cows, horses, etc.) and flowers with a charcoal pen.'\nd4: 'The cows were kept in pens behind the farm, hidden from the road. That was the typical kind of pen made for cows.'\nd5: 'If Dracula wrote with a fountain pen, this would be the kind of pen he would write with, filled with blood red ink. It was the pen she chose for my punishment, the pen of my torment. What a mean cow!'\nd6: 'What pen for what cow? A red pen for a red cow, a black pen for a black cow, a brown pen for a brown cow, ... Understand?'\n\nand suppose (toy example) that they are indexed only by the two words: pen and cow.\n\nWhat are their vector representations?",
        "question_options": null,
        "A_chat_id": 90827,
        "B_chat_id": 90828,
        "A": "In order to represent the documents in a vector space model, we can use the TF-IDF (Term Frequency-Inverse Document Frequency) method. \n\nFirst, we need to create a vocabulary based on the unique words in the documents. In this case, the vocabulary consists of the words 'pen', 'cow', 'because', 'not', 'sorted', 'return', 'fields', 'home', 'flows', 'improved', 'he', 'was', 'convinced', 'if', 'owned', 'fountain', 'that', 'seen', 'shop', 'window', 'years', 'could', 'write', 'fantastic', 'stories', 'with', 'kind', 'you', 'learn', 'how', 'draw', 'humans', 'animals', 'horses', 'etc', 'flowers', 'charcoal', 'kept', 'pens', 'behind', 'farm', 'hidden', 'from', 'road', 'typical', 'made', 'dracula', 'wrote', 'this', 'blood', 'red', 'ink', 'she', 'chose', 'my', 'punishment', 'torment', 'mean', 'what', 'for', 'red', 'black', 'brown', 'understand'.\n\nNext, we compute the TF-IDF values for each document. The TF-IDF value for a term in a document is calculated as follows:\nTF(t,d) = (Number of times term t appears in document d) / (Total number of terms in document d)\nIDF(t) = log( Total number of documents / Number of documents containing term t)\nTF-IDF(t,d) = TF(t,d) * IDF(t)\n\nFor example, the TF-IDF values for the term 'pen' in document d1 can be calculated as follows:\nTF(pen, d1) = 1 / 15\nIDF(pen) = log(6 / 5) \u2248 0.18\nTF-IDF(pen, d1) = (1 / 15) * 0.18 \u2248 0.012\n\nSimilarly, we can calculate the TF-IDF values for all terms in all documents to obtain their vector representations in the vector space model.",
        "B": "To represent the documents as vectors, we can use a simple bag-of-words model where each document is represented by a vector indicating the presence or absence of specific words. In this case, we have two words of interest: \"pen\" and \"cow\".\n\nHere are the vector representations of the documents based on the presence of the words \"pen\" and \"cow\":\n\n- d1: [1, 1] (pen: 1, cow: 1)\n- d2: [1, 0] (pen: 1, cow: 0)\n- d3: [1, 1] (pen: 1, cow: 1)\n- d4: [1, 1] (pen: 1, cow: 1)\n- d5: [1, 1] (pen: 1, cow: 1)\n- d6: [1, 1] (pen: 1, cow: 1)\n\nEach element in the vector corresponds to the presence (1) or absence (0) of the corresponding word in the document."
    },
    {
        "course_id": 4,
        "question_id": 6627,
        "question": "Which of the following is/are true about fuzzing?",
        "question_options": [
            "Fuzzers may get ``stuck'' and cannot easily detect that they are\n               no longer improving coverage",
            "The efficacy of a fuzzing campaign scales with its speed (executions per second)",
            "Fuzzers generally determine the exploitability of a crash.",
            "There is little to no benefit in running fuzzers in parallel."
        ],
        "A_chat_id": 90831,
        "B_chat_id": 90832,
        "A": "The following statements are true about fuzzing:\n- Fuzzers may get \"stuck\" and cannot easily detect that they are no longer improving coverage.\n- The efficacy of a fuzzing campaign scales with its speed (executions per second).\n- Fuzzers generally do not determine the exploitability of a crash.",
        "B": "Let's analyze each statement:\n\n- [x] Fuzzers may get ``stuck'' and cannot easily detect that they are no longer improving coverage. \n    - True. Fuzzers can sometimes get stuck in a particular part of the code and not progress further, leading to reduced effectiveness in finding vulnerabilities.\n\n- [ ] The efficacy of a fuzzing campaign scales with its speed (executions per second).\n    - False. While faster execution can lead to faster discovery of vulnerabilities, the efficacy of a fuzzing campaign is not solely dependent on speed. Other factors such as test case generation, code coverage, and mutation strategies also play a significant role.\n\n- [ ] Fuzzers generally determine the exploitability of a crash.\n    - False. Fuzzers are designed to generate test cases and inputs to trigger crashes or potential vulnerabilities, but determining the exploitability of a crash typically requires further analysis and investigation.\n\n- [ ] There is little to no benefit in running fuzzers in parallel.\n    - False. Running fuzzers in parallel can significantly increase the coverage and efficiency of the fuzzing campaign by exploring different paths simultaneously and finding vulnerabilities faster.\n\nSo, the true statement is: Fuzzers may get ``stuck'' and cannot easily detect that they are no longer improving coverage."
    },
    {
        "course_id": 0,
        "question_id": 5659,
        "question": "If we pick independent random numbers in $\\{1, 2, \\dots, N\\}$ with uniform distribution, $\\theta \\sqrt{N}$ times, we get at least one number twice with probability\\dots",
        "question_options": [
            "$e^{\\theta ^2}$",
            "$1-e^{\\theta ^2}$",
            "$e^{-\\theta ^2 /2}$",
            "$1-e^{-\\theta ^2 /2}$"
        ],
        "A_chat_id": 90834,
        "B_chat_id": 90835,
        "A": "The correct answer is $1-e^{-\\theta ^2}$. This is the probability of not getting any repeated numbers in $\\theta \\sqrt{N}$ picks. The probability of getting at least one number twice is the complement of this, which is $1 - (1-e^{-\\theta ^2}) = e^{-\\theta ^2}$.",
        "B": "Let's denote the probability of not getting any number twice in $\\theta \\sqrt{N}$ draws as $P(\\text{no repeats})$. \n\nIn the first draw, we have $N$ choices, in the second draw we have $N-1$ choices (we can't choose the same number as the first draw), and so on. In the $\\theta \\sqrt{N}$ draws, we have $N, N-1, N-2, \\dots, N-\\theta \\sqrt{N}+1$ choices respectively.\n\nTherefore, the probability of not getting any number twice in $\\theta \\sqrt{N}$ draws is:\n\\[ P(\\text{no repeats}) = \\frac{N}{N} \\times \\frac{N-1}{N} \\times \\frac{N-2}{N} \\times \\dots \\times \\frac{N-\\theta \\sqrt{N}+1}{N} = \\frac{N!}{(N-\\theta \\sqrt{N})!N^{\\theta \\sqrt{N}}} \\]\n\nUsing Stirling's approximation $n! \\approx \\sqrt{2\\pi n} \\left(\\frac{n}{e}\\right)^n$, we have:\n\\[ P(\\text{no repeats}) \\approx \\frac{\\sqrt{2\\pi N} \\left(\\frac{N}{e}\\right)^N}{\\sqrt{2\\pi(N-\\theta \\sqrt{N})} \\left(\\frac{N-\\theta \\sqrt{N}}{e}\\right)^{N-\\theta \\sqrt{N}} N^{\\theta \\sqrt{N}}} \\]\n\nSimplifying, we get:\n\\[ P(\\text{no repeats}) \\approx e^{-\\frac{\\theta^2}{2}} \\]\n\nTherefore, the probability of getting at least one number twice in $\\theta \\sqrt{N}$ draws is:\n\\[ P(\\text{at least one repeat}) = 1 - P(\\text{no repeats}) = 1 - e^{-\\frac{\\theta^2}{2}} \\]\n\nSo, the correct answer is:\n[ ] $1 - e^{-\\theta^2/2}$"
    },
    {
        "course_id": 15103,
        "question_id": 1768,
        "question": "Assume you are working in a company on the back-end of a mobile application. You are tasked with improving the integration of the authentication via Google in your app, but your manager tells you:\n\"Don't run any tests today, we only have a few API calls left for today's rate limit, we need to preserve those for customers.\"\nIn 1-2 sentences, propose a change to the codebase to avoid this problem.",
        "question_options": null,
        "A_chat_id": 90839,
        "B_chat_id": 90840,
        "A": "One possible solution could be to implement a local testing environment where API calls are not counted towards the rate limit, allowing for thorough testing without affecting the daily limit for customers.",
        "B": "One way to avoid hitting the rate limit while working on the authentication integration is to mock the Google authentication API responses locally in the development environment, so that actual API calls are not made during testing."
    },
    {
        "course_id": 15103,
        "question_id": 1710,
        "question": "Assume you are working on SuperQuiz, a trendy app that lets everyone design quizzes and share them with friends! Your first assignment is to add a new feature that is requested by users. You are given the following transcript of an interview with a customer of your product:\n> Hi!\n> So you're the developer of this quiz app?\n> The one where you write questions and answers and get your friends to guess?\n> It's fun, but I can't use it as much as I'd like.\n> I'm a firefighter, I don't have time for this app during the day, but I wish I could use it at home.\n> See, when I come back home after work, I have a bunch of stuff to do, cleaning, cooking, ...\n> And when I'm doing these tasks, I'm rather busy. Not like when I'm watching TV.\n> I don't always have my phone in my hands! Sometimes I even forget where I put it.\n> Maybe if you made it so I could talk to the app? You know, many apps have that feature now.\n> Then I could multitask! Think about quizzes while I'm cooking!\n> Otherwise, I won't use the app much.\n\nWrite down a user story, as a single sentence that follows the following guidelines:\n1) A user story that summarizes all necessary information from the feedback\n2) the user story does not contain any unnecessary information",
        "question_options": null,
        "A_chat_id": 90842,
        "B_chat_id": 90843,
        "A": "As a busy firefighter, I want to be able to use the SuperQuiz app hands-free by adding a voice interaction feature, so that I can multitask and think about quizzes while doing household chores.",
        "B": "As a firefighter with limited time and multitasking needs, I want the SuperQuiz app to have voice command capability so that I can engage with quizzes hands-free while doing household chores."
    },
    {
        "course_id": 0,
        "question_id": 6322,
        "question": "Tick the \\emph{incorrect} statement. When $x\\rightarrow+\\infty$ \\ldots",
        "question_options": [
            "$x^3 + 2x + 5 = \\mathcal{O}(x^3)$.",
            "$\\frac{1}{x^2} = \\mathcal{O}(\\frac{1}{x})$.",
            "$2^{\\frac{x}{\\log x}} = \\mathcal{O}(2^x)$.",
            "$n^x = \\mathcal{O}(x^n)$ for any constant $n>1$."
        ],
        "A_chat_id": 90844,
        "B_chat_id": 90845,
        "A": "The incorrect statement is: $n^x = \\mathcal{O}(x^n)$ for any constant $n>1$. This statement is not true, as $n^x$ grows faster than $x^n$ for any constant $n>1$.",
        "B": "The incorrect statement is:  \n[ ] $n^x = \\mathcal{O}(x^n)$ for any constant $n>1$."
    },
    {
        "course_id": 0,
        "question_id": 6078,
        "question": "Tick the \\emph{incorrect} assertion regarding the Diffie-Hellman key exchange",
        "question_options": [
            "It is secure under a passive adversary.",
            "It is secure when working in a subgroup of $\\mathbb{Z}_p^*$ of big prime order.",
            "It is secure under an active Man-In-The-Middle attack.",
            "It is not secure when working in $\\mathbb{Z}_n$ with $n$ having only small prime factors."
        ],
        "A_chat_id": 90847,
        "B_chat_id": 90848,
        "A": "The incorrect assertion is: It is secure under an active Man-In-The-Middle attack. Diffie-Hellman key exchange is vulnerable to Man-In-The-Middle attacks, where an attacker intercepts and alters the communication between the two parties to obtain the shared secret key. This is why additional measures such as authentication and digital signatures are needed to ensure security in real-world implementations.",
        "B": "The incorrect assertion regarding the Diffie-Hellman key exchange is:\n\n[ ] It is secure under an active Man-In-The-Middle attack."
    },
    {
        "course_id": 15003,
        "question_id": 7331,
        "question": "Can we devise a Best-effort Broadcast algorithm that satisfies the causal delivery property, without being a causal broadcast algorithm, i.e., without satisfying the agreement property of a reliable broadcast?",
        "question_options": null,
        "A_chat_id": 90849,
        "B_chat_id": 90850,
        "A": "Yes, it is possible to devise a Best-effort Broadcast algorithm that satisfies the causal delivery property without being a causal broadcast algorithm. In a Best-effort Broadcast algorithm, messages are delivered to all correct processes in the system, but there is no guarantee of agreement on the order of message delivery. By designing the algorithm to ensure causal delivery, messages are delivered in a causal order, i.e., messages are delivered according to the happened-before relation. This ensures that messages are delivered in the order in which they were causally related, but does not guarantee the agreement property of reliable broadcast, where all correct processes deliver the same set of messages in the same order.",
        "B": "Yes, it is possible to devise a Best-effort Broadcast algorithm that satisfies the causal delivery property without being a causal broadcast algorithm that satisfies the agreement property of reliable broadcast. \n\nIn a Best-effort Broadcast algorithm that satisfies causal delivery, messages are delivered in a causal order, meaning that messages are delivered in the same order as they are sent by the sender. This property ensures that messages are delivered in a way that respects the causality of events in the system.\n\nHowever, in a causal broadcast algorithm that satisfies the agreement property of reliable broadcast, all correct processes must deliver the same set of messages in the same order. This property ensures that all correct processes agree on the order of messages that are delivered.\n\nSo, it is possible to design a Best-effort Broadcast algorithm that only satisfies the causal delivery property without ensuring that all correct processes deliver the same set of messages in the same order. This means that the algorithm may not satisfy the agreement property of reliable broadcast, but it can still guarantee causal delivery."
    },
    {
        "course_id": 0,
        "question_id": 6139,
        "question": "Select the \\emph{incorrect} statement. Pedersen Commitment is",
        "question_options": [
            "unconditionally hiding.",
            "computationally binding.",
            "based on the hardness of the discrete logarithm problem.",
            "based on DSA."
        ],
        "A_chat_id": 90852,
        "B_chat_id": 90853,
        "A": "The incorrect statement is: based on DSA. Pedersen Commitment is based on the hardness of the discrete logarithm problem, not on the Digital Signature Algorithm (DSA).",
        "B": "The incorrect statement is: [ ] based on DSA. \n\nPedersen Commitment is not based on DSA, but rather on the hardness of the discrete logarithm problem."
    },
    {
        "course_id": 15006,
        "question_id": 4192,
        "question": "The goal of the 4 following questions is to prove that the methods map and mapTr are equivalent. The\nformer is the version seen in class and is specified by the lemmas MapNil and MapCons. The later version\nis a tail-recursive version and is specified by the lemmas MapTrNil and MapTrCons.\nAll lemmas on this page hold for all x: Int, y: Int, xs: List[Int], ys: List[Int], l: List\n[Int] and f: Int => Int.\nGiven the following lemmas:\n\n(MapNil) Nil.map(f) === Nil\n(MapCons) (x :: xs).map(f) === f(x) :: xs.map(f)\n(MapTrNil) Nil.mapTr(f, ys) === ys\n(MapTrCons) (x :: xs).mapTr(f, ys) === xs.mapTr(f, ys ++ (f(x) :: Nil))\n(NilAppend) Nil ++ xs === xs\n(ConsAppend) (x :: xs) ++ ys === x :: (xs ++ ys)\nLet us first prove the following lemma:\n\n(AccOut) l.mapTr(f, y :: ys) === y :: l.mapTr(f, ys)\n\nWe prove it by induction on l.\nInduction step: l is x :: xs. Therefore, we need to prove:\n(x :: xs).mapTr(f, y :: ys) === y :: (x :: xs).mapTr(f, ys)\n\nWe name the induction hypothesis IH.\nWhat exact sequence of lemmas should we apply to rewrite the left hand-side ((x :: xs).mapTr(f, y\n:: ys)) to the right hand-side (y :: (x :: xs).mapTr(f, ys))?",
        "question_options": null,
        "A_chat_id": 90856,
        "B_chat_id": 90857,
        "A": "To rewrite the left hand-side ((x :: xs).mapTr(f, y :: ys)) to the right hand-side (y :: (x :: xs).mapTr(f, ys)), we should apply the lemmas MapTrCons and ConsAppend in that exact sequence. \n\nApplying MapTrCons gives us:\n(x :: xs).mapTr(f, y :: ys) = xs.mapTr(f, y :: ys ++ (f(x) :: Nil))\n\nThen, applying ConsAppend gives us:\nxs.mapTr(f, y :: ys ++ (f(x) :: Nil)) = y :: xs.mapTr(f, ys ++ (f(x) :: Nil))\n\nTherefore, combining the two steps, we have:\n(x :: xs).mapTr(f, y :: ys) = y :: xs.mapTr(f, ys ++ (f(x) :: Nil))\n\nThis completes the proof of the lemma (AccOut).",
        "B": "To rewrite the left hand-side ((x :: xs).mapTr(f, y :: ys)) to the right hand-side (y :: (x :: xs).mapTr(f, ys)), we can follow these steps using the given lemmas:\n\n1. Apply the (MapTrCons) lemma:\n   (x :: xs).mapTr(f, y :: ys) === xs.mapTr(f, y :: ys ++ (f(x) :: Nil))\n\n2. Apply the (ConsAppend) lemma:\n   y :: ys ++ (f(x) :: Nil) === y :: (ys ++ (f(x) :: Nil))\n\n3. Apply the induction hypothesis (AccOut) with xs, f, and ys:\n   xs.mapTr(f, y :: ys ++ (f(x) :: Nil)) === y :: xs.mapTr(f, ys ++ (f(x) :: Nil))\n\nBy combining these steps, we can rewrite the left-hand side to the right-hand side as follows:\n(x :: xs).mapTr(f, y :: ys) === y :: xs.mapTr(f, ys ++ (f(x) :: Nil))\n\nThis sequence of lemmas allows us to prove the lemma (AccOut) by induction on the list l."
    },
    {
        "course_id": 15118,
        "question_id": 6918,
        "question": "When you're running and you need to stop all of a sudden, where does the force stopping you come from?\n\n",
        "question_options": null,
        "A_chat_id": 90860,
        "B_chat_id": 90861,
        "A": "When you are running and need to stop suddenly, the force stopping you comes from the friction between your feet and the ground. As you decelerate, the frictional force between your feet and the ground increases, ultimately bringing you to a stop. Additionally, your muscles also play a role in stopping your motion by contracting to slow down your movement.",
        "B": "When you're running and need to stop suddenly, the force that stops you comes from the friction between your feet and the ground. As your feet push against the ground in the opposite direction of your motion, the friction between your shoes and the ground generates a force that slows you down and eventually brings you to a stop. This force is known as the stopping force or frictional force."
    },
    {
        "course_id": 15103,
        "question_id": 1703,
        "question": "Assume that you are part of a team developing a mobile app using Scrum.\nWhen using the app, you identified multiple bugs and features which you think should be implemented, and took some notes. You want to\nshare these with the Product Owner. Your backlog of tasks includes the following task:\n- [ ] [Bug] The landing page doesn't render well, if the language of my device is not English.\nIs this item suitable to be submitted to the Product Backlog? Why?",
        "question_options": null,
        "A_chat_id": 90863,
        "B_chat_id": 90864,
        "A": "Yes, this item is suitable to be submitted to the Product Backlog. The Product Backlog is a list of all desired work on the project, and includes bugs, new features, improvements, and any other changes that need to be made to the product. The item \"The landing page doesn't render well, if the language of my device is not English\" falls under the category of a bug, so it is appropriate to add it to the Product Backlog for prioritization and resolution by the development team.",
        "B": "Yes, the item \"The landing page doesn't render well if the language of my device is not English\" is suitable to be submitted to the Product Backlog. \n\nIn Scrum, the Product Backlog is a prioritized list of all the work that needs to be done on the project, including new features, enhancements, and bug fixes. The item you have mentioned is a bug, and bugs are an essential part of the Product Backlog as they need to be addressed to ensure the quality of the product.\n\nBy adding this bug to the Product Backlog, the Product Owner can prioritize it along with other items based on its severity and impact on the users. This ensures that the team is aware of the issue and can plan to fix it in a future sprint."
    },
    {
        "course_id": 9,
        "question_id": 6839,
        "question": "Which of the following is equivalent to \\((10001)_2\\) ? (Multiple answers can be correct)",
        "question_options": [
            "\\(  (17)_{10}  \\)",
            "\\(  (F0)_{16}  \\)",
            "\\(  (23)_{8}  \\)",
            "\\(  (101)_{4}  \\)"
        ],
        "A_chat_id": 90866,
        "B_chat_id": 90867,
        "A": "\\( (10001)_2 = (17)_{10} \\) and \\( (F0)_{16} \\)",
        "B": "To convert the binary number \\((10001)_2\\) to decimal, we can use the positional notation. \n\n\\((10001)_2 = 1*2^4 + 0*2^3 + 0*2^2 + 0*2^1 + 1*2^0 = 16 + 1 = 17\\)\n\nTherefore, \\((10001)_2\\) is equivalent to \\( (17)_{10}\\).\n\nSo, the correct answer is:\n- [x] \\(  (17)_{10}  \\)"
    },
    {
        "course_id": 0,
        "question_id": 5954,
        "question": "How many different simple substitution ciphers do exist with respect to an alphabet of 26 characters?",
        "question_options": [
            "$26!$",
            "$2^{26}$",
            "$26^2$",
            "26"
        ],
        "A_chat_id": 90868,
        "B_chat_id": 90869,
        "A": "There are $26!$ different simple substitution ciphers that exist with respect to an alphabet of 26 characters. Each letter can be mapped to any of the other 25 letters, resulting in $26!$ possible permutations.",
        "B": "A simple substitution cipher is a method of encryption where each letter in the plaintext is replaced with another letter in the alphabet. Since there are 26 letters in the alphabet, there are 26 choices for the letter to replace each letter in the plaintext.\n\nTherefore, the number of different simple substitution ciphers that exist with respect to an alphabet of 26 characters is $26!$, which represents all the possible permutations of the alphabet. \n\nSo, the correct answer is:\n[X] $26!$"
    },
    {
        "course_id": 0,
        "question_id": 6073,
        "question": "In a challenge/response protocol,\\dots",
        "question_options": [
            "the secret is sent in clear over the channel.",
            "the response is derived using a pseudo-random function.",
            "the response is a hash of the secret (i.e., $h(w)$ for a secret $w$).",
            "the size of the challenge space can be 1."
        ],
        "A_chat_id": 90871,
        "B_chat_id": 90872,
        "A": "In a challenge/response protocol, the secret is sent in clear over the channel. This is false, as the secret should never be sent in clear over the channel in a challenge/response protocol. The other statements are true: the response is derived using a pseudo-random function, the response is often a hash of the secret, and the size of the challenge space should be more than 1 to provide sufficient security.",
        "B": "- [ ] the secret is sent in clear over the channel.  \n- [x] the response is derived using a pseudo-random function.  \n- [ ] the response is a hash of the secret (i.e., $h(w)$ for a secret $w$).  \n- [ ] the size of the challenge space can be 1."
    },
    {
        "course_id": 15000,
        "question_id": 0,
        "question": "Consider the following contains function defined on Iterable (in particular, it accepts both Vector and List).  def contains[A](l: Iterable[A], elem: A): Boolean =   val n = l.size   if n <= 5 then     for i <- l do       if i == elem then         return true     false   else     val (p0, p1) = parallel(       contains(l.take(n / 2), elem),       contains(l.drop(n / 2), elem)     )   p0 || p1 Let $n$$n$ be the size of l. Assume that drop and take run in $\\Theta(1)$ on Vector and $\\Theta(n)$ on List. What is the asymptotic depth of contains if it is called on a List?",
        "question_options": null,
        "A_chat_id": 90873,
        "B_chat_id": 90874,
        "A": "The asymptotic depth of the contains function when called on a List is $\\log_2(n)$, where $n$ is the size of the List. This is because the function recursively splits the List in half each time it is called, leading to a binary tree structure with a depth of $\\log_2(n)$.",
        "B": "When `contains` is called on a List, the `drop` and `take` operations run in $\\Theta(n)$ time. \n\nIn the case where $n \\leq 5$, the loop runs in $\\Theta(n)$ time since it iterates over the elements of the list once.\n\nIn the case where $n > 5$, the list is split into two parts of roughly equal size, and the `contains` function is recursively called on each half. This splitting process continues until the size of the sublists becomes less than or equal to 5. \n\nThe depth of the recursion tree in this case can be analyzed as follows:\n\n- At the first level of recursion, the list is split into two parts.\n- At the second level, each of these two parts is split into two parts, resulting in a total of 4 parts.\n- This splitting continues until the size of each sublist is less than or equal to 5.\n\nTherefore, the number of recursive splits needed to reduce the list to sublists of size 5 or less is $\\log_2(n/5)$.\n\nHence, the asymptotic depth of the `contains` function when called on a List is $\\log_2(n/5)$, where $n$ is the size of the List."
    },
    {
        "course_id": 15118,
        "question_id": 6958,
        "question": "A yoyo is made up of a disk of radius $R$ and mass $M$, around which a massless thread is wound up. The thread is attached to the ceiling and remains constantly vertical; gravity acts on the yo-yo which is free to lower down.\n\nFind the acceleration $\\vec{a}$ of the yo-yo's center of mass, as well as the thread's tension $\\vec{T}$.\n",
        "question_options": null,
        "A_chat_id": 90876,
        "B_chat_id": 90877,
        "A": "Let's denote the acceleration of the yo-yo's center of mass as $\\vec{a}$ and the tension in the thread as $\\vec{T}$. The forces acting on the yo-yo are the tension $\\vec{T}$ and the gravitational force $\\vec{F_{\\text{gravity}}} = -M\\vec{g}$, where $\\vec{g}$ is the acceleration due to gravity.\n\nThe acceleration of the center of mass $\\vec{a}$ is given by the equation of motion:\n$$\\sum \\vec{F} = M\\vec{a}$$\n\nConsidering the forces acting in the radial direction, we have the tension $\\vec{T}$ providing the centripetal force required for circular motion:\n$$T = M\\vec{a}$$\n\nIn the vertical direction, we have the tension $\\vec{T}$ balancing the gravitational force:\n$$T - Mg = 0$$\n\nSolving these equations simultaneously, we find:\n$$\\vec{a} = g \\hat{r}$$\n$$T = Mg$$\n\nTherefore, the acceleration of the yo-yo's center of mass is $\\vec{a} = g \\hat{r}$ and the tension in the thread is $\\vec{T} = Mg$.",
        "B": "To find the acceleration $\\vec{a}$ of the yo-yo's center of mass, we can start by considering the forces acting on the yo-yo. There are two forces acting on the yo-yo: the tension $\\vec{T}$ in the thread and the gravitational force $\\vec{F}_g = -Mg\\hat{j}$, where $g$ is the acceleration due to gravity and $\\hat{j}$ is the unit vector pointing downwards.\n\nThe net force acting on the yo-yo is given by the vector sum of these forces:\n\n$$\\vec{F}_{\\text{net}} = \\vec{T} + \\vec{F}_g$$\n\nSince the yo-yo is accelerating downwards, we have:\n\n$$\\vec{F}_{\\text{net}} = M\\vec{a}$$\n\nEquating these two expressions, we have:\n\n$$M\\vec{a} = \\vec{T} + (-Mg\\hat{j})$$\n\nSince the thread is vertical, the tension $\\vec{T}$ will also act vertically upwards. Therefore, the tension can be written as $\\vec{T} = T\\hat{j}$, where $T$ is the magnitude of the tension.\n\nNow, looking at the components in the vertical direction, we have:\n\n$$Ma = T - Mg$$\n\nSolving for the acceleration $a$, we get:\n\n$$a = \\frac{T - Mg}{M}$$\n\nTo find the tension $T$, we need to consider the torque equation. The torque $\\tau$ acting on the yo-yo is given by:\n\n$$\\tau = I\\alpha$$\n\nwhere $I$ is the moment of inertia of the yo-yo and $\\alpha$ is the angular acceleration.\n\nFor a disk rotating about its center, the moment of inertia $I$ is given by $I = \\frac{1}{2}MR^2$.\n\nThe torque due to the tension $\\tau_T$ is given by $\\tau_T = TR$, and the torque due to gravity $\\tau_g$ is given by $\\tau_g = -Mg\\frac{R}{2}$ (since the gravitational force acts at the center of mass).\n\nSince the yo-yo is accelerating downwards, the angular acceleration $\\alpha$ is related to the linear acceleration $a$ by $\\alpha = \\frac{a}{R}$.\n\nTherefore, the torque equation becomes:\n\n$$TR - Mg\\frac{R}{2} = \\frac{1}{2}MR^2 \\frac{a}{R}$$\n\nSubstitute the expression for acceleration $a$ we found earlier:\n\n$$TR - Mg\\frac{R}{2} = \\frac{1}{2}M(T - Mg)$$\n\nNow, solve this equation for tension $T$ to find the value of tension in the thread."
    },
    {
        "course_id": 15127,
        "question_id": 1192,
        "question": "Recall that we say that a kernel $K: \\R \times \\R \rightarrow \\R $ is valid if there exists $k \\in \\mathbb{N}$ and $\\Phi: \\R \rightarrow \\R^k$ such that for all $(x, x') \\in \\R \times \\R $, $K(x, x') =  \\Phi(x)^\top \\Phi(x')$. The kernel $K(x, x') = \\cos(x + x')$ is a valid kernel.",
        "question_options": [
            "True",
            "False"
        ],
        "A_chat_id": 90880,
        "B_chat_id": 90881,
        "A": "False. The cosine kernel $K(x, x') = \\cos(x + x')$ is not a valid kernel because it does not satisfy the property of positive semi-definiteness required for a valid kernel.",
        "B": "To determine if the given kernel $K(x, x') = \\cos(x + x')$ is a valid kernel, we need to check if there exists a positive integer $k \\in \\mathbb{N}$ and a mapping function $\\Phi: \\R \\rightarrow \\R^k$ such that for all $(x, x') \\in \\R \\times \\R$, we have $K(x, x') = \\Phi(x)^T \\Phi(x')$.\n\nLet's analyze the given kernel:\n$K(x, x') = \\cos(x + x')$\n\nWe can rewrite the kernel as:\n$K(x, x') = \\cos(x)\\cos(x') - \\sin(x)\\sin(x')$\n\nNow, let's consider the function $\\Phi: \\R \\rightarrow \\R^2$ defined as:\n$\\Phi(x) = \\begin{pmatrix} \\cos(x) \\\\ \\sin(x) \\end{pmatrix}$\n\nIf we calculate $\\Phi(x)^T \\Phi(x')$, we get:\n$\\Phi(x)^T \\Phi(x') = \\begin{pmatrix} \\cos(x) & \\sin(x) \\end{pmatrix} \\begin{pmatrix} \\cos(x') \\\\ \\sin(x') \\end{pmatrix} = \\cos(x)\\cos(x') + \\sin(x)\\sin(x')$\n\nComparing this result with the original kernel $K(x, x') = \\cos(x + x')$, we see that they match. Therefore, the given kernel $K(x, x') = \\cos(x + x')$ is a valid kernel, and we can choose $k = 2$ and $\\Phi(x)$ as defined above.\n\nTherefore, the answer is:\n[X] True"
    },
    {
        "course_id": 15003,
        "question_id": 7337,
        "question": "If process i fails, then eventually all processes j\u2260i fail\nIs the following true? If no process j\u2260i fails, then process i has not failed",
        "question_options": null,
        "A_chat_id": 90883,
        "B_chat_id": 90884,
        "A": "Yes, the statement is true. If process i fails, then eventually all processes j\u2260i fail implies that if no process j\u2260i fails, then process i has not failed. This is because the failure of process i is a necessary condition for all other processes to eventually fail. If no other process fails, then process i cannot have failed.",
        "B": "Yes, the statement is true. \n\nIf process i fails, then eventually all processes j\u2260i fail. This means that if process i has failed, then it is guaranteed that eventually all other processes will also fail.\n\nConversely, if no process j\u2260i fails, then process i has not failed. This is because the given statement tells us that if process i fails, then all other processes will eventually fail. Therefore, if all other processes have not failed, then process i must not have failed."
    },
    {
        "course_id": 15002,
        "question_id": 4935,
        "question": "Consider using a parser with the following (partial) grammar:\nS -> NP VP \nVP -> V\nNP -> Det N\nVP -> VP PP\nNP -> N\nVP -> VBP VBG PP\nNP -> NP PP\nPP -> P NP\nand (also partial) lexicon:\n2012 N\nfrom P\nSwitzerland N\nin P\nUSA N \nincreasing VBG\nare VBP \nthe Det\nexports N \nto P\nexports V\nUsing the CYK algorithm, parse the following sentence with the above lexicon/grammar:\nthe exports from the USA to Switzerland are increasing in 2012\nProvide both the complete, fully filled, data structure used by the algorithm, as well as the result of\nthe parsing in the form of a/the parse tree(s).",
        "question_options": null,
        "A_chat_id": 90886,
        "B_chat_id": 90887,
        "A": "The complete, fully filled data structure used by the CYK algorithm for the given sentence is as follows:\n\n|           |           |           |           |           |           |\n|-----------|-----------|-----------|-----------|-----------|-----------|\n| NP        |           |           |           |           |           |\n| Det       | NP        |           |           |           |           |\n| the       | exports   |           |           |           |           |\n|           | VP        |           |           |           |           |\n|           | VBP       | VBG       | PP        |           |           |\n|           |           | increasing|           |           |           |\n|           |           | from      | NP        |           |           |\n|           |           |           | N         |           |           |\n|           |           |           | USA       |           |           |\n|           |           |           |           | PP        |           |\n|           |           |           |           | P         | NP        |\n|           |           |           |           | to        |           |\n|           |           |           |           |           | N         |\n|           |           |           |           |           | Switzerland|\n|           |           |           | in        |           | 2012      |\n\nThe parse tree for the given sentence is as follows:\n\n```\n(S\n  (NP (Det the) (NP (N exports)))\n  (VP\n    (VBP are)\n    (VP\n      (VBG increasing)\n      (PP\n        (P from)\n        (NP (N USA))\n      )\n      (PP\n        (P to)\n        (NP (N Switzerland))\n      )\n      (PP\n        (P in)\n        (NP (N 2012))\n      )\n    )\n  )\n)\n```",
        "B": "To parse the sentence \"the exports from the USA to Switzerland are increasing in 2012\" using the CYK algorithm with the provided lexicon and grammar, we need to create a table and fill it according to the algorithm. \n\nHere is the table with the first few steps filled:\n\n|   |   |   |   |   |   |   |   |\n|---|---|---|---|---|---|---|---|\n|   |   |   |   |   |   |   | S |\n|   |   |   |   |   | NP |   |   |\n|   |   |   |   | Det |   |   |   |\n|   |   |   | the |   |   |   |   |\n|   |   | N |   |   |   |   |   |\n|   | exports |   |   |   |   |   |   |\n|   |   |   |   |   |   |   |   |\n|   |   |   |   |   |   |   |   |\n|   |   |   |   |   |   |   |   |\n|   |   |   |   |   |   |   |   |\n|   |   |   |   |   |   |   |   |\n|   |   |   |   |   |   |   |   |\n|   |   |   |   |   |   |   |   |\n\nContinuing to fill the table:\n\n|   |   |   |   |   |   |   | S |\n|   |   |   |   |   | NP |   |   |\n|   |   |   |   | Det |   |   |   |\n|   |   |   | the |   |   |   |   |\n|   |   | N |   |   |   |"
    },
    {
        "course_id": 15008,
        "question_id": 2758,
        "question": "Considering the set of triangles previously calculated, estimate the fraction of triangles of configurations  {+,+,+}\n ,  {\u2212,\u2212,\u2212}\n ,  {+,+,\u2212}\n , and  {+,\u2212,\u2212}\n . What fraction of triangles are \"balanced\" according to structural balance theory?",
        "question_options": null,
        "A_chat_id": 90893,
        "B_chat_id": 90894,
        "A": "To estimate the fraction of triangles of each configuration, we need to consider the possible combinations of signs for each edge of the triangle. \n\nFor the configuration {+,+,+}, all three edges must have positive signs. This corresponds to 1 out of 8 possible combinations (++, ++, ++).\n\nFor the configuration {\u2212,\u2212,\u2212}, all three edges must have negative signs. This also corresponds to 1 out of 8 possible combinations (--, --, --).\n\nFor the configuration {+,+,\u2212}, two edges must have positive signs and one edge must have a negative sign. This corresponds to 3 out of 8 possible combinations (++, ++, -+), (++,-+, ++), (-+, ++, ++).\n\nFor the configuration {+,\u2212,\u2212}, one edge must have a positive sign and two edges must have negative signs. This corresponds to 3 out of 8 possible combinations (+-, --, --), (-+, --, --), (--, +-, --).\n\nTo find the fraction of triangles that are \"balanced\" according to structural balance theory, we need to consider the configurations where all three edges have the same sign. This includes the configurations {+,+,+} and {\u2212,\u2212,\u2212}. \n\nThe fraction of \"balanced\" triangles is therefore 2 out of 8 possible combinations, which simplifies to 1 out of 4 or 25%.",
        "B": "To estimate the fraction of triangles with different configurations, we can consider the total number of triangles calculated previously.\n\nLet's denote:\n- n_{+++} as the number of triangles with the configuration {+,+,+}\n- n_{---} as the number of triangles with the configuration {\u2212,\u2212,\u2212}\n- n_{++,\u2212} as the number of triangles with the configuration {+,+,\u2212}\n- n_{+\u2212,\u2212} as the number of triangles with the configuration {+,\u2212,\u2212}\n\nFrom the values calculated previously, we can determine these quantities.\n\nThe total number of triangles can be calculated as the sum of all these configurations: n_{total} = n_{+++} + n_{---} + n_{++,\u2212} + n_{+\u2212,\u2212}\n\nThe fraction of triangles with each configuration can be calculated as:\n- Fraction of {+,+,+} triangles = n_{+++} / n_{total}\n- Fraction of {\u2212,\u2212,\u2212} triangles = n_{---} / n_{total}\n- Fraction of {+,+,\u2212} triangles = n_{++,\u2212} / n_{total}\n- Fraction of {+,\u2212,\u2212} triangles = n_{+\u2212,\u2212} / n_{total}\n\nTo determine the fraction of \"balanced\" triangles according to structural balance theory, we need to consider the configurations where all edges are positive (+). So, the fraction of balanced triangles = n_{+++} / n_{total}\n\nBy calculating these fractions, we can estimate the proportions of different triangle configurations and the fraction of \"balanced\" triangles."
    },
    {
        "course_id": 15006,
        "question_id": 4219,
        "question": "To which expression is the following for-loop translated? for x <- xs if x > 5; y <- ys yield x + y",
        "question_options": [
            "xs.flatMap(x => ys.map(y => x + y)).withFilter(x => x > 5)",
            "xs.withFilter(x => x > 5).map(x => ys.flatMap(y => x + y))",
            "xs.withFilter(x => x > 5).flatMap(x => ys.map(y => x + y))",
            "xs.map(x => ys.flatMap(y => x + y)).withFilter(x => x > 5)"
        ],
        "A_chat_id": 90897,
        "B_chat_id": 90898,
        "A": "The correct translation of the given for-loop is : xs.withFilter(x => x > 5).flatMap(x => ys.map(y => x + y))",
        "B": "To translate the given for-loop expression `for x <- xs if x > 5; y <- ys yield x + y` into method calls, we can break it down step by step:\n\n1. Filter elements in `xs` that are greater than 5: `xs.withFilter(x => x > 5)`\n2. For each filtered element `x`, generate a new sequence by mapping each element `y` in `ys` to the sum of `x` and `y`: `flatMap(x => ys.map(y => x + y))`\n\nTherefore, the correct translation of the given for-loop expression is:\n[ ] xs.withFilter(x => x > 5).flatMap(x => ys.map(y => x + y))"
    },
    {
        "course_id": 15125,
        "question_id": 3649,
        "question": "10 itemsets out of 100 contain item A, of which 5 also contain B. The rule B -> A has:",
        "question_options": [
            "unknown support and 50% confidence",
            "unknown support and unknown confidence",
            "5% support and 50% confidence",
            "5% support and unknown confidence"
        ],
        "A_chat_id": 90900,
        "B_chat_id": 90901,
        "A": "The rule B -> A has 5% support (5 out of 100 transactions contain both A and B) and 50% confidence (5 out of 10 transactions containing A also contain B). Therefore, the correct answer is 5% support and 50% confidence.",
        "B": "To determine the support and confidence of the rule B -> A, we need to calculate both values based on the given information.\n\nGiven:\n- Number of itemsets containing A = 10\n- Number of itemsets containing both A and B = 5\n\nSupport for the rule B -> A is the percentage of itemsets that contain both B and A out of the total number of itemsets. This can be calculated as follows:\n\nSupport = (Number of itemsets containing both A and B) / (Total number of itemsets containing A)\nSupport = 5 / 10 = 0.5 or 50%\n\nConfidence for the rule B -> A is the percentage of itemsets that contain both B and A out of the total number of itemsets containing B. This can be calculated as follows:\n\nConfidence = (Number of itemsets containing both A and B) / (Total number of itemsets containing B)\nConfidence = 5 / ? (unknown total number of itemsets containing B)\n\nBased on the given information, we can calculate the support as 50% but we cannot determine the confidence since the total number of itemsets containing B is not provided. Therefore, the correct answer is:\n\n[ ] unknown support and unknown confidence"
    },
    {
        "course_id": 15113,
        "question_id": 7076,
        "question": "We consider a ball of mass $m$ sliding without friction in a ring of radius $R$. The ring itself is rotating around the vertical axis passing through its centre $O$ with a constant angular speed $\\boldsymbol{\\omega}$ with respect to the frame of reference of the laboratory. We suppose that the ball can be considered as a material point.\n\nDetermine the equilibrium positions $0\\leq\\theta_1<\\theta_2\\leq\\frac{\\pi}{2}$ of the ball with respect to the ring and the minimal angular speed $\\omega_{\\,\\text{min}}$ for which the equilibrium position $\\theta_2$ exists.\n\n",
        "question_options": null,
        "A_chat_id": 90903,
        "B_chat_id": 90904,
        "A": "Let's denote the equilibrium positions of the ball as $\\theta_1$ and $\\theta_2$, with $0\\leq\\theta_1<\\theta_2\\leq\\frac{\\pi}{2}$. The forces acting on the ball are the gravitational force $m\\boldsymbol{g}$ and the centrifugal force $-mR\\omega^2\\boldsymbol{e}_\\theta$ due to the rotation of the ring.\n\nAt equilibrium, the net force acting on the ball must be zero. This gives us the equation:\n\n\\[m\\boldsymbol{g}\\cos\\theta - mR\\omega^2\\sin\\theta = 0\\]\n\nSolving for $\\theta$, we get:\n\n\\[\\cos\\theta = R\\omega^2/g\\]\n\nFor equilibrium to exist, $\\cos\\theta$ must be less than or equal to 1. This gives us the condition:\n\n\\[R\\omega^2/g \\leq 1\\]\n\nSolving for $\\omega$, we get:\n\n\\[\\omega \\leq \\sqrt{\\frac{g}{R}}\\]\n\nTherefore, the minimal angular speed $\\omega_{\\text{min}}$ for which the equilibrium position $\\theta_2$ exists is $\\omega_{\\text{min}} = \\sqrt{\\frac{g}{R}}$.\n\nAs for the equilibrium positions $\\theta_1$ and $\\theta_2$, we know that $\\cos\\theta_1 = R\\omega^2/g$ and $\\cos\\theta_2 = -R\\omega^2/g$.\n\nSince $0\\leq\\theta_1<\\theta_2\\leq\\frac{\\pi}{2}$, we have that $\\theta_1 = \\cos^{-1}(R\\omega^2/g)$ and $\\theta_2 = \\cos^{-1}(-R\\omega^2/g)$.\n\nThese are the equilibrium positions of the ball with respect to the rotating ring.",
        "B": "To determine the equilibrium positions of the ball in the rotating ring, we need to consider the forces acting on the ball in the rotating frame of reference. In this case, we have two forces acting on the ball: the gravitational force and the centrifugal force due to the rotation of the ring.\n\nLet's denote $\\theta$ as the angle between the vertical axis passing through the center of the ring and the line connecting the center of the ring to the ball. The forces acting on the ball can be decomposed into radial and tangential components:\n\n1. Radial forces:\n- The gravitational force $mg\\cos\\theta$ points towards the center of the ring.\n- The centrifugal force $mR\\omega^2\\sin\\theta$ points away from the center of the ring.\n\nThe radial equilibrium condition is given by:\n\\[ mR\\omega^2\\sin\\theta = mg\\cos\\theta. \\]\n\n2. Tangential forces:\n- The component of the gravitational force in the tangential direction is $mg\\sin\\theta$.\n- The tangential acceleration of the ball in the rotating frame is $R\\omega^2\\cos\\theta$.\n\nThe tangential equilibrium condition is given by:\n\\[ mg\\sin\\theta = mR\\omega^2\\cos\\theta. \\]\n\nFrom the tangential equilibrium condition, we find:\n\\[ \\tan\\theta = \\frac{\\omega^2 R}{g}. \\]\n\nThe equilibrium positions $\\theta_1$ and $\\theta_2$ correspond to the points where the ball remains at rest in the rotating frame. This occurs when the radial and tangential forces balance each other. Let's solve for $\\theta_1$ and $\\theta_2$ by considering the range $0\\leq\\theta\\leq\\frac{\\pi}{2}$.\n\n1. For the lower equilibrium position $\\theta_1$:\n\\[ \\tan\\theta_1 = \\frac{\\omega^2 R}{g} \\]\n\\[ \\theta_1 = \\arctan\\left(\\frac{\\omega^2 R}{g}\\right). \\]\n\n2. For the upper equilibrium position $\\theta_2$:\n\\[ \\tan\\theta_2 = \\frac{\\omega^2 R}{g} \\]\n\\[ \\theta_2 = \\arctan\\left(\\frac{\\omega^2 R}{g}\\right). \\]\n\nFor the equilibrium position $\\theta_2$ to exist, the angle $\\theta_2$ must be within the range $0\\leq\\theta_2\\leq\\frac{\\pi}{2}$. Therefore, the minimal angular speed $\\omega_{\\text{min}}$ for which the equilibrium position $\\theta_2$ exists is determined by the condition $\\theta_2\\leq\\frac{\\pi}{2}$.\n\nFinally, the equilibrium positions $\\theta_1$ and $\\theta_2$ and the minimal angular speed $\\omega_{\\text{min}}$ can be determined by solving the above equations."
    },
    {
        "course_id": 15125,
        "question_id": 1429,
        "question": "TransE is known to have difficulties in representing symmetric relationships r. Which of the following statements is correct for a symmetric relationship and provides insight for this problem?",
        "question_options": [
            "For all e1 , e2: \\( f(e_1, r, e_2) = f(e_2, r, e_1)  \\)",
            "For all e1 , e2: \\( f(e_1, r, e_2) = -f(e_2, r, e_1) \\)",
            "\\( \\Sigma_{ e_1, e_2} f(e_1, r, e_2) + f(e_2, r, e_1) \\)\u00a0is minimized if the embedding vector of r is large",
            "\\( \\Sigma_{ e_1, e_2} f(e_1, r, e_2) + f(e_2, r, e_1) \\) is minimized if the embedding vectors of e1and e2 are close to each other"
        ],
        "A_chat_id": 90909,
        "B_chat_id": 90910,
        "A": "For a symmetric relationship r, the correct statement is: For all e1, e2: \\( f(e_1, r, e_2) = f(e_2, r, e_1) \\). This means that the scoring function should return the same score regardless of the order of the entities in the relationship, which is a characteristic of symmetric relationships. This property can help alleviate the difficulties TransE faces in representing symmetric relationships.",
        "B": "The correct statement for a symmetric relationship and the insight for the problem with TransE is:\n\n[ ] For all e1 , e2: \\( f(e_1, r, e_2) = f(e_2, r, e_1) \\)\n\nThis statement reflects the property of a symmetric relationship, where the scoring function should return the same score regardless of the order of the entities involved in the relationship. In TransE, which models relationships by translating the embeddings of entities, this symmetry property may not hold for symmetric relationships leading to difficulties in representing them accurately."
    },
    {
        "course_id": 15127,
        "question_id": 3910,
        "question": "(Nearest Neighbor) The training error of the 1-nearest neighbor classifier is zero.",
        "question_options": [
            "True",
            "False"
        ],
        "A_chat_id": 90911,
        "B_chat_id": 90912,
        "A": "True. The training error of the 1-nearest neighbor classifier is always zero, as it simply memorizes the training data and assigns the label of the nearest data point in the training set to any new data point.",
        "B": "[ ] True\n[\u2714\ufe0f] False\n\nExplanation: The training error of the 1-nearest neighbor classifier will be zero only if the data points in the training set are all well separated and there are no ties in the distances between points. In practice, it is rare to have a training error of zero with the 1-nearest neighbor classifier, as there may be instances where multiple points have the same nearest neighbor distance, leading to misclassifications."
    },
    {
        "course_id": 15118,
        "question_id": 6912,
        "question": "A car drives over a very bumpy road (see diagram). We model the car as a material point of mass $m$, riding at constant horizontal speed $v_x$. The material point is attached to a spring with spring constant $k$ and rest length $l_0$. At the other end of the spring is a massless wheel, with negligible radius, that's following the curves of the bumpy road.\n\n\nThe spring remains upright at all times; we'll ignore how it does so, and assume that it has no influence on the motion of the material point.\nThe parameters of the problem are such that the wheel never lifts from the road and that the car never hits the wheel. \nThe road's profile follows a sinusoidal shape. The bumps have height $H$ and length $L$.\nExpress the vertical position of the wheel $h(t)$ as a function of time.\n",
        "question_options": null,
        "A_chat_id": 90914,
        "B_chat_id": 90915,
        "A": "The vertical position of the wheel can be modeled as a sinusoidal function due to the sinusoidal shape of the road's profile. Let's denote the amplitude of the sinusoidal function as A. \n\nAt time t = 0, the material point is at its equilibrium position, which means the spring is at its rest length and the wheel is at height A. Therefore, the initial condition is $h(0) = A$.\n\nThe equation of motion for the system can be derived using Newton's second law. The only force acting on the material point is the spring force, which is given by Hooke's Law: $F_{\\text{spring}} = -k(h - l_0)$. This force causes the material point to oscillate vertically.\n\nApplying Newton's second law gives us the following differential equation:\n\n$$m\\ddot{h} = -k(h - l_0)$$\n\nThis is a second-order linear homogeneous differential equation. The general solution to this differential equation is of the form:\n\n$$h(t) = A\\cos(\\omega t + \\phi) + l_0$$\n\nwhere $\\omega = \\sqrt{\\frac{k}{m}}$ is the angular frequency of the oscillation and $\\phi$ is the phase constant.\n\nUsing the initial condition $h(0) = A$, we find that $\\phi = 0$.\n\nTherefore, the vertical position of the wheel as a function of time is:\n\n$$h(t) = A\\cos(\\omega t) + l_0$$",
        "B": "To find the vertical position of the wheel as a function of time, we can break down the problem into different parts:\n\n1. **Initial Conditions:** Let's start by setting up the initial conditions of the problem. At $t=0$, the car is at its equilibrium position and the spring is unstretched. Therefore, the initial vertical position of the wheel is $h(0) = -L_0$, where $L_0$ is the equilibrium length of the spring.\n\n2. **Equation of Motion:** The forces acting on the mass point are the gravitational force and the spring force. The gravitational force is $mg$ downward, and the spring force is $k(h(t) - l_0)$ upward, where $h(t)$ is the vertical position of the wheel at time $t$. The equation of motion for the mass point is given by:\n$$m\\frac{d^2h}{dt^2} = -mg + k(l_0 - h)$$\n\n3. **Solving the Differential Equation:** We can solve this differential equation to find $h(t)$. First, let's rearrange the equation:\n$$\\frac{d^2h}{dt^2} + \\frac{k}{m}h = \\frac{k}{m}l_0 - g$$\n\nThis is a second-order linear differential equation with constant coefficients. The general solution to this differential equation will be a combination of the complementary function and the particular integral. \n\n4. **Particular Integral:** To find the particular integral, we assume a solution of the form $h_p(t) = A\\cos(\\omega t + \\phi)$, where $\\omega$ is the angular frequency of the oscillation. Substituting this into the differential equation and solving for $A$, we can find the particular integral.\n\n5. **Complementary Function:** The complementary function is the solution to the homogeneous equation obtained by setting the right-hand side of the differential equation to zero. The solution will be of the form $h_c(t) = B\\cos(\\omega_0 t) + C\\sin(\\omega_0 t)$, where $\\omega_0 = \\sqrt{\\frac{k}{m}}$.\n\n6. **General Solution:** The general solution to the differential equation will be the sum of the particular integral and the complementary function:\n$$h(t) = h_p(t) + h_c(t)$$\n\nBy substituting the initial conditions into the general solution, we can determine the constants $A$, $B$, and $C.\n\n7. **Final Expression for $h(t)$:** Once we have the constants, we can express the vertical position of the wheel $h(t)$ as a function of time.\n\nThis step-by-step approach will help us tackle the problem systematically and find the desired expression for $h(t)$. Let's proceed with solving the differential equation and finding the constants to obtain the final expression for $h(t)$."
    },
    {
        "course_id": 15116,
        "question_id": 7237,
        "question": "Two disks A and B of radius R, aligned, can rotate around a vertical axis $z$ inside a container filled with a fluid of viscosity $\\eta$. The distances between A and B and between A and the bottom of the container are equal to $d$. The disk B rotates at constant angular velocity $\\omega_0~\\vec{e}_z$.\nAt $t=0$ the angular velocity of A is $\\omega=0$.\n\nWhat is the temporal dependency of $\\omega$?\n",
        "question_options": null,
        "A_chat_id": 90919,
        "B_chat_id": 90920,
        "A": "The angular velocity of disk A, denoted as $\\omega$, will increase over time due to the fluid viscosity acting as a resistance to the motion. The torque exerted on disk A can be expressed as $\\tau = -\\eta \\omega$.\n\nApplying the rotational analog of Newton's second law, we have $\\tau = I \\frac{d\\omega}{dt}$, where $I$ is the moment of inertia of disk A.\n\nThe moment of inertia for a disk rotating around its axis perpendicular to its surface is $I = \\frac{1}{2}MR^2$.\n\nSubstituting the torque and moment of inertia into the equation gives $-\\eta \\omega = \\frac{1}{2}MR^2 \\frac{d\\omega}{dt}$.\n\nRearranging and integrating the equation gives $\\int \\frac{d\\omega}{\\omega} = -\\frac{2\\eta}{MR^2} \\int dt$.\n\nSolving the integral yields $\\ln(\\omega) = -\\frac{2\\eta}{MR^2}t + C$, where $C$ is a constant of integration.\n\nExponentiating both sides gives $\\omega = e^{-\\frac{2\\eta}{MR^2}t + C} = Ae^{-\\frac{2\\eta}{MR^2}t}$, where $A$ is another constant.\n\nTherefore, the temporal dependency of $\\omega$ is exponential decay, where the angular velocity of disk A decreases over time.",
        "B": "To determine the temporal dependency of the angular velocity $\\omega$ of disk A, we can analyze the fluid flow between the disks A and B. The fluid between the disks will experience shear due to the rotation of disk B, which will exert a torque on disk A causing it to start rotating.\n\nThe torque on disk A can be determined using the viscous drag force between the fluid and the disk. The torque on disk A is given by:\n\n$\\tau = I\\alpha = R^2 \\eta \\frac{d\\omega}{dz}$\n\nWhere $I$ is the moment of inertia of disk A, $\\alpha$ is the angular acceleration, $R$ is the radius of the disk, $\\eta$ is the viscosity of the fluid, and $d\\omega/dz$ is the angular velocity gradient along the $z$ axis.\n\nAssuming the fluid flow between the disks is laminar, we can use the formula for the viscous drag force between two parallel plates:\n\n$F = \\eta A \\frac{dv}{dx}$\n\nWhere $F$ is the viscous drag force, $\\eta$ is the viscosity of the fluid, $A$ is the area of the plates, $v$ is the velocity of the fluid, and $dx$ is the distance between the plates.\n\nNow, let's consider the fluid flow between the disks. The velocity of the fluid between the disks can be related to the angular velocities of the disks by the equation:\n\n$v = R\\omega - \\omega_0 d$\n\nThis equation represents the relative velocity of the fluid with respect to disk A due to the rotation of disk B.\n\nNow, we can determine the torque on disk A by substituting the expression for the velocity of the fluid between the disks into the formula for the torque:\n\n$\\tau = R^2 \\eta \\frac{d}{dz}(R\\omega - \\omega_0 d)$\n\nSolving for $\\omega$ will give us the temporal dependency of the angular velocity of disk A. The solution will depend on the specific form of the velocity gradient $d\\omega/dz$ and the boundary conditions of the system."
    },
    {
        "course_id": 15113,
        "question_id": 7053,
        "question": "A missile is launched with an initial velocity $\\boldsymbol{v_0}$ from point $O$. At the same time, a material point is launched with no initial velocity from point $P$. \n\nShow that if the missile is initially launched along the line of fire $\\boldsymbol{OP}$, then the missile will inevitably hit the material point.\n",
        "question_options": null,
        "A_chat_id": 90922,
        "B_chat_id": 90923,
        "A": "Since the missile is launched along the line of fire $\\boldsymbol{OP}$, the initial velocity of the missile $\\boldsymbol{v_0}$ is in the same direction as the position vector $\\boldsymbol{r}$ from the missile to the material point. This means that the angle between the velocity of the missile and the position vector is $0$ degrees.\n\nThe relative velocity $\\boldsymbol{v_{rel}}$ of the missile with respect to the material point is given by:\n$$\\boldsymbol{v_{rel}} = \\boldsymbol{v_{missile}} - \\boldsymbol{v_{point}}$$\n\nSince the material point is launched with no initial velocity, $\\boldsymbol{v_{point}} = \\boldsymbol{0}$. Therefore, $\\boldsymbol{v_{rel}} = \\boldsymbol{v_{missile}}$.\n\nThe relative velocity is along the line of sight from the missile to the material point, which means that the missile is heading directly towards the material point. Hence, if the missile is initially launched along the line of fire $\\boldsymbol{OP}$, it will inevitably hit the material point.",
        "B": "To show that the missile will inevitably hit the material point if it is initially launched along the line of fire $OP$, we can consider the motion of the missile and the material point.\n\nLet's denote the position vectors of the missile and the material point at time $t$ as $\\boldsymbol{r_m}(t)$ and $\\boldsymbol{r_p}(t)$, respectively.\n\nThe position vector of the material point at any time $t$ can be given by $\\boldsymbol{r_p}(t) = \\boldsymbol{r_0} + \\boldsymbol{v_p}t$, where $\\boldsymbol{r_0}$ is the initial position of the material point and $\\boldsymbol{v_p}$ is its velocity.\n\nSince the missile is initially launched along the line of fire $OP$, we have that the initial position of the missile $\\boldsymbol{r_0}$ coincides with the initial position of the material point. Therefore, $\\boldsymbol{r_m}(0) = \\boldsymbol{r_p}(0)$.\n\nNow, let's consider the position vector of the missile at any time $t$, which can be given by $\\boldsymbol{r_m}(t) = \\boldsymbol{r_0} + \\boldsymbol{v_0}t$.\n\nSince the missile is initially launched along the line of fire $OP$, we can write $\\boldsymbol{v_0} = k\\boldsymbol{v_p}$ for some scalar $k$. This means that the missile's initial velocity is a scalar multiple of the material point's velocity.\n\nNow, for the missile to hit the material point, we need to show that there exists a time $t_{hit}$ such that $\\boldsymbol{r_m}(t_{hit}) = \\boldsymbol{r_p}(t_{hit})$. Substituting the expressions for the position vectors of the missile and the material point, we get:\n\n$$\\boldsymbol{r_0} + \\boldsymbol{v_0}t_{hit} = \\boldsymbol{r_0} + \\boldsymbol{v_p}t_{hit}$$\n\nSince $\\boldsymbol{v_0} = k\\boldsymbol{v_p}$, we have:\n\n$$\\boldsymbol{r_0} + k\\boldsymbol{v_p}t_{hit} = \\boldsymbol{r_0} + \\boldsymbol{v_p}t_{hit}$$\n\nSolving this equation gives $t_{hit} = 0$. Therefore, the missile will inevitably hit the material point when it is initially launched along the line of fire $OP$."
    },
    {
        "course_id": 5,
        "question_id": 6731,
        "question": "Let  $$G= \begin{pmatrix} 1 &1 &1 &0 &1 &0\\ 0 &1 &1 &1 &0 &0\\ 0 &1 &1 &0 &0 &0\\ 0 &1 &1 &1 &0 &1 \\end{pmatrix}$$ be the generator matrix of a $(6,4)$ linear code $\\mathcal C$ over $\\mathbb F_2$. True or false: $d_{min}=2$.",
        "question_options": [
            "False",
            "True"
        ],
        "A_chat_id": 90927,
        "B_chat_id": 90928,
        "A": "False. The minimum distance $d_{min}$ of a linear code is the minimum weight of a non-zero codeword. In this case, the codeword $\\begin{pmatrix} 1 & 1 & 1 & 0 & 1 & 0 \\end{pmatrix}$ has weight 3, so $d_{min}=3$.",
        "B": "To determine the minimum distance of the code $\\mathcal C$ with generator matrix $G$, we need to find the minimum number of linearly dependent columns in $G$. \n\nGiven that the code is a $(6,4)$ linear code, the minimum distance $d_{min}$ is the minimum weight of a non-zero codeword in the code. Since the code is a $(6,4)$ linear code, the minimum weight of a non-zero codeword will correspond to the minimum number of linearly dependent columns in the generator matrix $G$.\n\nLooking at the columns of $G$, we observe that the second and third columns are identical. Thus, these two columns are linearly dependent. \n\nTherefore, the minimum distance $d_{min}$ of the code $\\mathcal C$ is at least 2. \n\nTrue, the statement $d_{min} = 2$ is true."
    },
    {
        "course_id": 0,
        "question_id": 5769,
        "question": "Let $H:\\{0,1\\}^* \\rightarrow \\{0,1\\}^n$ be a hash function and $x_1,x_2\\in\\{0,1\\}^{2n}$ two random different messages. In cryptography, we usually assume that the probability of collision, i.e. $\\Pr[H(x_1)=H(x_2)]$, is close to \\ldots",
        "question_options": [
            "$2^{-\\frac{n}{2}}$.",
            "$2^{-n}$.",
            "$2^{-2n}$.",
            "$0$."
        ],
        "A_chat_id": 90930,
        "B_chat_id": 90931,
        "A": "$2^{-n}$. In cryptography, a good hash function should have a collision probability close to $2^{-n}$, where n is the length of the output of the hash function.",
        "B": "In cryptography, we usually aim for hash functions where the probability of collision is very low. The correct answer in this case is:\n\n[X] $2^{-n}$.\n\nA good hash function should provide a collision probability close to $2^{-n}$, where $n$ is the output length of the hash function. This means that the probability of two different messages hashing to the same value is very small."
    },
    {
        "course_id": 15113,
        "question_id": 7099,
        "question": "We consider the earth as a compact set of concentric spheres.\n\na) Use a geometric argument to show that the field vanishes inside a spherical shell. Thus, for a given point, only the spherical shell for which the point is in the exterior contributes to the field at that point.\n",
        "question_options": null,
        "A_chat_id": 90932,
        "B_chat_id": 90933,
        "A": "a) Inside a spherical shell, the gravitational field due to each shell cancels out. This is because at any point inside the shell, the gravitational force due to one part of the shell is canceled out by the force due to the opposite part of the shell. This cancellation occurs for every concentric spherical shell within the larger sphere, resulting in a net gravitational field of zero inside the spherical shell. Therefore, only the spherical shell for which the point is in the exterior contributes to the field at that point.",
        "B": "Let's consider a point inside a spherical shell within the Earth. Since the Earth is considered a compact set of concentric spheres, the point lies on a smaller sphere within the larger spherical shell.\n\nNow, let's consider the contribution of the smaller sphere to the electric field at the given point. Since the electric field due to a point charge is spherically symmetric, the electric field vectors at different points on the smaller sphere will cancel each other out because they point in different directions. This is because for every point on the smaller sphere, there is an opposite point on the sphere that is diametrically opposite.\n\nTherefore, the electric field due to the smaller sphere cancels out inside the spherical shell, and the net electric field at the given point is contributed only by the spherical shell outside the point.\n\nThis geometric argument shows that the field vanishes inside a spherical shell, and only the spherical shell for which the point is in the exterior contributes to the field at that point."
    },
    {
        "course_id": 15041,
        "question_id": 2828,
        "question": "Your friend Charlie was training a neural net, and observed something very curious. During training, the same network, with the same exact weights, doesn't always give the same prediction for a given example. What can cause this behavior?",
        "question_options": [
            "Residual layers (skip-connections)",
            "Convolutional layers",
            "Batch normalization",
            "SGD with momentum",
            "Adam",
            "Dropout",
            "L1 regularization",
            "L2 regularization",
            "This is not possible, Charlie made a mistake"
        ],
        "A_chat_id": 90937,
        "B_chat_id": 90938,
        "A": "Dropout can cause this behavior. Dropout is a regularization technique commonly used in neural networks to prevent overfitting. During training, a fraction of the neurons are randomly set to zero, which can result in different predictions for the same example with the same weights.",
        "B": "The behavior you described can be caused by a few factors. Let's break them down one by one:\n\n1. Residual layers (skip-connections): Skip connections can allow gradients to flow more easily during training, which can affect the predictions. However, they are unlikely to directly cause different predictions for the same input with the same weights.\n\n2. Convolutional layers: Convolutional layers apply filters to input data, and their output can be affected by the specific patterns in the input. However, if the weights are exactly the same, the output of the convolutional layers should be deterministic.\n\n3. Batch normalization: Batch normalization normalizes the activations of each layer, which can introduce randomness during training due to the batch statistics. This can lead to slightly different predictions for the same input with the same weights.\n\n4. SGD with momentum: SGD with momentum can cause the optimizer to overshoot local minima, which can introduce some randomness in the training process. This can also lead to different predictions for the same input with the same weights.\n\n5. Adam: Adam optimizer also has adaptive learning rates and momentum, which can introduce randomness in the optimization process. This can lead to different predictions for the same input with the same weights.\n\n6. Dropout: Dropout randomly drops units during training, which can lead to different predictions for the same input with the same weights.\n\n7. L1 regularization and L2 regularization: Regularization techniques can affect the optimization process by penalizing large weights. This can also lead to different predictions for the same input with the same weights.\n\nGiven these factors, the most likely culprits for the observed behavior are batch normalization, SGD with momentum, Adam, dropout, and regularization techniques. Therefore, the correct options are:\n- [ ] Batch normalization\n- [ ] SGD with momentum\n- [ ] Adam\n- [ ] Dropout\n- [ ] L1 regularization\n- [ ] L2 regularization"
    },
    {
        "course_id": 0,
        "question_id": 5891,
        "question": "Which of these plaintexts can be the result of the decryption of \\texttt{SERGEV} using a simple subsitution cipher?",
        "question_options": [
            "\\texttt{VERNAM}",
            "\\texttt{ENIGMA}",
            "\\texttt{TURING}",
            "\\texttt{CAESAR}"
        ],
        "A_chat_id": 90940,
        "B_chat_id": 90941,
        "A": "The plaintext \\texttt{CAESAR} can be the result of the decryption of \\texttt{SERGEV} using a simple substitution cipher, where each letter is shifted by a fixed number of positions in the alphabet.",
        "B": "To decrypt a simple substitution cipher, we need to know the exact mapping of each letter in the ciphertext to its corresponding plaintext letter. \n\nFor the given plaintext \"SERGEV\", we can see that there is no direct mapping to any of the provided options:\n- \"VERNAM\" has a 'V' in the second position, which does not match any letter in \"SERGEV\".\n- \"ENIGMA\" has an 'E' in the first position, which does not match the first letter 'S' in \"SERGEV\".\n- \"TURING\" has a 'U' in the second position, which does not match any letter in \"SERGEV\".\n- \"CAESAR\" has an 'A' in the third position, which does not match any letter in \"SERGEV\".\n\nTherefore, none of the provided options can be the result of decrypting \"SERGEV\" using a simple substitution cipher."
    },
    {
        "course_id": 15118,
        "question_id": 6905,
        "question": "A spider of mass $m$ is suspended by its thread to a tree. The thread has elastic constant $k$ and rest length $L$ and the tree is assumed to stand still.  Gravity acts on the spider which oscillates vertically around its equilibrium position.\nWrite the spider's equation of motion",
        "question_options": null,
        "A_chat_id": 90944,
        "B_chat_id": 90945,
        "A": "The spider's equation of motion can be derived using Newton's second law. The forces acting on the spider are the tension in the thread and the gravitational force. \n\nLet $y(t)$ be the displacement of the spider from its equilibrium position at time $t$. The restoring force due to the thread can be expressed as $-ky$, where $k$ is the elastic constant of the thread. The gravitational force acting on the spider is $-mg$, where $m$ is the mass of the spider and $g$ is the acceleration due to gravity.\n\nApplying Newton's second law, we have:\n\n$$m\\ddot{y} = -ky - mg$$\n\nwhere $\\ddot{y}$ is the acceleration of the spider. This is the equation of motion for the spider oscillating vertically around its equilibrium position.",
        "B": "To write the spider's equation of motion, we first need to consider the forces acting on the spider. \n\n1. Gravity: The force due to gravity acting on the spider is $mg$ in the downward direction, where $m$ is the mass of the spider and $g$ is the acceleration due to gravity.\n\n2. Tension in the thread: The tension in the thread acts in the upward direction and varies with the displacement of the spider from its equilibrium position. When the spider is displaced downward by a distance $x$ from the equilibrium position, the tension in the thread can be calculated using Hooke's Law as $kx$, where $k$ is the elastic constant of the thread.\n\nThe net force acting on the spider is the sum of the forces due to gravity and tension in the thread. Since the spider is oscillating vertically, we can write the equation of motion using Newton's second law:\n\n\\[ m\\frac{d^2x}{dt^2} = mg - kx \\]\n\nThis is the differential equation that describes the motion of the spider. Solving this equation will give us the motion of the spider as a function of time."
    },
    {
        "course_id": 15103,
        "question_id": 1754,
        "question": "Assume you are working on a text editor written in Java. Your colleague is interested in optimizing a wrapper around \"String.substring()\" used to let users copy and paste parts of the text and decides to write some benchmarks to measure the current performance of the feature. How would you suggest that he proceeds?",
        "question_options": null,
        "A_chat_id": 90946,
        "B_chat_id": 90947,
        "A": "I would suggest that your colleague first identifies the critical parts of the code that need to be optimized, such as the \"String.substring()\" method calls. He can then create different test cases that mimic real-world scenarios where users would be copying and pasting text. These test cases should cover a range of different input sizes and scenarios to ensure a comprehensive benchmark.\n\nNext, your colleague can use benchmarking tools in Java, such as JMH (Java Microbenchmarking Harness), to measure the performance of the current implementation. By running these benchmarks multiple times and analyzing the results, he can identify the bottlenecks and areas for optimization.\n\nAdditionally, it would be beneficial for your colleague to compare the performance of the current implementation with potential optimized versions, such as using a different data structure or algorithm for copying and pasting text. This will help him understand the impact of the optimizations and make informed decisions on how to proceed with improving the performance of the text editor.",
        "B": "Here are some steps your colleague can follow to benchmark the performance of the \"String.substring()\" wrapper in the text editor:\n\n1. **Define Benchmark Goals**: Clearly define the goals of the benchmarking process. Is the goal to measure the speed of the current implementation, compare different implementation approaches, or identify bottlenecks in the code?\n\n2. **Select Benchmarking Tool**: Choose a suitable benchmarking tool for Java, such as JMH (Java Microbenchmark Harness). JMH is a popular tool for writing, running, and analyzing microbenchmarks in Java.\n\n3. **Write Benchmark Code**: Write a simple benchmark code that simulates the usage of the \"String.substring()\" wrapper in the text editor. The benchmark code should be representative of real-world usage scenarios.\n\n4. **Analyze Warm-up Time**: Run the benchmark code with a warm-up period to allow the Just-In-Time (JIT) compiler to optimize the code. This helps in obtaining more accurate benchmark results.\n\n5. **Run Benchmark Tests**: Run the benchmark tests multiple times to ensure consistency in results. Measure performance metrics such as throughput, average latency, and memory usage.\n\n6. **Analyze Results**: Analyze the benchmark results to identify any performance bottlenecks or areas for improvement in the \"String.substring()\" wrapper implementation.\n\n7. **Optimize and Retest**: If performance improvements are needed, optimize the implementation of the \"String.substring()\" wrapper and re-run the benchmark tests to measure the impact of the changes.\n\n8. **Iterate**: Continue iterating on the optimization process by making incremental changes to the implementation, retesting with benchmarks, and analyzing the results until the desired performance level is achieved.\n\nBy following these steps, your colleague can effectively benchmark the performance of the \"String.substring()\" wrapper in the text editor and make informed decisions on optimizing its implementation."
    },
    {
        "course_id": 15115,
        "question_id": 7308,
        "question": "A charged body with $Q=-1~\\mu C$ is dropped from a height $h=1~m$ above the Earth surface. A plate charged with $10^{19}$ electrons is placed exactly below it on the surface.\nBy using the concept of potential energy, evaluate the mass $M$ of the body in order to let it float at height $h$ \\textit{(remember that the gravitational potential energy at distance $z$ close to the Earth surface can be approximated as $U=Mgz$, with $g=9.81~m/s^2$)}.",
        "question_options": null,
        "A_chat_id": 90952,
        "B_chat_id": 90953,
        "A": "The potential energy of the charged body at a height $h$ above the Earth's surface is given by the sum of its gravitational potential energy and electrical potential energy. The gravitational potential energy at height $h$ is $U_{grav} = Mgh$, where $M$ is the mass of the body and $g = 9.81~m/s^2$ is the acceleration due to gravity.\n\nThe electrical potential energy of the charged body with charge $Q$ at a distance $r$ from the plate with charge $Q_2$ is given by the formula $U_{elec} = \\frac{k|Q||Q_2|}{r}$, where $k$ is the Coulomb constant.\n\nFor the body to float at height $h$, the total potential energy (gravitational + electrical) should be equal to zero. Therefore, we have:\n$U_{grav} + U_{elec} = 0$\n$Mgh + \\frac{k|Q||Q_2|}{r} = 0$\n$Mgh = \\frac{k|Q||Q_2|}{r}$\n$M = \\frac{k|Q||Q_2|}{rgh}$\n\nSubstitute the given values: $Q = -1~\\mu C$, $Q_2 = 10^{19}$ electrons, $r = 1~m$, $g = 9.81~m/s^2$, and $h = 1~m$ into the equation above to find the mass $M$ required for the body to float at height $h$.",
        "B": "To determine the mass of the body that will allow it to float at a height of 1 meter above the Earth's surface, we need to consider the balance between the electrical potential energy and the gravitational potential energy.\n\nThe electric potential energy of the charged body is given by the formula:\n\n\\[ U_{elec} = \\frac{k|Q|N}{r} \\]\n\nwhere:\n- $k$ is the Coulomb constant ($8.99 \\times 10^9~N m^2/C^2$),\n- $Q$ is the charge of the body ($-1~\\mu C = -1 \\times 10^{-6}~C$),\n- $N$ is the number of electrons on the plate ($10^{19}$),\n- $r$ is the distance between the charged body and the plate (which is the height $h = 1~m$ in this case).\n\nThe gravitational potential energy of the body at height $h$ above the Earth's surface is given by the formula you provided:\n\n\\[ U_{grav} = Mgh \\]\n\nTo let the body float at height $h$, the electric potential energy must balance the gravitational potential energy:\n\n\\[ U_{elec} = U_{grav} \\]\n\n\\[ \\frac{k|Q|N}{r} = Mgh \\]\n\nSubstitute the given values:\n\n\\[ \\frac{(8.99 \\times 10^9) \\times 1 \\times 10^{-6} \\times 10^{19}}{1} = M \\times 9.81 \\times 1 \\]\n\n\\[ M = \\frac{(8.99 \\times 10^9) \\times 1 \\times 10^{-6} \\times 10^{19}}{9.81} \\]\n\n\\[ M \\approx 9.19 \\times 10^{12}~kg \\]\n\nTherefore, the mass of the body needed to let it float at a height of 1 meter above the Earth's surface is approximately $9.19 \\times 10^{12}~kg$."
    },
    {
        "course_id": 15126,
        "question_id": 2339,
        "question": "Why does Intel Itanium contain more general-purpose registers (128) than most RISC instruction sets (usually 32)?",
        "question_options": null,
        "A_chat_id": 90957,
        "B_chat_id": 90958,
        "A": "The reason Intel Itanium contains more general-purpose registers than most RISC instruction sets is because Itanium is a VLIW (Very Long Instruction Word) architecture, which relies on the compiler to schedule instructions in parallel. Having a larger number of registers allows for more flexibility in instruction scheduling and optimization by the compiler. RISC architectures, on the other hand, rely on simpler instructions and fewer registers to achieve high performance through pipelining and efficient use of registers.",
        "B": "One of the main reasons why Intel Itanium contains more general-purpose registers (128) compared to most RISC instruction sets (usually around 32) is because Itanium was designed to execute very long instruction words (VLIW - Very Long Instruction Word) with explicit parallelism. \n\nIn VLIW architectures like Itanium, the compiler is responsible for scheduling instructions and specifying dependencies between operations. Having a large number of general-purpose registers allows the compiler to efficiently schedule and exploit parallelism within the instruction stream.\n\nAdditionally, the large number of registers in Itanium helps reduce memory accesses and improves performance by allowing more data to be kept in registers for faster access, reducing the need to frequently load and store data to and from memory.\n\nIn contrast, traditional RISC architectures typically rely on simpler instruction sets with fewer registers to streamline instruction decoding and execution. RISC architectures often rely on register renaming techniques and other optimizations to handle parallelism and reduce the impact of memory latency.\n\nOverall, the design choices in Itanium with a large number of registers were made to support the VLIW architecture and enable high-performance parallel processing."
    },
    {
        "course_id": 15125,
        "question_id": 2636,
        "question": "Which of the following is TRUE for Recommender Systems (RS)?",
        "question_options": [
            "The complexity of the Content-based RS depends on the number of users",
            "Item-based RS need not only the ratings but also the item features",
            "Matrix Factorization is typically robust to the cold-start problem.",
            "Matrix Factorization can predict a score for any user-item combination in the dataset."
        ],
        "A_chat_id": 90960,
        "B_chat_id": 90961,
        "A": "Matrix Factorization can predict a score for any user-item combination in the dataset. This is true for Matrix Factorization based Recommender Systems, where the goal is to predict the missing entries in the user-item interaction matrix. Content-based RS complexity depends on the number of items, not users. Item-based RS only require ratings, not item features. Matrix Factorization is not robust to the cold-start problem, as it requires historical interaction data to make accurate predictions.",
        "B": "Let's evaluate each statement:\n\n1. The complexity of the Content-based RS depends on the number of users - This statement is generally not true. The complexity of a content-based RS typically depends more on the number of items or features rather than the number of users.\n\n2. Item-based RS need not only the ratings but also the item features - This statement is true. Item-based RS rely on item features in addition to ratings to make recommendations.\n\n3. Matrix Factorization is typically robust to the cold-start problem - This statement is true. Matrix Factorization techniques can help in alleviating the cold-start problem by making use of the latent factors learned from the data.\n\n4. Matrix Factorization can predict a score for any user-item combination in the dataset - This statement is true. Matrix Factorization models can predict ratings for user-item combinations even if there is no explicit rating available in the dataset.\n\nTherefore, the correct statements are:\n- [x] Item-based RS need not only the ratings but also the item features\n- [x] Matrix Factorization is typically robust to the cold-start problem.\n- [x] Matrix Factorization can predict a score for any user-item combination in the dataset."
    },
    {
        "course_id": 15113,
        "question_id": 7085,
        "question": "A ball of mass $m$ is hanging (vertically) on a spring of elastic constant $k$ and of natural length $l_0$. The suspension point of the spring is attached to the ceiling at height $H$.\n\nDetermine the equilibrium position of the ball.\n",
        "question_options": null,
        "A_chat_id": 90965,
        "B_chat_id": 90966,
        "A": "At equilibrium, the weight of the ball is balanced by the tension in the spring. The gravitational force on the ball is given by $mg$, where $m$ is the mass of the ball and $g$ is the acceleration due to gravity. The tension in the spring is given by $T = k(l-l_0)$, where $l$ is the length of the spring at equilibrium.\n\nSince the ball is hanging vertically, the forces are balanced in the vertical direction. Therefore, we have:\n\n$mg = k(l-l_0)$\n\nSolving for $l$, we get:\n\n$l = \\frac{mg}{k} + l_0$\n\nTherefore, the equilibrium position of the ball is at a distance $\\frac{mg}{k} + l_0$ below the ceiling.",
        "B": "To determine the equilibrium position of the ball, we need to consider the forces acting on it at that position. \n\nAt the equilibrium position, the forces acting on the ball are its weight $mg$ acting downward, the restoring force due to the spring, and the tension in the spring.\n\nThe weight of the ball is balanced by the tension in the spring. The restoring force due to the spring is equal in magnitude and opposite in direction to the weight of the ball.\n\nLet's consider the position of the ball when it is in equilibrium. The spring will be stretched by a distance $l$ from its natural length $l_0$. The total elongation of the spring will be $H+l$.\n\nThe force due to the spring is given by Hooke's Law:\n\n$$F_{\\text{spring}} = -k(l-l_0)$$\n\nAt equilibrium, this force must balance the weight of the ball:\n\n$$mg = k(H+l-l_0)$$\n\nSolving for $l$, we get:\n\n$$l = \\frac{mg}{k} - H + l_0$$\n\nTherefore, the equilibrium position of the ball is at a distance $\\frac{mg}{k} - H + l_0$ below the ceiling."
    },
    {
        "course_id": 15006,
        "question_id": 4203,
        "question": "An expression is referentially transparent if it always returns the same value, no matter\nthe global state of the program. A referentially transparent expression can be replaced by its value without\nchanging the result of the program.\nSay we have a value representing a class of students and their GPAs. Given the following defintions:\n1 case class Student(gpa: Double)\n2\n3 def count(c: List[Student], student: Student): Double =\n4 c.filter(s => s == student).size\n5\n6 val students = List(\n7 Student(1.0), Student(2.0), Student(3.0),\n8 Student(4.0), Student(5.0), Student(6.0)\n9 )\nAnd the expression e:\n1 count(students, Student(6.0))\nIs the expression e referentially transparent?",
        "question_options": null,
        "A_chat_id": 90970,
        "B_chat_id": 90971,
        "A": "No, the expression e is not referentially transparent because the result of the expression depends on the global state of the program, specifically the contents of the list \"students\". If the list \"students\" is modified or if the global state of the program changes, the result of the expression may also change.",
        "B": "The expression e, count(students, Student(6.0)), is not referentially transparent. \n\nThis is because the count function relies on the List c which is passed as a parameter. The count function filters the list based on a condition (in this case, comparing the student to each element in the list), and then returns the size of the filtered list. The result of this function depends on the specific list passed to it, thus making it dependent on the global state of the program (the contents of the list c).\n\nIf the list c were to change in the program, for example, by adding or removing elements, the result of the count function would also change. This violates the property of referential transparency, as the expression cannot be replaced by a fixed value without considering the global state of the program."
    },
    {
        "course_id": 15125,
        "question_id": 3630,
        "question": "What is WRONG\u00a0regarding the Transformer model?",
        "question_options": [
            "It uses a self-attention mechanism to compute representations of the input and output.",
            "Its computation cannot be parallelized compared to LSTMs and other sequential models.",
            "Its complexity is quadratic to the input size.",
            "It captures the semantic context of the input."
        ],
        "A_chat_id": 90972,
        "B_chat_id": 90973,
        "A": "The wrong statement is: Its computation cannot be parallelized compared to LSTMs and other sequential models. The Transformer model can be parallelized efficiently due to its self-attention mechanism, allowing for faster training and inference compared to sequential models like LSTMs.",
        "B": "The correct option that is wrong regarding the Transformer model is:\n\n[ ] Its computation cannot be parallelized compared to LSTMs and other sequential models.\n\nExplanation: \n- The Transformer model is designed to enable parallelization of computation across the input sequence, making it more efficient in terms of training time compared to sequential models like LSTMs."
    },
    {
        "course_id": 0,
        "question_id": 6123,
        "question": "A simple substitution cipher can be broken \\dots",
        "question_options": [
            "by analysing the probability occurence of the language.",
            "only by using a quantum computer.",
            "by using the ENIGMA machine.",
            "by using public-key cryptogaphy."
        ],
        "A_chat_id": 90975,
        "B_chat_id": 90976,
        "A": "The false statement is : by using public-key cryptography. A simple substitution cipher can be broken by analyzing the probability occurrence of the language, using frequency analysis, and other techniques. It does not require a quantum computer or the use of the ENIGMA machine. Public-key cryptography is not directly related to breaking a simple substitution cipher.",
        "B": "[ ] by analysing the probability occurence of the language."
    },
    {
        "course_id": 15011,
        "question_id": 5185,
        "question": " Design and analyze a  polynomial-time algorithm for the following problem: \\begin{center} \\begin{boxedminipage}[t]{0.83\\textwidth} \\begin{description} \\item[Input:] a vertex set $V$. \\item[Output:]  vertex subsets $S_1, S_2, \\ldots, S_\\ell \\subseteq V$ with the following property:\\\\[2mm] For every set of edges $E\\subseteq {V \\choose 2}$, there is an $i\\in \\{1,2, \\ldots, \\ell\\}$ such that \\begin{align*} |\\{e\\in E: |e\\cap S_i| = 1\\}| \\geq |E|/2\\,, \\end{align*} i.e., $S_i$ cuts at least half the edges in $G = (V,E)$. \\end{description} \\end{boxedminipage} \\end{center} We remark that, since your algorithm should run in time polynomial in $n=|V|$, it can output at most polynomially (in $n$) many vertex sets. We also emphasize that the algorithm does \\textbf{not} take the edge set $E$ as input. {\\em (In this problem you are asked to (i) design the algorithm, (ii) show that it runs in time polynomial in $n$, and (iii) prove that the output satisfies the property given in the problem statement. Recall that you are allowed to refer to material covered in the lecture notes.)}",
        "question_options": null,
        "A_chat_id": 90977,
        "B_chat_id": 90978,
        "A": "(i) The algorithm works as follows:\n\n1. Start with an empty set $S_i$.\n2. For each vertex $v \\in V$, add $v$ to $S_i$ with probability $\\frac{1}{2}$.\n3. Repeat step 2 for $\\sqrt{n}$ times to generate $\\sqrt{n}$ sets $S_1, S_2, ..., S_{\\sqrt{n}}$.\n\n(ii) The algorithm runs in polynomial time because for each vertex $v \\in V$, we do a constant number of operations (deciding whether to add $v$ to $S_i$ or not) $\\sqrt{n}$ times. Therefore, the total number of operations is polynomial in $n$.\n\n(iii) To prove that the output satisfies the property given in the problem statement, consider any arbitrary set of edges $E \\subseteq {V \\choose 2}$. The probability that a particular edge $e \\in E$ is cut by a random set $S_i$ is $\\frac{1}{2}$. Therefore, the expected number of edges cut by $S_i$ is $\\frac{|E|}{2}$. By repeating the process $\\sqrt{n}$ times and forming $\\sqrt{n}$ sets, at least one of the sets $S_i$ will cut at least half the edges in $E$ with high probability.",
        "B": "To design a polynomial-time algorithm for this problem, we can follow these steps:\n\n1. Construct a graph $G = (V, E)$ where $V$ is the given vertex set and $E = {V \\choose 2}$ is the set of all possible edges between vertices in $V$.\n\n2. For each vertex $v \\in V$, compute its degree $d(v)$, which is the number of edges incident to $v$.\n\n3. Initialize an empty list $S$ to store the output vertex subsets.\n\n4. Iterate through the vertices $v \\in V$ in any order. For each vertex $v$, add it to a candidate set $S_{\\text{candidate}}$ if the number of edges incident to $v$ is at least half of the total possible edges incident to $v$, i.e., if $d(v) \\geq \\frac{|V|-1}{2}$.\n\n5. After iterating through all vertices, add the candidate set $S_{\\text{candidate}}$ to the list $S$.\n\n6. Output the list of subsets $S$ as the final result.\n\nNow let's analyze the runtime complexity of this algorithm:\n\n- Computing the degree of each vertex in step 2 takes $O(n^2)$ time, where $n = |V|$.\n- Iterating through all vertices in step 4 takes $O(n)$ time.\n- Checking the condition for each vertex in step 4 takes $O(1)$ time.\n- Adding the candidate set to the list in step 5 takes $O(1)$ time.\n\nTherefore, the total runtime complexity of the algorithm is $O(n^2)$, which is polynomial in the size of the input $n$.\n\nTo prove that the output satisfies the property given in the problem statement, we need to show that for any set of edges $E \\subseteq {V \\choose 2}$, there exists a subset $S_i$ in the output such that $|{\\{e \\in E: |e \\cap S_i| = 1\\}}| \\geq |E|/2$.\n\nGiven the construction of the algorithm, any vertex added to $S_{\\text{candidate}}$ has at least half of the edges incident to it. Therefore, for any set of edges $E$, there will be a subset $S_i$ in the output that contains at least half of the edges in $E$ incident to its vertices. This property is guaranteed by the condition used to add vertices to $S_{\\text{candidate}}$.\n\nTherefore, the algorithm correctly outputs vertex subsets $S_1, S_2, \\ldots, S_\\ell$ that satisfy the given property in the problem statement."
    },
    {
        "course_id": 15125,
        "question_id": 2641,
        "question": "In classification, which of the following is true regarding class imbalance?",
        "question_options": [
            "Classes should have the same distribution in the validation set and in the full dataset.",
            "Oversampling the larger class can reduce the impact of the skewed distribution.",
            "Oversampling rare classes in the testing set can reduce the impact of skewed distribution.",
            "The leave-one-out methodology produces the same class distribution in the training and the testing set."
        ],
        "A_chat_id": 90982,
        "B_chat_id": 90983,
        "A": "Oversampling the larger class can reduce the impact of the skewed distribution. This is a common technique used to address class imbalance in classification tasks. The other options are not generally true in the context of class imbalance.",
        "B": "[X] Oversampling the larger class can reduce the impact of the skewed distribution.\n\nClass imbalance occurs when one class has significantly more samples than another class in a classification problem. This can lead to biased models that perform poorly on the minority class. One common technique to address class imbalance is oversampling the minority class to balance the class distribution. This can help the model learn from the minority class and improve its performance on predicting that class."
    },
    {
        "course_id": 15125,
        "question_id": 2649,
        "question": "We learnt in the lecture that terms are typically stored in an inverted list. Now, in the inverted list, instead of only storing document identifiers of the documents in which the term appears, assume we also store an *offset* of the appearance of a term in a document. An $offset$ of a term $l_k$ given a document is defined as the number of words between the start of the document and $l_k$. Thus our inverted list is now:  $l_k= \\langle f_k: \\{d_{i_1} \\rightarrow [o_1,\\ldots,o_{n_{i_1}}]\\},  \\{d_{i_2} \\rightarrow [o_1,\\ldots,o_{n_{i_2}}]\\}, \\ldots,  \\{d_{i_k} \\rightarrow [o_1,\\ldots,o_{n_{i_k}}]\\} \\rangle$  This means that in document $d_{i_1}$ term $l_k$ appears $n_{i_1}$ times and at offset $[o_1,\\ldots,o_{n_{i_1}}]$, where $[o_1,\\ldots,o_{n_{i_1}}]$ are sorted in ascending order, these type of indices are also known as term-offset indices. An example of a term-offset index is as follows:  **Obama** = $\u27e84 : {1 \u2192 [3]},{2 \u2192 [6]},{3 \u2192 [2,17]},{4 \u2192 [1]}\u27e9$  **Governor** = $\u27e82 : {4 \u2192 [3]}, {7 \u2192 [14]}\u27e9$  **Election** = $\u27e84 : {1 \u2192 [1]},{2 \u2192 [1,21]},{3 \u2192 [3]},{5 \u2192 [16,22,51]}\u27e9$  Which is to say that the term **Governor** appear in 2 documents. In document 4 at offset 3, in document 7 at offset 14. Now let us consider the *SLOP/x* operator in text retrieval. This operator has the syntax: *QueryTerm1 SLOP/x QueryTerm2* finds occurrences of *QueryTerm1* within $x$ (but not necessarily in that order) words of *QueryTerm2*, where $x$ is a positive integer argument ($x \\geq 1$). Thus $x = 1$ demands that *QueryTerm1* be adjacent to *QueryTerm2*.  Consider the general procedure for \"merging\" two term-offset inverted lists for a given document, to determine where the document satisfies a *SLOP/x* clause (since in general there will be many offsets at which each term occurs in a document). Let $L$ denote the total number of occurrences of the two terms in the document. Assume we have a pointer to the list of occurrences of each term and can move the pointer along this list. As we do so we check whether we have a hit for $SLOP/x$ (i.e. the $SLOP/x$ clause is satisfied). Each move of either pointer counts as a step. Based on this assumption is there a general \"merging\" procedure to determine whether the document satisfies a $SLOP/x$ clause, for which the following is true? Justify your answer.      1. The merge can be accomplished in a number of steps linear in $L$ regardless of $x$, and we can ensure that each pointer moves only to the right (i.e. forward).     2. The merge can be accomplished in a number of steps linear in $L$, but a pointer may be forced to move to the left (i.e. backwards).     3. The merge can require $x \\times L$ steps in some cases.",
        "question_options": null,
        "A_chat_id": 90985,
        "B_chat_id": 90986,
        "A": "2. The merge can be accomplished in a number of steps linear in L, but a pointer may be forced to move to the left (i.e. backwards).\n\nIn order to determine whether the document satisfies a SLOP/x clause, we need to consider all possible combinations of offsets of the two terms within x words of each other. This requires moving the pointers along the inverted lists of the two terms and checking the distances between the offsets.\n\nIf the pointers only move to the right, we may miss valid combinations where the offsets are in reverse order. Therefore, in order to accurately determine whether the SLOP/x clause is satisfied, the pointers may need to move backwards as well. This means that the merge can be accomplished in a number of steps linear in L, but a pointer may be forced to move to the left.",
        "B": "Let's analyze the given scenario and determine the correct statement:\n\nIn the case of merging two term-offset inverted lists for a given document to determine if the document satisfies a SLOP/x clause, we have to consider the following:\n\n- We are dealing with two term-offset inverted lists for the same document.\n- Each list contains occurrences of a term within the document with their respective offsets.\n- We need to determine if the terms satisfy the SLOP/x condition, which means they can appear within x words of each other in any order.\n\nGiven these considerations, let's analyze the statements:\n\n1. **The merge can be accomplished in a number of steps linear in L regardless of x, and we can ensure that each pointer moves only to the right (i.e. forward).**\nThis statement implies that we can merge the lists in a linear number of steps in terms of the total number of occurrences L, and we only move the pointers forward. This statement is correct because we can iterate through the occurrences of each term in the document in a sequential manner and check if the SLOP/x condition is satisfied without needing to move the pointers backward. Therefore, this statement is justified.\n\n2. **The merge can be accomplished in a number of steps linear in L, but a pointer may be forced to move to the left (i.e. backwards).**\nThis statement suggests that we may need to move the pointers backward during the merging process. However, in the context of term-offset inverted lists and checking for the SLOP/x condition, moving the pointers backward is not necessary. Since we are looking for occurrences of terms within a certain proximity of each other, we can achieve this by moving the pointers only forward. Therefore, this statement is not justified.\n\n3. **The merge can require x * L steps in some cases.**\nThis statement implies that the merge process could potentially require x times the total number of occurrences L as steps. However, in the context of checking for the SLOP/x condition, we can accomplish the merge in a linear number of steps in terms of the total number of occurrences L, regardless of the value of x. Therefore, this statement is not justified.\n\nTherefore, the correct statement is:\n\n- **The merge can be accomplished in a number of steps linear in L regardless of x, and we can ensure that each pointer moves only to the right (i.e. forward).**"
    },
    {
        "course_id": 9,
        "question_id": 6836,
        "question": "Let \\(f(x) = 3 x \\left(\\log_2(x) \right)^2 + x^2 \\). Select the correct statement:",
        "question_options": [
            "\\(f(x)\\) is \\(O\\left(3 x \\left(\\log_2(x) \right)^2  \right)  \\) but not \\(\\Omega\\left(3 x \\left(\\log_2(x) \right)^2  \right)  \\)",
            "None of the other statements are correct",
            "\\(f(x)\\) is \\(\\Theta \\left(3 x \\left(\\log_2(x) \right)^2 \right)  \\)",
            "\\(f(x)\\) is \\(\\Omega\\left(3 x \\left(\\log_2(x) \right)^2  \right)  \\) but not \\(O\\left(3 x \\left(\\log_2(x) \right)^2  \right)  \\)"
        ],
        "A_chat_id": 90989,
        "B_chat_id": 90990,
        "A": "The correct statement is : \\(f(x)\\) is \\(\\Theta \\left(3 x \\left(\\log_2(x) \right)^2 \right)  \\). This is because \\(\\Theta\\) notation represents both the upper and lower bounds of a function.",
        "B": "To determine if \\( f(x) \\) is Big O, Big Omega, or Theta of \\( 3 x \\left(\\log_2(x)\\right)^2 \\), we need to analyze the growth rates of these functions as \\( x \\) approaches infinity.\n\nLet's simplify the functions first:\n\n1. \\( f(x) = 3 x \\left(\\log_2(x)\\right)^2 + x^2 \\)\n2. \\( g(x) = 3 x \\left(\\log_2(x)\\right)^2 \\)\n\nNow, let's analyze the growth rates of these functions:\n\n### Growth Rate Analysis:\n\n1. \\( f(x) = 3 x \\left(\\log_2(x)\\right)^2 + x^2 \\)\n   - As \\( x \\) grows large, the dominant term in \\( f(x) \\) is \\( x^2 \\).\n   - Therefore, \\( f(x) \\) grows asymptotically like \\( x^2 \\).\n\n2. \\( g(x) = 3 x \\left(\\log_2(x)\\right)^2 \\)\n   - As \\( x \\) grows large, the dominant term in \\( g(x) \\) is \\( x \\left(\\log_2(x)\\right)^2 \\).\n   - It is clear that \\( g(x) \\) does not grow faster than \\( f(x) \\) because it lacks the \\( x^2 \\) term.\n\n### Conclusion:\n\nSince \\( f(x) \\) grows faster than \\( g(x) \\), we can say that \\( f(x) \\) is \\( \\Omega(3 x (\\log_2(x))^2) \\) but not \\( O(3 x (\\log_2(x))^2) \\). \n\nTherefore, the correct statement is:\n[ ] \\( f(x) \\) is \\( \\Omega(3 x (\\log_2(x))^2) \\) but not \\( O(3 x (\\log_2(x))^2) \\)"
    },
    {
        "course_id": 15002,
        "question_id": 1021,
        "question": "Consider an IR engine, which uses an indexing mechanism implementing the following 3 consecutive filters:\n\na morpho-syntactic filter that restricts indexing term candidates to only nouns, and reduces them to their root forms;\na frequencial filter parameterized with \\(f_\\text{min}=0.06\\) (resp. \\(f_\\text{max}=0.20\\)) as lower (resp. upper) cut-off value, expressed as relative frequencies;\na stop word filter using the following stop list: {a, in, mouse, the}.\n\nand the following document \\(d\\):\n\nCats are the worst enemies of rodents. After all, a cat is a cat: as soon as it can, it rushes into the bushes with only one target in mind: mice, mice and mice! Naturally, the cats of houses are less frightening, as for them croquette loaded dressers have replaced prey hiding bushes. Cat's life in the house is easy!...\n\nWhat is the multi-set resulting from the indexing of document \\(d\\) by the above described IR engine?\nFormat your answer as an alphabetically ordered list of the form: \"lemma1(tf1), lemma2(tf2), ...\", where tfi is the term frequency of indexing term i.\nFor instance: dog(2), frog(3), zebra(1)",
        "question_options": null,
        "A_chat_id": 90994,
        "B_chat_id": 90995,
        "A": "After applying the three consecutive filters to the document \\(d\\), the resulting multi-set is:\n\"cat(3), enemy(1), house(2), life(1), prey(1), rodent(1), target(1)\"",
        "B": "To index the document \\(d\\) using the described IR engine, we will go through each step of the indexing mechanism:\n\n1. **Morpho-syntactic filter**: This filter restricts indexing term candidates to only nouns and reduces them to their root forms. After applying this filter to document \\(d\\), we get the following nouns in their root forms:\n   - cat\n   - enemy\n   - rodent\n   - cat\n   - cat\n   - target\n   - mouse\n   - cat\n   - house\n   - prey\n   - bush\n   - cat\n   - life\n   - house\n\n2. **Frequencial filter**: The frequency filter will remove terms based on their relative frequencies. The terms and their frequencies in the document after the morpho-syntactic filter are:\n   - cat(4), enemy(1), rodent(1), target(1), mouse(1), house(2), prey(1), bush(1), life(1)\n\n3. **Stop word filter**: Removing stop words from the list gives us:\n   - cat(4), enemy(1), rodent(1), target(1), mouse(1), house(2), prey(1), bush(1), life(1)\n\nSo, the resulting multi-set from the indexing of document \\(d\\) by the described IR engine is:\ncat(4), enemy(1), rodent(1), target(1), mouse(1), house(2), prey(1), bush(1), life(1)\n\nThe terms are presented in an alphabetically ordered list with their respective frequencies."
    },
    {
        "course_id": 15002,
        "question_id": 993,
        "question": "For each column of the lattice used in the Viterbi algorithm applied to that input sequence (\"process time control\"), how many nodes (= lines) does it have?Provide your answer as a coma separated list of integers.For instance, if the lattice has 5 columns, you could for instance here write:4,3,2,4,2",
        "question_options": null,
        "A_chat_id": 90998,
        "B_chat_id": 90999,
        "A": "10,9,8,7,6,5,4,3,2,1",
        "B": "To determine the number of nodes in each column of the lattice used in the Viterbi algorithm, we need to consider the number of possible states at each time step.\n\n1. The first column corresponds to the initial states, which are typically the start states of the HMM. In this case, there would be 1 node.\n\n2. For the second column, we consider the transitions from the initial states to the next set of states. The number of nodes in this column would depend on the number of possible states in the HMM model.\n\n3. Repeat the same process for each subsequent column until the final column, which would correspond to the end states.\n\nBased on the given input sequence \"process time control,\" we can estimate the number of nodes in each column by considering the possible states and transitions between states. Would you like to proceed with this calculation for a specific HMM model, or do you need further clarification on the concept?"
    },
    {
        "course_id": 15041,
        "question_id": 2744,
        "question": "What is the mean squared error of $f$ for a sample, where $\\textbf{x}$ is an input, $y$ a target and $f(\\textbf{x},W)$ the mapping function  ? \n(One answer)",
        "question_options": [
            " $||y - f(\\textbf{x},W)||^2 $ ",
            " $||y - f(\\textbf{x},W)|| $",
            " $-\\log(P(y=i | \\textbf{x})) = -\\log(\\frac{e^{\\textbf{f}_i(\\textbf{x},W)}}{\\sum_j e^{\\textbf{f}_j(\\textbf{x},W)}})$ ",
            " $P(y=i |\\textbf{x}) = \\frac{e^{\\textbf{f}_i(\\textbf{x},W)}}{\\sum_j e^{\\textbf{f}_j(\\textbf{x},W)}}$ "
        ],
        "A_chat_id": 91000,
        "B_chat_id": 91001,
        "A": "$||y - f(\\textbf{x},W)||^2 $",
        "B": "The mean squared error (MSE) of $f$ for a sample is given by the formula:\n\n$||y - f(\\textbf{x},W)||^2 $\n\nSo the correct answer is:\n\n[ ]  $||y - f(\\textbf{x},W)||^2 $"
    },
    {
        "course_id": 15127,
        "question_id": 3362,
        "question": "Given a joint data distribution $\\mathcal D$ on $\\mathcal X \times \\{-1,1\\}$ and $n$ independent and identically distributed observations from $\\mathcal D$, the goal of the classification task is to learn a classifier $f:\\mathcal X \to \\{-1,1\\}$ with minimum true risk $\\mathcal L(f) = \\mathbb E_{(X,Y)\\sim \\mathcal D} [\boldsymbol{\\mathbb{1}}_{f(X)\neq Y}]$ where $\boldsymbol{\\mathbb{1}}_{C} = \begin{cases}\n            1 \\; \text{ if } C  \text{ is true} \\\n            0 \\quad \text{otherwise} \n            \\end{cases}$. %\n            We denote by $\\mathcal D_{X}$ the marginal law (probability distribution) of $X$, and $\\mathcal D_{Y|X}$ the conditional law of $Y$ given $X$.\n            Give the two reasons seen in the course which explain that minimizing the true risk with the $0-1$ loss over the set of classifiers $f:\\mathcal X \to \\{-1,1\\}$ is problematic.",
        "question_options": null,
        "A_chat_id": 91002,
        "B_chat_id": 91003,
        "A": "The two main reasons why minimizing the true risk with the $0-1$ loss over the set of classifiers $f:\\mathcal X \\to \\{-1,1\\}$ is problematic are:\n\n1. Non-convexity: The $0-1$ loss function is non-convex, meaning that it is not a smooth function and does not have a unique global minimum. This makes it difficult to optimize directly, as standard optimization techniques may not work effectively. This can lead to convergence issues and suboptimal solutions.\n\n2. Non-differentiability: The $0-1$ loss function is non-differentiable, which means that it does not have a gradient that can be used for gradient-based optimization methods. This makes it challenging to find the optimal solution using gradient descent or other iterative optimization algorithms. This lack of smoothness and differentiability hinders the optimization process and makes it difficult to find the best classifier that minimizes the true risk.",
        "B": "The two reasons why minimizing the true risk with the $0-1$ loss over the set of classifiers $f:\\mathcal X \\to \\{-1,1\\}$ can be problematic are:\n\n1. **Non-convex Optimization**: The $0-1$ loss function is non-convex, making the optimization problem of directly minimizing the true risk with this loss function difficult. Non-convex optimization problems are harder to solve, as they may have multiple local minima and the optimization algorithms may get stuck in suboptimal solutions.\n\n2. **Non-differentiability**: The $0-1$ loss function is not differentiable, which poses challenges in using gradient-based optimization methods to minimize the true risk. Gradient descent and other optimization algorithms rely on the gradient of the loss function to update the model parameters, but when the loss function is not differentiable, these methods cannot be directly applied.\n\nThese issues make it impractical to directly minimize the true risk with the $0-1$ loss, leading to the need for surrogate loss functions that are differentiable and convex, such as the hinge loss or logistic loss, which can be more effectively optimized to approximate the true risk."
    },
    {
        "course_id": 15003,
        "question_id": 7325,
        "question": "Can we devise a broadcast algorithm that does not ensure the causal delivery property but only (in) its non-uniform variant:\nNo correct process pi delivers a message m2 unless pi has already delivered every message m1 such that m1 \u2192 m2?",
        "question_options": null,
        "A_chat_id": 91005,
        "B_chat_id": 91006,
        "A": "Yes, we can devise a broadcast algorithm that ensures the non-uniform variant of causal delivery. In this variant, a correct process will not deliver a message unless it has already delivered all causally preceding messages. This can be achieved by incorporating timestamps or sequence numbers in the messages and using them to determine the order of message delivery. Processes can then wait until they have received and delivered all messages that causally precede the message they are about to deliver. This way, the non-uniform causal delivery property can be enforced without the need for strict causal ordering of messages.",
        "B": "To design a broadcast algorithm that only ensures non-uniform causal delivery, we can modify the traditional causal broadcast algorithm to ignore the ordering of messages that are concurrent in the happened-before relationship. \n\nHere is a high-level description of a broadcast algorithm that achieves non-uniform causal delivery:\n\n1. Each process maintains a local sequence number for messages it sends and receives.\n2. When a process wants to broadcast a message m, it includes its local sequence number in the message.\n3. Upon receiving a message m, a process checks if it has already delivered all messages m' such that m' \u2192 m (i.e., m' causally precedes m).\n4. If it has not delivered all such messages, it buffers the message until all preceding messages have been delivered.\n5. Once all preceding messages have been delivered, the process delivers the message to the application and updates its local sequence number.\n\nBy ignoring the ordering of concurrent messages, this algorithm ensures that a process will not deliver a message m2 unless it has already delivered every message m1 such that m1 \u2192 m2. This achieves non-uniform causal delivery while not guaranteeing the causal delivery property.\n\nIt is important to note that ensuring non-uniform causal delivery may still allow for some out-of-order delivery of messages, as long as the causal relationships are preserved."
    },
    {
        "course_id": 0,
        "question_id": 6237,
        "question": "Tick the \\textbf{false} assertion. In Linear Cryptanalysis, the corresponding mask circuit of \\ldots",
        "question_options": [
            "\\ldots a XOR gate ($X\\oplus Y = Z$) is $a\\cdot Z=(a\\cdot X)\\oplus (a\\cdot Y)$",
            "\\ldots a XOR to constant gate ($Y=X\\oplus K$) is $a\\cdot Y = (a\\cdot X)\\oplus (a\\cdot K)$",
            "\\ldots a linear circuit ($Y=M\\times X$) is $a\\cdot Y = (M\\times a)\\cdot X$",
            "\\ldots a duplicate gate ($X=Y=Z$) is $(a\\oplus b)\\cdot X=(a\\cdot Y)\\oplus (b\\cdot Z)$"
        ],
        "A_chat_id": 91009,
        "B_chat_id": 91010,
        "A": "The false assertion is : In Linear Cryptanalysis, the corresponding mask circuit of a duplicate gate ($X=Y=Z$) is $(a\\oplus b)\\cdot X=(a\\cdot Y)\\oplus (b\\cdot Z)$. The correct corresponding mask circuit should be $(a\\oplus b)\\cdot X=(a\\cdot Y)\\oplus (b\\cdot Y)$.",
        "B": "The false assertion is:\n[ ] ... a duplicate gate ($X=Y=Z$) is $(a\\oplus b)\\cdot X=(a\\cdot Y)\\oplus (b\\cdot Z)$"
    },
    {
        "course_id": 15003,
        "question_id": 7341,
        "question": "What is the communication complexity of the FloodSet algorithm in number of bits?",
        "question_options": null,
        "A_chat_id": 91012,
        "B_chat_id": 91013,
        "A": "The FloodSet algorithm has a communication complexity of O(n log n) in terms of the number of bits, where n is the number of nodes in the network. This complexity arises from the fact that each node needs to communicate with its neighbors to propagate the information about the set of nodes that have been flooded. Each communication involves sending a message containing the set of flooded nodes, which can be represented using log n bits for each node identifier in the worst case scenario. Therefore, the total number of bits communicated in the network is O(n log n).",
        "B": "The FloodSet algorithm is a distributed algorithm used to achieve consensus in a network of processes. In the worst-case scenario, where each process has a unique value to propose and all processes need to agree on the same value, the communication complexity of the FloodSet algorithm can be analyzed as follows:\n\n1. Initialization: Each process needs to initially broadcast its own value to all other processes in the network. This requires each process to send a message containing its value to all other processes. Therefore, in the worst-case scenario, the communication complexity for the initialization phase is O(n) bits, where n is the number of processes in the network.\n\n2. Flooding: In the flooding phase, each process forwards the values it receives to all other processes in the network until all processes converge on the same value. The number of messages exchanged in the flooding phase depends on the diameter of the network and the convergence time of the algorithm. In the worst-case scenario, where the diameter of the network is large and convergence takes a long time, the communication complexity for the flooding phase can be significant.\n\nOverall, the communication complexity of the FloodSet algorithm in terms of the number of bits exchanged can be significant, especially in large networks with high message overhead. It is important to consider the network topology, the number of processes, and the convergence time when analyzing the communication complexity of the FloodSet algorithm."
    },
    {
        "course_id": 0,
        "question_id": 5740,
        "question": "In order to achieve a collision resistance equivalent to a $128$ bit key search, the size of the digest must be at least $\\ldots$",
        "question_options": [
            "$128$ bits.",
            "$2^{256}$ bits.",
            "$256$ bits.",
            "$2^{128}$ bits."
        ],
        "A_chat_id": 91015,
        "B_chat_id": 91016,
        "A": "[ ] $256$ bits. \n\nTo achieve a collision resistance equivalent to a $128$ bit key search, the size of the digest must be at least double that, which is $256$ bits.",
        "B": "In order to achieve a collision resistance equivalent to a $128$ bit key search, we need the size of the digest to be at least $2^{128}$ bits. This is because a $128$ bit key search means that there are $2^{128}$ possible keys, and in a collision-resistant hash function, the digest size should be at least as large as the number of possible outputs to ensure that finding a collision is computationally infeasible."
    },
    {
        "course_id": 15113,
        "question_id": 7040,
        "question": "Make some rough estimations (in terms of powers of $10$) according to your ``feeling'' (test for a job interview as ``management consultant''...):\n\nHow many pencils would you need to draw a straight line along the Earth's equator ?\n",
        "question_options": null,
        "A_chat_id": 91017,
        "B_chat_id": 91018,
        "A": "The Earth's equator is approximately 40,075 kilometers long. Let's assume a pencil can draw a line that is 0.1 mm wide. \n\nIf we divide the length of the equator by the width of the line that can be drawn by a pencil, we get: \n40,075,000 meters / 0.0001 meters = 400,750,000 pencils \n\nRoughly speaking, you would need around 10^8 pencils to draw a straight line along the Earth's equator.",
        "B": "Let's make some rough estimations for this problem:\n\n- Earth's equator is approximately $40,075$ kilometers long.\n- Let's assume the length of a pencil is about $15$ centimeters.\n- To cover the Earth's equator with pencils, we need to calculate how many pencils are required to span $40,075$ kilometers.\n\nEstimation:\n- $40,075$ kilometers is equivalent to $40,075,000$ meters.\n- Each pencil is approximately $0.15$ meters long.\n\nNow, we can estimate the number of pencils needed as follows:\n$$\\frac{40,075,000}{0.15} = 267,166,666.67 \\approx 10^8 \\text{ pencils}$$\n\nSo, you would roughly need about $10^8$ pencils to draw a straight line along the Earth's equator."
    },
    {
        "course_id": 0,
        "question_id": 5730,
        "question": "Tick the \\emph{false} answer. In a group, the operation\\dots",
        "question_options": [
            "is commutative",
            "is associative.",
            "has a neutral element.",
            "associates an inverse to each value."
        ],
        "A_chat_id": 91020,
        "B_chat_id": 91021,
        "A": "The false answer is : is commutative. In a group, the operation is not necessarily commutative.",
        "B": "The false answer is: [ ] is commutative."
    },
    {
        "course_id": 15113,
        "question_id": 7088,
        "question": "A particle of mass $m_1$, of initial speed $\\boldsymbol{v}_1 = v_1\\,\\boldsymbol{e}_x$ collides with a particle of mass $m_2$, initially at rest, i.e. $\\boldsymbol{v}_2 = \\boldsymbol{0}$. We consider here the case of an elastic collision between the two particles. We denote $\\boldsymbol{v}'_1 = v'_1\\left(\\cos\\theta_1\\,\\boldsymbol{e}_x + \\sin\\theta_1\\,\\boldsymbol{e}_y\\right)$ and $\\boldsymbol{v}'_2 = v'_2\\left(\\cos\\theta_2\\,\\boldsymbol{e}_x -\\,\\sin\\theta_2\\,\\boldsymbol{e}_y\\right)$ the speeds after the collision.\n\nUsing the conservation laws, write the equations relating the physical variables $v_1$, $m_1$, $m_2$ with the physical variables $v'_1$, $v'_2$, $\\theta_1$, $\\theta_2$.\n",
        "question_options": null,
        "A_chat_id": 91022,
        "B_chat_id": 91023,
        "A": "The conservation laws to be used in this case are conservation of momentum and conservation of kinetic energy.\n\nConservation of momentum in the x-direction:\n\\[ m_1v_1 = m_1v'_1\\cos\\theta_1 + m_2v'_2\\cos\\theta_2 \\]\n\nConservation of momentum in the y-direction:\n\\[ 0 = m_1v'_1\\sin\\theta_1 - m_2v'_2\\sin\\theta_2 \\]\n\nConservation of kinetic energy:\n\\[ \\frac{1}{2}m_1v_1^2 = \\frac{1}{2}m_1v'_1^2 + \\frac{1}{2}m_2v'_2^2 \\]\n\nThese are the equations relating the physical variables $v_1$, $m_1$, $m_2$ with the physical variables $v'_1$, $v'_2$, $\\theta_1$, $\\theta_2$ for an elastic collision between two particles.",
        "B": "To solve this problem, we can use the conservation laws of momentum and kinetic energy. \n\n1. Conservation of momentum in the x-direction:\nBefore the collision:\n$$m_1 v_1 = m_1 v'_1 \\cos\\theta_1 + m_2 v'_2 \\cos\\theta_2$$\n\n2. Conservation of momentum in the y-direction:\nBefore the collision:\n$$0 = m_1 v'_1 \\sin\\theta_1 - m_2 v'_2 \\sin\\theta_2$$\n\n3. Conservation of kinetic energy:\nBefore the collision:\n$$\\frac{1}{2} m_1 v_1^2 = \\frac{1}{2} m_1 v'_1^2 + \\frac{1}{2} m_2 v'_2^2$$\n\nNow we need to solve this system of equations to find the relations between the variables $v_1$, $m_1$, $m_2$, $v'_1$, $v'_2$, $\\theta_1$, and $\\theta_2$."
    },
    {
        "course_id": 15113,
        "question_id": 7057,
        "question": "Let us consider a hydrogen atom of mass $m = 1.67\\cdot10^{-27}\\,\\text{kg}$ chemically bound to a metallic surface. We model the chemical bound by a spring of constant $k$, of natural length $l_0$ and of vibration frequency $f=10^{13}\\,\\text{s}^{-1}$. \nDetermine the value of the spring constant $k$ of the chemical bound, neglecting the action of the gravitational force.\n",
        "question_options": null,
        "A_chat_id": 91024,
        "B_chat_id": 91025,
        "A": "The vibration frequency of a spring is given by the formula $f = \\frac{1}{2\\pi}\\sqrt{\\frac{k}{m}}$, where $k$ is the spring constant and $m$ is the mass attached to the spring.\n\nGiven that the vibration frequency $f = 10^{13}\\, \\text{s}^{-1}$ and the mass $m = 1.67\\cdot10^{-27}\\, \\text{kg}$, we can rearrange the formula to solve for $k$:\n\n$10^{13} = \\frac{1}{2\\pi}\\sqrt{\\frac{k}{1.67\\cdot10^{-27}}}$\n\nSolving for $k$:\n\n$k = (2\\pi \\cdot 10^{13})^2 \\cdot 1.67\\cdot10^{-27}$\n\n$k = 2.77\\cdot10^{-4}\\, \\text{N/m}$\n\nTherefore, the value of the spring constant $k$ of the chemical bound is $2.77\\cdot10^{-4}\\, \\text{N/m}$.",
        "B": "First, we can find the angular frequency of the spring using the relation $f = \\frac{\\omega}{2\\pi}$, where $f$ is the vibration frequency in Hz and $\\omega$ is the angular frequency in rad/s. \n\nGiven that $f = 10^{13}\\,\\text{s}^{-1}$, we have:\n\n$\\omega = 2\\pi f = 2\\pi \\times 10^{13} = 2\\times10^{13}\\pi\\,\\text{s}^{-1}$\n\nNext, we can relate the angular frequency of the spring to the spring constant $k$ using the formula $\\omega = \\sqrt{\\frac{k}{m}}$, where $m$ is the mass of the hydrogen atom.\n\nSubstituting in the values, we get:\n\n$2\\times10^{13}\\pi = \\sqrt{\\frac{k}{1.67\\cdot10^{-27}}}$\n\nSquaring both sides, we find:\n\n$4\\times10^{26}\\pi^2 = \\frac{k}{1.67\\cdot10^{-27}}$\n\nSolving for $k$, we have:\n\n$k = 4\\times10^{26}\\pi^2 \\times 1.67\\cdot10^{-27}$\n\n$k \\approx 1.32\\times10^{-12}\\,\\text{N/m}$\n\nTherefore, the value of the spring constant $k$ of the chemical bound is approximately $1.32\\times10^{-12}\\,\\text{N/m}$."
    },
    {
        "course_id": 15008,
        "question_id": 2739,
        "question": "For students born in April, how many months older are they than the average student in their grade? 5.4898 months\nFor students born in March, how many months younger are they than the average student in their grade? 5.5102 months\n\nDiscuss: Considering your common sense and the results obtained from the simulation: what advantage do students born in April have over those born in March? How may this affect their odds of becoming professional athletes?",
        "question_options": null,
        "A_chat_id": 91026,
        "B_chat_id": 91027,
        "A": "Students born in April have a slight advantage over those born in March in terms of age difference from the average student in their grade. This advantage may potentially translate into a physical advantage in sports, as older students may be more physically developed and mature compared to their younger peers. This could give students born in April a slight edge in sports competitions and potentially increase their odds of becoming professional athletes. However, it is important to note that factors such as talent, dedication, training, and coaching also play a significant role in determining success in sports.",
        "B": "Students born in April have a slight advantage over those born in March in terms of age difference from the average student in their grade. This advantage may potentially impact their physical development and maturity level when compared to their peers. \n\nIn sports, especially in highly competitive fields like professional athletics, even a small age difference can have significant effects on performance and skill development. Young athletes who are slightly older and more physically mature than their peers may have a slight edge in terms of strength, coordination, and skill acquisition.\n\nThis advantage may give students born in April a head start in their athletic endeavors, potentially increasing their odds of excelling in sports and eventually becoming professional athletes. However, it's important to note that many other factors contribute to success in sports, such as dedication, training, coaching, and talent. So while age may play a role, it is not the sole determinant of athletic success."
    }
]