[{"question_body": "Having two security controls one after the other provides better security than using the security controls in parallel.", "question_options": ["True", "False"], "answer": "This is True. Having two security controls one after the other provides the security of the strongest one, since you have to break both to attack the system. Having two security controls in parallel provides the security of the weakest one, since you only have to break that one to attack the system."}, {"question_body": "Tick the false assertion. Bluetooth ... ", "question_options": ["uses a stream cipher for encryption.", "has secure integrity protection for secure communication.", "has devices which use the same secret key with any other device.uses the SAFER+ block cipher in a one-way mode."], "answer": "The false assertion is : has secure integrity protection for secure communication. Bluetooth is indeed a secure protocol which relies on the E0 stream cipher for privacy. In Bluetooth, the encryption process involves the Long Term Key (LTK), which is a secret key shared and stored by both connected devices. Bluetooth implements confidentiality, authentication and key derivation with custom algorithms based on a SAFER+ block cipher"}, {"question_body": "In you opinion, is an anti-virus software a good protection against social engineering attacks carried out over e-mail? Explain why. What would be the best way of protecting against these attacks?", "question_options": null, "answer": "An antivirus software might be able to detect some typical characteristics of a social engineering e-mail (e.g. a fake sender address, or some keywords like \u201cplease pay\u201d, etc.). Social engineering attacks can have so many different forms that it does not seem possible to create a software that can detect all of them. The best protection is to raise the awareness of the users. This can be done through specific training of the users. This could include running a fake social engineering attack and informing them of the results."}, {"question_body": "Why can't you use a message authentication code (e.g. HMAC-SHA2) to sign a contract between a buyer and a seller ?", "question_options": null, "answer": "The MAC is based on a symmetric key that both parties need to know. Any party could modify the contract, replace the MAC and pretend it is authentic."}, {"question_body": "Tick the \\textbf{false} statement : ", "question_options": ["As a defender of a machine learning model you should be more worried about black-box effective attacks than white-box effective attacks.", "Privacy problems in machine learning stem solely from the need for data to train models.", "Poisoning attacks can be used to increase vulnerability to adversarial examples."], "answer": "The false statement is : Privacy problems in machine learning stem solely from the need for data to train models. Data collection for training is one of many privacy attack vectors in machine learning. There exist attacks on models and outputs; and naturally exposing data for test is a risk in itself. The first statement is true, an adversary performing a black-box attack needs much less resources and capabilities than a white-box adversary. This is much more dangerous, as the adversary only needs the ability to interact with the model. The last statement is also true because by providing poisoning inputs, the adversary gets to shape the boundaries of the model. Thus, she can carve this boundary to facilitate classification errors. In fact, you can understand a backdoor attack as a particular instance of an adversarial example."}, {"question_body": "(Privacy) It is possible to deploy surveillance only on end-users of systems.", "question_options": ["True", "False"], "answer": "This is False. Developers and CEOs of companies, government employees, and in general everyone is at the end of the day an end-user. Once the surveillance infrastructure is deployed, everyone will be under surveillance."}, {"question_body": "Privacy as control ensures that only the minimal amount of information is provided to the service.", "question_options": ["True", "False"], "answer": "This is False. The paradigm of privacy as control does not really focus on quantity. It focuses on the user knowing how the information is going to be used, but not on minimizing the amount of information disclosed."}, {"question_body": "(Privacy) To provide users with anonymity when accessing a web all accesses from one user must be unlinkable.", "question_options": ["True", "False"], "answer": "This is True. If accesses by a user are linkable, even if we do not know the identity, these accesses become a pseudonym. We cannot anymore say that it is truly anonymous. Thus, in general, unlinkability is needed for anonymity."}, {"question_body": "(Privacy) Fine-grained accountability and auditability make it difficult to implement systems with strong privacy protection.", "question_options": ["True", "False"], "answer": "This is True. Accountability and auditability mainly rely on logging actions. These logs typically record all actions in the system, becoming an extra source of information that can be used to infer private information about users."}, {"question_body": "We consider a binary classification problem for which a predictor is being considered. A validation set which contains 20$%$ of positive examples is used to evaluate the classifier. On this set the recall is at 90$%$ and the false positive rate is of 5$%$. What is the misclassification error? Please detail your reasoning and calculation.", "question_options": null, "answer": "Let P, N, FP, FN, TP denote respectively the number of positives, of negatives, of false positives, false negatives and true positives. Let n = N + P be the total number of datapoints in the validation set. By definition, the rate of false positives is rFP = FP/N and the rate of true positives, aka recall, is rTP = TP/P = 1 / FN/P. Let \u03c0 = P/n. \n By definition, the misclassification error is: $Rb0-1 = (FP/n) + (FN/n) = (1 - \u03c0) * (FP/N) + \u03c0 * (1 - rTP).$ \n With the numbers provided: $Rb0-1 = (1 - 0.2) * 0.05 + 0.2 * (1 - 0.9) = 0.8 * 0.05 + 0.2 * 0.1 = 0.06.$ \n The misclassification error is thus 6%."}, {"question_body": "Which of the following statements is true about the logistic regression model?", "question_options": ["Logistic regression gives a max-margin classifier", "By minimizing negative log-likelihood, we can obtain a closed-form solution for logistic regression", "In logistic regression, we calculate the weights $\\hat{\boldsymbol{\theta}}=(\\mathbf{X}^\top\\mathbf{X})^{-1}\\mathbf{X}^\top\\mathbf{y},\text{ and then fit responses as }\\hat{\\mathbf{y}}=\\sigma(\\mathbf{X}\\hat{\boldsymbol{\theta}})$", "If we run Gradient Descent to solve a logistic regression task on linearly separable data, the weights will not converge"], "answer": "If we run Gradient Descent to solve a logistic regression task on linearly separable data, the weights will not converge. There is no closed-form solution when minimizing negative log-likelihood for logistic regression. We cannot solve for $\\hat\theta$ analytically in logistic regression like in linear regression. Optimization techniques like GD or Newton methods are required. Logistic regression finds any solution that separates two classes. To solve logistic regression, we maximize log likelihood, i.e. $\\max_{\boldsymbol{\theta}}\\log\\prod_{n\boldsymbol{=}1}^N\\sigma(\\mathbf{x}_n^\top\boldsymbol{\theta})^{y_n}[1\boldsymbol{-}\\sigma(\\mathbf{x}_n^\top\boldsymbol{\theta})]^{1\boldsymbol{-}y_n}$. For linearly separable case, by increasing $\\|\theta\\|$, one could always increase the likelihood. The weights can thus go to infinity."}, {"question_body": "A rigid and homogeneous horizontal rod of mass M, of length L, of negligible thickness e, i.e. e << L, is rotating in a horizontal plane around the vertical axis $C e_3$ at angular velocity $\\Omega=\\dot{\\psi}\boldsymbol{e}3$. The moment of inertia of the rod with respect to the vertical principal axis that contains its centre of mass G is $I{G,3}=\frac1{12}ML^{2}$. Let C be the point of contact between the extremity of the rod and the rotation axis, and P the point at the other extremity. The unit vector $e_1$ is oriented along the rod and the vector $e_2$ is orthogonal to $e_1$. The unit vectors $e_1$, $e_2$ and $e_3$, attached to the rod at point C, are the principal axis frame of the rod. We consider that there is no friction and we do not take explicitly into account the influence of the gravitational field. Give the expression of the moment of inertia $I_{C,3}$ of the rod with respect to the vertical axis of rotation $C e_3$ in terms of the scalar quantities M and L.", "question_options": null, "answer": "Applying the Huygens-Steiner theorem, we determine the moment of inertia $I_{C,3}$ of the rod with respect to the vertical axis $C e_3$ taking into account the fact that d = L/2 is the distance separating points G and C and that $I_{G,3}=\frac1{12}ML^{2}$,\n$I_{C,3}=I_{G,3}+Md^2=I_{G,3}+M\\left(\frac{L}{2}\right)^2=\frac{1}{12}ML^2+\frac{1}{4}ML^2=\frac{1}{3}ML^2$"}, {"question_body": "A material point P of mass m subjected to the gravitational field g = - g $\\hat{x_3}$ is constrained to move on the inside surface of a cone. The vertex of the cone is located at the origin O of the Cartesian frame $(O,\boldsymbol{\\hat{x}}1,\boldsymbol{\\hat{x}}2,\boldsymbol{\\hat{x}}_3)$. The axis of symmetry of the cone is the vertical line containing point O and its opening angle is $\theta$ = const. The spherical frame $(P, e_r, e{\theta}, e{\\phi},)$ attached to the material point P is such that the basis vectors $e_r$ and $e_\theta$ are always contained in a vertical plane and the basis vector $e_\\phi$ is horizontal. We assume that there is no friction force. Determine the three scalar equations of motion along the lines of coordinates tangent to the unit vectors $\boldsymbol{e}r,\boldsymbol{e}\theta\\mathrm{and}\boldsymbol{e}_\\phi$ in terms of the scalar quantities m, g, N (norm of the normal reaction force), $r,\\dot{r},\\ddot{r}\theta,\\dot{\\phi},\\ddot{\\phi}$ taking explicitly into account the geometric constraints.", "question_options": null, "answer": "The external forces are the weight P and the normal reaction force N. These forces are expressed in spherical coordinates as: $\boldsymbol{P}=m\boldsymbol{g}=mg(-\\cos\theta\boldsymbol{e}r+\\sin\theta\boldsymbol{e}\theta)$ and $N=-N\boldsymbol{e}\theta$. \nTaking into account the geometric constraints, i.e. $\theta=\\mathrm{const}$, thus $\\dot{\theta}=0$ and $\\ddot{\theta}=0$, \nthe acceleration is expressed in spherical coordinates as: \n$\boldsymbol{a}=\\left(\\ddot{r}-r\\dot{\\phi}^2\\sin^2\theta\right)\boldsymbol{e}r+\\left(r\\ddot{\theta}-r\\dot{\\phi}^2\\sin\theta\\cos\theta\right)\boldsymbol{e}\theta+\\left(r\\ddot{\\phi}\\sin\theta+2\\dot{r}\\dot{\\phi}\\sin\theta\right)\boldsymbol{e}\\phi $\nThe vectorial law of motion: $\\sum\boldsymbol{F}^\\mathrm{ext}=\boldsymbol{P}+\boldsymbol{N}=m\boldsymbol{a}$\nprojected along the unit vectors tangent to the three lines of coordinates yields the three scalar equations :\n$$\begin{aligned}&\text{along}\\quad\boldsymbol{e}_r:\\quad-mg\\cos\theta=m\\left(\\ddot{r}-r\\dot{\\phi}^2\\sin^2\theta\right)\\&\text{along}\\quad\boldsymbol{e}\theta:\\quad mg\\sin\theta-N=m\\left(-r\\dot{\\phi}^2\\sin\theta\\cos\theta\right)\\&\text{along}\\quad\boldsymbol{e}\\phi:\\quad0=m\\left(r\\ddot{\\phi}\\sin\theta+2\\dot{r}\\dot{\\phi}\\sin\theta\right)\\end{aligned}$$"}, {"question_body": "(Self-supervised learning) When GPT models are trained on next token prediction with teacher-forcing, during training, each predicted token is added to the input sequence and fed back as input to the model in an autoregressive manner.", "question_options": ["True", "False"], "answer": "False. In GPT models, predicting the next token in an autoregressive manner is used for inference only, while for training the model is fed with the entire sequence of original tokens and the masked (causal) attention prevents the model from cheating. This is also called teacher-forcing."}, {"question_body": "(Adversarial Training) Adversarial training typically results in a decreased (standard) accuracy on a test set drawn from the same distribution as the training and validation sets.", "question_options": ["True", "False"], "answer": "True. Robustness often comes at the cost of standard accuracy as non-robust features can be useful and generalize to the test set."}, {"question_body": "(EM Algorithm) The Expectation-Maximization (EM) algorithm is guaranteed to converge to the global maximum likelihood solution for fitting GMMs.", "question_options": ["True", "False"], "answer": "False. The EM algorithm for GMMs may converge to a local maximum likelihood solution, not necessarily the global maximum likelihood solution."}, {"question_body": "(Gradient Descent) When training a Deep Neural Network, it is better to use classical gradient descent rather than stochastic gradient descent with mini-batches to optimize the parameters of the network.", "question_options": ["True", "False"], "answer": "False. Deep Learning requires large datasets, and going through all of the data to compute a full gradient is unnecessarily expensive, compared to many cheaper SGD steps. In addition, noisy updates for several reasons seem to perform better in the non-convex landscapes stemming from deep learning training."}, {"question_body": "(Feature expansion) Unnecessary polynomial expansion of input features can lead to underfitting.", "question_options": ["True", "False"], "answer": "False. It can lead to overfitting."}, {"question_body": "(Linear regression) For linear regression with no regularization, scaling the features (e.g. as in data normalization) does not change the model's performance, assuming that we can efficiently compute the optimum model in both cases, i.e. numerical stability/efficiency of finding the optimum model is not a concern.", "question_options": ["True", "False"], "answer": "True. Linear regression models are scale invariant when there is no regularization. The model will learn the same relationship between inputs and outputs regardless of the scale of the data."}, {"question_body": "Considering a sequence of n tokens, the computational complexity of the masked attention mechanism in BERT language models is: (select the smallest correct complexity)", "question_options": ["O(n^3)", "O(nlog(n))", "O(n)", "O(n^(1/2))", "O(n^2)"], "answer": "Masked attention is quadratic in the sequence length n because it computes the attention between all pairs of tokens, therefore O(n^2)."}, {"question_body": "Given matrix $A\\in\\mathbb{R}^{d\times d}$ with eigenvectors $(1,2,1)^\top$ and $(1,1,0)^\top$, both with eigenvalue 4, and trace(A) = 2. What is the determinant of A?", "question_options": ["det(A) = -16", "det(A) = 128", "det(A) = 16", "The determinant of a matrix cannot be determined, since the dimension of A is unknown. ", "det(A) = -128", "det(A) = -96"], "answer": "Since the eigenvectors of A is in $R^3$, the total number of eigenvalues is 3. Using trace(A) = $\\lambda_1 + \\lambda_2 + \\lambda_3$, we can determine the eigenvalues are (4, 4, -6). Thus, det(A) = 4 x 4 x (-6) = -96."}, {"question_body": "We are using Gradient Descent to find the 1-dimensional global minimum $w^$ by optimizing the loss function L(w) at iteration t. L(w) is strictly convex, so it has a unique minimum. If $w^t > w^$, what is true about the gradient of the loss function, \u2207L(wt), and the next iteration of the parameter $w_{t+1}$?", "question_options": ["$\nabla\\mathcal{L}(w^t)>0\\mathrm{and}w^{t+1}<w^t$", "$\nabla\\mathcal{L}(w^t)<0\\mathrm{and}w^{t+1}<w^t$", "$\nabla\\mathcal{L}(w^t)>0\\mathrm{and}w^{t+1}>w^t$", "$\nabla\\mathcal{L}(w^t)<0\\mathrm{and}w^{t+1}>w^t$"], "answer": "$\nabla\\mathcal{L}(w^t)>0\\mathrm{and}w^{t+1}<w^t$. Take, for example, the MSE loss function. $w^t > w^*$, it means that the current parameter is greater than the optimum and at this point the gradient is positive. In a Gradient Descent optimization step $w^{t+1}=w^t-\\gamma\nabla\\mathcal{L}(w^t)$, the parameter moves closer to the optimum and therefore\u00a0decreases."}]